[msx] before_class
2020-04-02 05:06:19,897 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
Formatting using clusterid: testClusterID
2020-04-02 05:06:20,494 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:20,509 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:20,510 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:20,511 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:20,526 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:20,527 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:20,528 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:20,528 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:20,576 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:20,580 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-04-02 05:06:20,581 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:20,581 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:20,586 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:20,587 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:20
2020-04-02 05:06:20,589 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:20,590 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:20,592 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:20,592 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:20,611 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:20,618 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:20,619 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:20,619 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:20,619 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:20,620 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:20,620 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:20,620 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:20,620 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:20,621 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:20,621 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:20,621 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:20,650 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:initialize(77)) - GLOBAL serial map: bits=24 maxEntries=16777215
2020-04-02 05:06:20,671 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:20,671 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:20,672 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:20,672 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:20,679 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:20,680 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:20,680 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:20,680 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:20,685 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:20,688 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:20,692 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:20,692 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:20,693 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:20,693 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:20,701 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:20,702 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:20,702 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:20,706 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:20,706 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:20,709 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:20,710 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:20,710 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:20,710 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:20,750 [main] INFO  namenode.FSImage (FSImage.java:format(170)) - Allocated new BlockPoolId: BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:20,774 [main] INFO  common.Storage (NNStorage.java:format(583)) - Storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 has been successfully formatted.
2020-04-02 05:06:20,783 [main] INFO  common.Storage (NNStorage.java:format(583)) - Storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 has been successfully formatted.
2020-04-02 05:06:20,801 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 using no compression
2020-04-02 05:06:20,802 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 using no compression
2020-04-02 05:06:20,952 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds .
2020-04-02 05:06:20,952 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds .
2020-04-02 05:06:20,998 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-04-02 05:06:21,003 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:21,143 [main] INFO  beanutils.FluentPropertyBeanIntrospector (FluentPropertyBeanIntrospector.java:introspect(147)) - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2020-04-02 05:06:21,187 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:21,566 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:21,567 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:21,572 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:21,597 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:21,650 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7d4f9aae] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:21,652 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:21,658 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:21,673 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @3321ms
2020-04-02 05:06:21,785 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:21,789 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:21,790 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:21,798 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:21,801 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:21,802 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:21,802 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:21,834 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:21,835 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:21,846 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 46810
2020-04-02 05:06:21,848 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:21,903 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@311bf055{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:21,904 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7d322cad{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:21,953 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@55c53a33{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:21,967 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@6f6745d6{HTTP/1.1,[http/1.1]}{localhost:46810}
2020-04-02 05:06:21,967 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3616ms
2020-04-02 05:06:21,980 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:21,981 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:21,981 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:21,981 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:21,982 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:21,982 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:21,982 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:21,982 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:21,983 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:21,983 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:21,983 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:21,984 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:21,984 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:21
2020-04-02 05:06:21,984 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:21,984 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:21,985 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-04-02 05:06:21,985 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:21,993 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:21,993 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:21,994 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:21,994 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:21,995 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:21,995 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:21,995 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:21,995 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:21,996 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:21,996 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:21,996 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:21,997 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:21,997 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:21,997 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:21,998 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-04-02 05:06:21,998 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:22,002 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:22,003 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:22,003 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:22,003 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:22,004 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:22,004 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:22,004 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:22,004 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:22,005 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-04-02 05:06:22,005 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:22,006 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:22,007 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:22,007 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:22,007 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:22,008 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:22,008 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:22,008 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:22,009 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-04-02 05:06:22,009 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:22,014 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:22,017 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:22,020 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:22,020 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:22,021 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(718)) - No edit log streams selected.
2020-04-02 05:06:22,021 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-04-02 05:06:22,060 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 1 INodes.
2020-04-02 05:06:22,066 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:22,067 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 0 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-04-02 05:06:22,072 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:22,073 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 1
2020-04-02 05:06:22,098 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:22,098 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 86 msecs
2020-04-02 05:06:22,260 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:22,271 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:22,285 [Socket Reader #1 for port 46221] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 46221
2020-04-02 05:06:22,533 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:46221 to access this namenode/service.
2020-04-02 05:06:22,537 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:22,555 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:22,567 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:22,567 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:22,568 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:22,568 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:22,571 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 3 msec
2020-04-02 05:06:22,621 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:22,623 [IPC Server listener on 46221] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 46221: starting
2020-04-02 05:06:22,624 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:46221
2020-04-02 05:06:22,627 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:22,627 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:22,630 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 2 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:22,661 [CacheReplicationMonitor(1374967596)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:22,662 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 46221 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:22,675 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:22,756 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:22,785 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:22,806 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:22,807 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:22,811 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:22,814 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:22,817 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:22,818 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:22,823 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:22,830 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:44775
2020-04-02 05:06:22,832 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:22,832 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:22,846 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:22,851 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:22,852 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:22,852 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:22,854 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:22,855 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:22,855 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:22,855 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:22,859 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 43309
2020-04-02 05:06:22,859 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:22,860 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@478ee483{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:22,861 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2974f221{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:22,867 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@21baa903{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:22,868 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@607fbe09{HTTP/1.1,[http/1.1]}{localhost:43309}
2020-04-02 05:06:22,868 [main] INFO  server.Server (Server.java:doStart(419)) - Started @4517ms
2020-04-02 05:06:23,262 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45948
2020-04-02 05:06:23,263 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@119f1f2a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:23,264 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:23,265 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:23,283 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:23,284 [Socket Reader #1 for port 38296] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 38296
2020-04-02 05:06:23,299 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:38296
2020-04-02 05:06:23,323 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:23,339 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:23,793 [IPC Server listener on 38296] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 38296: starting
2020-04-02 05:06:23,795 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:23,812 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 38296 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:23,842 [Thread-59] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46221 starting to offer service
2020-04-02 05:06:24,182 [Thread-59] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46221
2020-04-02 05:06:24,184 [Thread-59] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:24,187 [Thread-59] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:24,188 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 is not formatted for namespace 976002151. Formatting...
2020-04-02 05:06:24,189 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 
2020-04-02 05:06:24,193 [Thread-59] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:24,193 [Thread-59] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 is not formatted for namespace 976002151. Formatting...
2020-04-02 05:06:24,193 [Thread-59] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-b272f152-1868-4df9-9efd-758e32857e3b for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 
2020-04-02 05:06:24,213 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,214 [Thread-59] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,215 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1 and block pool id BP-792146049-172.17.0.8-1585803980740 is not formatted. Formatting ...
2020-04-02 05:06:24,215 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-792146049-172.17.0.8-1585803980740 directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current
2020-04-02 05:06:24,226 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,229 [Thread-59] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,230 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2 and block pool id BP-792146049-172.17.0.8-1585803980740 is not formatted. Formatting ...
2020-04-02 05:06:24,230 [Thread-59] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-792146049-172.17.0.8-1585803980740 directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current
2020-04-02 05:06:24,232 [Thread-59] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=null
2020-04-02 05:06:24,233 [Thread-59] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1556)) - Generated and persisted new Datanode UUID 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:24,401 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:24,402 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:24,411 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:24,414 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:24,420 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:24,425 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:24,440 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:24,447 [Thread-59] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:24,447 [Thread-59] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:24,490 [Thread-59] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,498 [IPC Server handler 4 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,500 [Thread-77] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:24,512 [Thread-78] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:24,523 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:24,523 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:24,575 [Thread-78] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 62ms
2020-04-02 05:06:24,581 [Thread-77] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 73ms
2020-04-02 05:06:24,585 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 96ms
2020-04-02 05:06:24,593 [Thread-81] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:24,593 [Thread-81] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:24,597 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:24,598 [Thread-81] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 6ms
2020-04-02 05:06:24,598 [Thread-82] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:24,600 [Thread-82] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 2ms
2020-04-02 05:06:24,600 [Thread-59] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 12ms
2020-04-02 05:06:24,602 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:24,602 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:24,603 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): finished scanning block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,608 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): finished scanning block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,616 [Thread-59] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 9:14 AM with interval of 21600000ms
2020-04-02 05:06:24,628 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46221 beginning handshake with NN
2020-04-02 05:06:24,630 [IPC Server handler 9 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,631 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:24,632 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:24,646 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814399956 ms.
2020-04-02 05:06:24,647 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814399955 ms.
2020-04-02 05:06:24,665 [IPC Server handler 7 on 46221] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:44775, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45948, infoSecurePort=0, ipcPort=38296, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:24,669 [IPC Server handler 7 on 46221] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:44775
2020-04-02 05:06:24,670 [IPC Server handler 7 on 46221] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:44775).
2020-04-02 05:06:24,675 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46221 successfully registered with NN
2020-04-02 05:06:24,675 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:46221 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:24,698 [IPC Server handler 6 on 46221] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:44775
2020-04-02 05:06:24,698 [IPC Server handler 6 on 46221] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:44775
2020-04-02 05:06:24,726 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xa875aa9569cbebee: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:24,728 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xa875aa9569cbebee: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:44775, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45948, infoSecurePort=0, ipcPort=38296, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 2 msecs, invalidatedBlocks: 0
2020-04-02 05:06:24,728 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xa875aa9569cbebee: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:24,729 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xa875aa9569cbebee: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:44775, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45948, infoSecurePort=0, ipcPort=38296, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:06:24,741 [IPC Server handler 5 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,749 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:24,760 [IPC Server handler 3 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,761 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0xa875aa9569cbebee,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 3 msec to generate and 46 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:24,761 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:24,766 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterChange
[msx] unitTestCounterInClass = 0
2020-04-02 05:06:24,819 [IPC Server handler 8 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p1	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:24,839 [IPC Server handler 4 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p1	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:24,869 [IPC Server handler 9 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p1	dst=null	perm=root:supergroup:rwxr-x---	proto=rpc
2020-04-02 05:06:24,903 [IPC Server handler 7 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,931 [IPC Server handler 5 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,932 [IPC Server handler 5 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 46221, call Call#13 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:24,960 [IPC Server handler 2 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:24,961 [IPC Server handler 2 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 46221, call Call#14 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:24,964 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot1 for /p1
2020-04-02 05:06:24,984 [IPC Server handler 3 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,009 [IPC Server handler 0 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,038 [IPC Server handler 1 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p1	dst=/p1/.snapshot/snapshot1	perm=null	proto=rpc
2020-04-02 05:06:25,048 [IPC Server handler 8 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,060 [IPC Server handler 4 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,079 [IPC Server handler 9 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,087 [IPC Server handler 7 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,095 [IPC Server handler 6 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,105 [IPC Server handler 5 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,119 [IPC Server handler 3 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,119 [IPC Server handler 3 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 46221, call Call#25 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:25,134 [IPC Server handler 0 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,135 [IPC Server handler 0 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 46221, call Call#26 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:25,147 [IPC Server handler 1 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p1	dst=null	perm=root:supergroup:r-xr-x---	proto=rpc
2020-04-02 05:06:25,155 [IPC Server handler 8 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,162 [IPC Server handler 4 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,174 [IPC Server handler 9 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,179 [IPC Server handler 7 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,191 [IPC Server handler 6 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,191 [IPC Server handler 6 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 46221, call Call#32 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36234: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:25,222 [IPC Server handler 5 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,223 [IPC Server handler 5 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 46221, call Call#33 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36234: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:25,226 [IPC Server handler 2 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,238 [IPC Server handler 0 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,254 [IPC Server handler 8 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,256 [IPC Server handler 8 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 46221, call Call#38 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:25,262 [IPC Server handler 4 on 46221] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:25,262 [IPC Server handler 4 on 46221] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 46221, call Call#39 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36242: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:25,267 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:25,267 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:25,268 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 38296 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:25,270 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5bd82fed] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:25,269 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:25,282 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:25,283 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:25,412 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@21baa903{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:25,418 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@607fbe09{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:25,419 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2974f221{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:25,432 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@478ee483{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:25,482 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 38296
2020-04-02 05:06:25,483 [IPC Server listener on 38296] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 38296
2020-04-02 05:06:25,490 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:25,493 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:25,493 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46221
2020-04-02 05:06:25,602 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:25,602 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46221] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:25,613 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:25,641 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:25,653 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:25,654 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:25,655 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:25,655 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:25,674 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:25,675 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:25,675 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 46221 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:25,675 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:25,678 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 1, 8
2020-04-02 05:06:25,678 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@3ecd267f] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:25,681 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@77d2e85] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:25,682 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 9 Total time for transactions(ms): 24 Number of transactions batched in Syncs: 0 Number of syncs: 10 SyncTimes(ms): 1 1 
2020-04-02 05:06:25,685 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000009
2020-04-02 05:06:25,686 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000009
2020-04-02 05:06:25,686 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:25,687 [CacheReplicationMonitor(1374967596)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:25,710 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 46221
2020-04-02 05:06:25,714 [IPC Server listener on 46221] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 46221
2020-04-02 05:06:25,714 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:25,714 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:25,725 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:25,773 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:25,774 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:25,775 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@55c53a33{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:25,780 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@6f6745d6{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:25,780 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7d322cad{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:25,781 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@311bf055{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:25,786 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:25,790 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:25,791 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:25,804 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:25,806 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:25,810 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:25,812 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:25,813 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:25,813 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:25,814 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:25,823 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5471388b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:25,823 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:25,823 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:25,824 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:25,825 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:25,825 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:25,833 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:25,834 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:25,834 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:25,834 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:25,836 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:25,836 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:25,837 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 38256
2020-04-02 05:06:25,837 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:25,844 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@517bd097{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:25,845 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4a9cc6cb{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:25,859 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@64712be{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:25,860 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@53499d85{HTTP/1.1,[http/1.1]}{localhost:38256}
2020-04-02 05:06:25,860 [main] INFO  server.Server (Server.java:doStart(419)) - Started @7509ms
2020-04-02 05:06:25,869 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:25,869 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:25,870 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:25,870 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:25,870 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:25,870 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:25,870 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:25,871 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:25,871 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:25,872 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:25,872 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:25,872 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:25,873 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:25
2020-04-02 05:06:25,873 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:25,873 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:25,873 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:25,874 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:25,889 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:25,892 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:25,892 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:25,893 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:25,898 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:25,898 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:25,898 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:25,898 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:25,899 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:25,899 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:25,899 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:25,905 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:25,905 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:25,905 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:25,905 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:25,909 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:25,910 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:25,910 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:25,910 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:25,910 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:25,911 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:25,913 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:25,913 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:25,913 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:25,914 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:25,914 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:25,914 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:25,914 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:25,915 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:25,915 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:25,925 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:25,935 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:25,937 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:25,938 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:25,945 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-04-02 05:06:25,946 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 1 INodes.
2020-04-02 05:06:25,947 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:25,947 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 0 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000
2020-04-02 05:06:25,947 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@54709809 expecting start txid #1
2020-04-02 05:06:25,948 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000009, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000009 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:25,948 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000009' to transaction ID 1
2020-04-02 05:06:25,963 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000001-0000000000000000009, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000001-0000000000000000009 of size 390 edits # 9 loaded in 0 seconds
2020-04-02 05:06:25,964 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:25,965 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 10
2020-04-02 05:06:25,980 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:25,980 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 63 msecs
2020-04-02 05:06:25,981 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:25,981 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:25,982 [Socket Reader #1 for port 35149] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 35149
2020-04-02 05:06:25,989 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:35149 to access this namenode/service.
2020-04-02 05:06:25,991 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:26,016 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:26,018 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:26,018 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:26,018 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:26,018 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:26,022 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:26,023 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:26,023 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:26,023 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:26,023 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:26,023 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:26,027 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:26,027 [IPC Server listener on 35149] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 35149: starting
2020-04-02 05:06:26,029 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:35149
2020-04-02 05:06:26,030 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:26,030 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:26,031 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 1 milliseconds
name space=2
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:26,039 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 35149 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:26,042 [CacheReplicationMonitor(103399920)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:26,045 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:26,048 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:26,049 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:26,083 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:26,084 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:26,084 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:26,084 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:26,095 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:26,095 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:26,096 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:26,103 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:41365
2020-04-02 05:06:26,103 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:26,104 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:26,105 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:26,121 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:26,123 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:26,123 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:26,124 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:26,125 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:26,125 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:26,125 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:26,126 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 33140
2020-04-02 05:06:26,126 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:26,129 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4e4efc1b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:26,131 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7cc586a8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:26,138 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@5db99216{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:26,138 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3ec11999{HTTP/1.1,[http/1.1]}{localhost:33140}
2020-04-02 05:06:26,139 [main] INFO  server.Server (Server.java:doStart(419)) - Started @7787ms
2020-04-02 05:06:26,253 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:35740
2020-04-02 05:06:26,253 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:26,253 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@9f46d94] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:26,253 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:26,254 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:26,255 [Socket Reader #1 for port 39133] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 39133
2020-04-02 05:06:26,259 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:39133
2020-04-02 05:06:26,272 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:26,275 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:26,287 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:26,287 [IPC Server listener on 39133] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 39133: starting
2020-04-02 05:06:26,295 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 39133 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:26,295 [Thread-142] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35149 starting to offer service
2020-04-02 05:06:26,321 [IPC Server handler 2 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,321 [Thread-142] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35149
2020-04-02 05:06:26,323 [Thread-142] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:26,327 [Thread-142] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:26,328 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:26,328 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:26,330 [Thread-142] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:26,369 [Thread-142] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,370 [Thread-142] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,384 [Thread-142] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,384 [Thread-142] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,388 [Thread-142] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:26,391 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:26,392 [Thread-142] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:26,394 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:26,394 [Thread-142] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:26,395 [Thread-142] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:26,398 [Thread-142] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:26,402 [Thread-142] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:26,422 [Thread-142] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:26,423 [Thread-142] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:26,424 [Thread-142] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,424 [Thread-157] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:26,424 [Thread-158] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:26,434 [IPC Server handler 1 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,437 [Thread-157] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:26,438 [Thread-158] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:26,449 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:26,449 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:26,452 [Thread-157] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 29ms
2020-04-02 05:06:26,474 [Thread-158] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 49ms
2020-04-02 05:06:26,475 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 51ms
2020-04-02 05:06:26,476 [Thread-159] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:26,476 [Thread-159] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:26,476 [Thread-160] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:26,476 [Thread-160] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:26,477 [Thread-159] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-04-02 05:06:26,477 [Thread-160] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1ms
2020-04-02 05:06:26,478 [Thread-142] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 2ms
2020-04-02 05:06:26,495 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814398107 ms.
2020-04-02 05:06:26,497 [Thread-142] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 10:06 AM with interval of 21600000ms
2020-04-02 05:06:26,497 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814398105 ms.
2020-04-02 05:06:26,505 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:35149 beginning handshake with NN
2020-04-02 05:06:26,507 [IPC Server handler 3 on 35149] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:41365, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35740, infoSecurePort=0, ipcPort=39133, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:26,508 [IPC Server handler 3 on 35149] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:41365
2020-04-02 05:06:26,508 [IPC Server handler 3 on 35149] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:41365).
2020-04-02 05:06:26,513 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:35149 successfully registered with NN
2020-04-02 05:06:26,513 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:35149 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:26,518 [IPC Server handler 4 on 35149] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:41365
2020-04-02 05:06:26,519 [IPC Server handler 4 on 35149] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:41365
2020-04-02 05:06:26,525 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x901eeb4dfa9325d4: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:26,527 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x901eeb4dfa9325d4: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:41365, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35740, infoSecurePort=0, ipcPort=39133, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:06:26,529 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x901eeb4dfa9325d4: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:26,533 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x901eeb4dfa9325d4: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:41365, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35740, infoSecurePort=0, ipcPort=39133, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 3 msecs, invalidatedBlocks: 0
2020-04-02 05:06:26,543 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x901eeb4dfa9325d4,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 18 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:26,544 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,552 [IPC Server handler 8 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,555 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:26,558 [IPC Server handler 6 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,564 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:26,569 [IPC Server handler 7 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,572 [IPC Server handler 9 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,583 [IPC Server handler 0 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,588 [IPC Server handler 2 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,598 [IPC Server handler 1 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,598 [IPC Server handler 1 on 35149] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 35149, call Call#52 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:48624: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:26,606 [IPC Server handler 3 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,606 [IPC Server handler 3 on 35149] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 35149, call Call#53 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:48624: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:26,624 [IPC Server handler 4 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,635 [IPC Server handler 8 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,639 [IPC Server handler 7 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,639 [IPC Server handler 7 on 35149] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 35149, call Call#58 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:48626: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:26,644 [IPC Server handler 9 on 35149] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:26,645 [IPC Server handler 9 on 35149] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 35149, call Call#59 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:48626: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:26,654 [main] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:06:26,654 [main] INFO  namenode.FSImage (FSImage.java:saveNamespace(1101)) - Save namespace ...
2020-04-02 05:06:26,654 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 10, 10
2020-04-02 05:06:26,657 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 9 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:06:26,658 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000010 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000010-0000000000000000011
2020-04-02 05:06:26,659 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000010 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000010-0000000000000000011
2020-04-02 05:06:26,661 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000011 using no compression
2020-04-02 05:06:26,661 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000011 using no compression
2020-04-02 05:06:26,686 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000011 of size 623 bytes saved in 0 seconds .
2020-04-02 05:06:26,691 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000011 of size 623 bytes saved in 0 seconds .
2020-04-02 05:06:26,694 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 2 images with txid >= 0
2020-04-02 05:06:26,697 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 12
2020-04-02 05:06:26,717 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:saveNamespace(4419)) - New namespace image has been created
2020-04-02 05:06:26,717 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:26,717 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:26,718 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 39133 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:26,718 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:26,719 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@4940809c] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:26,722 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:26,733 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:26,774 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@5db99216{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:26,777 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3ec11999{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:26,777 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7cc586a8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:26,778 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4e4efc1b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:26,803 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 39133
2020-04-02 05:06:26,804 [IPC Server listener on 39133] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 39133
2020-04-02 05:06:26,807 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:26,806 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:26,837 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:35149
2020-04-02 05:06:26,938 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:26,938 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:35149] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:26,946 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:26,957 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:26,960 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:26,960 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:26,961 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:26,961 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:26,963 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:26,964 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:26,964 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 35149 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:26,964 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:26,964 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 12, 12
2020-04-02 05:06:26,964 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@4b21844c] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:26,965 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@363f6148] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:26,968 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 3 2 
2020-04-02 05:06:26,969 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000012 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013
2020-04-02 05:06:26,969 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000012 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000012-0000000000000000013
2020-04-02 05:06:26,970 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:26,973 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 35149
2020-04-02 05:06:26,973 [CacheReplicationMonitor(103399920)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:26,986 [IPC Server listener on 35149] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 35149
2020-04-02 05:06:26,986 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:26,986 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:26,986 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:27,000 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:27,000 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:27,003 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@64712be{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:27,015 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@53499d85{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:27,016 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4a9cc6cb{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:27,017 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@517bd097{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:27,027 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:27,031 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:27,032 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:27,038 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:27,040 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:27,044 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:27,050 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:27,050 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:27,051 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:27,051 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:27,058 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6f70f32f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:27,058 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:27,058 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:27,061 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:27,070 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:27,070 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:27,072 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:27,073 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:27,073 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:27,073 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:27,076 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:27,076 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:27,076 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 34605
2020-04-02 05:06:27,077 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:27,079 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3aa3193a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:27,079 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@59a67c3a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:27,092 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4d63b624{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:27,094 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@466cf502{HTTP/1.1,[http/1.1]}{localhost:34605}
2020-04-02 05:06:27,191 [main] INFO  server.Server (Server.java:doStart(419)) - Started @8839ms
2020-04-02 05:06:27,193 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:27,250 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:27,251 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:27,252 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:27,252 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:27,252 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:27,253 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:27,254 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:27,257 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:27,257 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:27,258 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:27,258 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:27,259 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:27
2020-04-02 05:06:27,260 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:27,260 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:27,260 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:27,261 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:27,271 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:27,272 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:27,273 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:27,273 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:27,273 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:27,274 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:27,275 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:27,276 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:27,276 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:27,277 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:27,277 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:27,285 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:27,285 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:27,286 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:27,286 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:27,286 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:27,286 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:27,287 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:27,287 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:27,287 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:27,287 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:27,288 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:27,289 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:27,289 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:27,291 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:27,291 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:27,291 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:27,291 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:27,292 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:27,292 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:27,295 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:27,296 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:27,298 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:27,299 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:27,300 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-04-02 05:06:27,302 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 2 INodes.
2020-04-02 05:06:27,310 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:27,310 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 11 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000011
2020-04-02 05:06:27,310 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@46cc127b expecting start txid #12
2020-04-02 05:06:27,311 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000012-0000000000000000013 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:27,311 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013' to transaction ID 12
2020-04-02 05:06:27,311 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000012-0000000000000000013 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:27,311 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:27,312 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 14
2020-04-02 05:06:27,339 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:27,339 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 47 msecs
2020-04-02 05:06:27,340 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:27,341 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:27,343 [Socket Reader #1 for port 45689] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 45689
2020-04-02 05:06:27,349 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:45689 to access this namenode/service.
2020-04-02 05:06:27,350 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:27,382 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:27,383 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:27,384 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:27,384 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:27,384 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:27,388 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:27,389 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:27,389 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:27,389 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:27,389 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:27,389 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:27,392 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:27,397 [IPC Server listener on 45689] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 45689: starting
2020-04-02 05:06:27,404 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:45689
2020-04-02 05:06:27,404 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:27,405 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:27,408 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 3 milliseconds
name space=2
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:27,411 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 45689 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:27,411 [CacheReplicationMonitor(932432021)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:27,415 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:27,416 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:27,417 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:27,422 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:27,423 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:27,423 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:27,424 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:27,424 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:27,424 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:27,424 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:27,425 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:42214
2020-04-02 05:06:27,425 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:27,425 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:27,429 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:27,435 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:27,449 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:27,450 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:27,451 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:27,452 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:27,452 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:27,452 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:27,453 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 43929
2020-04-02 05:06:27,453 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:27,455 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@127e70c5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:27,455 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4108fa66{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:27,460 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2427e004{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:27,463 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5ebd56e9{HTTP/1.1,[http/1.1]}{localhost:43929}
2020-04-02 05:06:27,463 [main] INFO  server.Server (Server.java:doStart(419)) - Started @9112ms
2020-04-02 05:06:27,481 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34306
2020-04-02 05:06:27,482 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:27,482 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@641856] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:27,482 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:27,483 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:27,484 [Socket Reader #1 for port 40782] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 40782
2020-04-02 05:06:27,488 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:40782
2020-04-02 05:06:27,494 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:27,495 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:27,499 [Thread-217] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:45689 starting to offer service
2020-04-02 05:06:27,506 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:27,506 [IPC Server listener on 40782] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 40782: starting
2020-04-02 05:06:27,510 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 40782 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:27,520 [Thread-217] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:45689
2020-04-02 05:06:27,521 [Thread-217] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:27,523 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,524 [Thread-217] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:27,536 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:27,536 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:27,539 [Thread-217] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:27,583 [Thread-217] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,583 [Thread-217] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,593 [Thread-217] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,593 [Thread-217] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,595 [Thread-217] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:27,597 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:27,598 [Thread-217] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:27,600 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:27,602 [Thread-217] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:27,604 [Thread-217] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:27,606 [Thread-217] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:27,608 [Thread-217] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:27,608 [Thread-217] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:27,609 [Thread-217] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:27,613 [Thread-217] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,613 [Thread-233] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:27,613 [Thread-232] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:27,615 [Thread-233] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:27,616 [Thread-232] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:27,623 [Thread-233] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 9ms
2020-04-02 05:06:27,624 [Thread-232] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 10ms
2020-04-02 05:06:27,631 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 18ms
2020-04-02 05:06:27,632 [Thread-234] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:27,632 [Thread-235] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:27,632 [Thread-234] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:27,632 [Thread-235] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:27,632 [Thread-235] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1ms
2020-04-02 05:06:27,632 [Thread-234] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-04-02 05:06:27,634 [Thread-217] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 2ms
2020-04-02 05:06:27,635 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814396967 ms.
2020-04-02 05:06:27,635 [Thread-217] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 5:30 AM with interval of 21600000ms
2020-04-02 05:06:27,638 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814396964 ms.
2020-04-02 05:06:27,638 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:45689 beginning handshake with NN
2020-04-02 05:06:27,643 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,643 [IPC Server handler 3 on 45689] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:42214, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34306, infoSecurePort=0, ipcPort=40782, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:27,643 [IPC Server handler 3 on 45689] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:42214
2020-04-02 05:06:27,644 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:27,644 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:27,644 [IPC Server handler 3 on 45689] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:42214).
2020-04-02 05:06:27,646 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:45689 successfully registered with NN
2020-04-02 05:06:27,647 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:45689 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:27,653 [IPC Server handler 4 on 45689] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:42214
2020-04-02 05:06:27,654 [IPC Server handler 4 on 45689] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:42214
2020-04-02 05:06:27,657 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xb9e5786b050c722: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:27,658 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xb9e5786b050c722: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:42214, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34306, infoSecurePort=0, ipcPort=40782, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:06:27,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xb9e5786b050c722: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:27,659 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xb9e5786b050c722: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:42214, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34306, infoSecurePort=0, ipcPort=40782, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:27,662 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0xb9e5786b050c722,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 4 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:27,662 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:27,746 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,748 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:27,750 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,751 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:27,755 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,757 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,759 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,760 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,764 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,765 [IPC Server handler 2 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 45689, call Call#72 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41438: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,768 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,768 [IPC Server handler 3 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 45689, call Call#73 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41438: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,774 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,778 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,782 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,783 [IPC Server handler 8 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 45689, call Call#78 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
2020-04-02 05:06:27,786 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p1/.snapshot/snapshot1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,786 [IPC Server handler 9 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 45689, call Call#79 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p1":root:supergroup:drwxr-x---
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterChange
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterChange
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testSetAclSnapshotPath
[msx] perform reset as unitTestCounterInClass 1 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:27,800 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p2	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:27,803 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p2	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:27,803 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot2 for /p2
2020-04-02 05:06:27,804 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p2	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,806 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p2	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,807 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p2	dst=/p2/.snapshot/snapshot2	perm=null	proto=rpc
2020-04-02 05:06:27,808 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p2	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,813 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p2/.snapshot/snapshot2	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,819 [IPC Server handler 5 on 45689] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 5 on 45689, call Call#86 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.setAcl from 127.0.0.1:41428
org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException: Modification on a read-only snapshot is disallowed
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.setAcl(FSDirAclOp.java:135)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.setAcl(FSNamesystem.java:7221)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.setAcl(NameNodeRpcServer.java:2070)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.setAcl(ClientNamenodeProtocolServerSideTranslatorPB.java:1489)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testSetAclSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testSetAclSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclSnapshotPath
[msx] perform reset as unitTestCounterInClass 2 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:27,826 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p3	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:27,828 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p3	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:27,828 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot3 for /p3
2020-04-02 05:06:27,829 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p3	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,831 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p3	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,832 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p3	dst=/p3/.snapshot/snapshot3	perm=null	proto=rpc
2020-04-02 05:06:27,833 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p3	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,836 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p3/.snapshot/snapshot3	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,841 [IPC Server handler 3 on 45689] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 3 on 45689, call Call#93 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.removeAcl from 127.0.0.1:41428
org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException: Modification on a read-only snapshot is disallowed
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.removeAcl(FSDirAclOp.java:116)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeAcl(FSNamesystem.java:7201)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeAcl(NameNodeRpcServer.java:2064)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.removeAcl(ClientNamenodeProtocolServerSideTranslatorPB.java:1478)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterChange
[msx] perform reset as unitTestCounterInClass 3 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:27,845 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p4	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:27,846 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p4	dst=null	perm=root:supergroup:rwxrwxrwx	proto=rpc
2020-04-02 05:06:27,871 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p4/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:27,892 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p4/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:27,898 [IPC Server handler 8 on 45689] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p4/file1 is closed by DFSClient_NONMAPREDUCE_1305616824_1
2020-04-02 05:06:27,907 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p4/subdir1	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:27,909 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p4/subdir1	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:27,912 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p4/file1	dst=null	perm=root:supergroup:r-xr-x---	proto=rpc
2020-04-02 05:06:27,915 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p4/subdir1	dst=null	perm=root:supergroup:r-xr-x---	proto=rpc
2020-04-02 05:06:27,919 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,933 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,934 [IPC Server handler 4 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 45689, call Call#104 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:27,936 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,942 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,942 [IPC Server handler 7 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 45689, call Call#107 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,947 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,948 [IPC Server handler 8 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 45689, call Call#108 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,950 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot4 for /p4
2020-04-02 05:06:27,951 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,953 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p4	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,955 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p4	dst=/p4/.snapshot/snapshot4	perm=null	proto=rpc
2020-04-02 05:06:27,957 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p4	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,958 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,959 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,960 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,962 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,963 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,964 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,966 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,967 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,967 [IPC Server handler 0 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 45689, call Call#120 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:27,972 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,974 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,976 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,978 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,978 [IPC Server handler 6 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 45689, call Call#125 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,980 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,980 [IPC Server handler 5 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 45689, call Call#126 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:27,991 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p4/file1	dst=null	perm=root:supergroup:r-xrwx---	proto=rpc
2020-04-02 05:06:27,997 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p4/subdir1	dst=null	perm=root:supergroup:r-xrwx---	proto=rpc
2020-04-02 05:06:27,998 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:27,999 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,001 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,001 [IPC Server handler 1 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 45689, call Call#131 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41438: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/file1":root:supergroup:-r-xrwx---
2020-04-02 05:06:28,003 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,005 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,006 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,007 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,007 [IPC Server handler 6 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 45689, call Call#135 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41438: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:28,010 [IPC Server handler 5 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,010 [IPC Server handler 5 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 45689, call Call#136 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41438: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:28,012 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,014 [IPC Server handler 9 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,016 [IPC Server handler 0 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,017 [IPC Server handler 1 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,018 [IPC Server handler 2 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,019 [IPC Server handler 2 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 45689, call Call#142 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:28,022 [IPC Server handler 3 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,023 [IPC Server handler 4 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,024 [IPC Server handler 6 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,026 [IPC Server handler 7 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,026 [IPC Server handler 7 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 45689, call Call#147 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:28,028 [IPC Server handler 8 on 45689] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,029 [IPC Server handler 8 on 45689] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 45689, call Call#148 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:41442: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:28,030 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:28,030 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:28,030 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 40782 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:28,031 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:28,031 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@71f67a79] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:28,032 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:28,032 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:28,057 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2427e004{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:28,058 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5ebd56e9{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:28,058 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4108fa66{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:28,058 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@127e70c5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:28,059 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 40782
2020-04-02 05:06:28,061 [IPC Server listener on 40782] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 40782
2020-04-02 05:06:28,063 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:28,063 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:45689
2020-04-02 05:06:28,063 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:28,165 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:28,166 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:45689] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,177 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:28,190 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:28,198 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:28,198 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:28,199 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:28,199 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:28,203 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:28,204 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:28,204 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 45689 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:28,204 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:28,206 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 14, 38
2020-04-02 05:06:28,208 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@4b629f13] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:28,206 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@70925b45] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:28,208 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 26 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 13 Number of syncs: 27 SyncTimes(ms): 11 2 
2020-04-02 05:06:28,209 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000014 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000014-0000000000000000039
2020-04-02 05:06:28,210 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000014 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000014-0000000000000000039
2020-04-02 05:06:28,210 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:28,211 [CacheReplicationMonitor(932432021)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:28,214 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 45689
2020-04-02 05:06:28,216 [IPC Server listener on 45689] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 45689
2020-04-02 05:06:28,216 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:28,217 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:28,219 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:28,229 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:28,229 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:28,231 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4d63b624{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:28,233 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@466cf502{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:28,233 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@59a67c3a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:28,234 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3aa3193a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:28,238 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:28,241 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:28,242 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:28,247 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:28,249 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:28,255 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:28,257 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:28,257 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:28,258 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:28,258 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:28,263 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@aec50a1] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:28,263 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:28,263 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:28,265 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:28,265 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:28,266 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:28,267 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:28,268 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:28,268 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:28,268 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:28,270 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:28,270 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:28,271 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 42868
2020-04-02 05:06:28,271 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:28,273 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@173b9122{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:28,274 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7646731d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:28,278 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@499b2a5c{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:28,281 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@596df867{HTTP/1.1,[http/1.1]}{localhost:42868}
2020-04-02 05:06:28,281 [main] INFO  server.Server (Server.java:doStart(419)) - Started @9930ms
2020-04-02 05:06:28,283 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:28,283 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:28,284 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:28,284 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:28,284 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:28,284 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:28,284 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:28,285 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:28
2020-04-02 05:06:28,285 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:28,285 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:28,285 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:28,285 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:28,292 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:28,293 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:28,293 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:28,293 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:28,293 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:28,293 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:28,293 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:28,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:28,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:28,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:28,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:28,294 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:28,294 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:28,294 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:28,295 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:28,295 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:28,303 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:28,305 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:28,305 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:28,305 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:28,305 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:28,305 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:28,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:28,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:28,305 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:28,306 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:28,307 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:28,307 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:28,307 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:28,308 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:28,308 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:28,308 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:28,308 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:28,309 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:28,309 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:28,310 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:28,311 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:28,312 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:28,312 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:28,313 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-04-02 05:06:28,314 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 2 INodes.
2020-04-02 05:06:28,315 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:28,315 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 11 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000011
2020-04-02 05:06:28,315 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@285f09de expecting start txid #12
2020-04-02 05:06:28,316 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000012-0000000000000000013 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:28,316 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013' to transaction ID 12
2020-04-02 05:06:28,316 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000012-0000000000000000013, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000012-0000000000000000013 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:28,316 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@73393584 expecting start txid #14
2020-04-02 05:06:28,316 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000014-0000000000000000039, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000014-0000000000000000039 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:28,316 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000014-0000000000000000039' to transaction ID 12
2020-04-02 05:06:28,321 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000014-0000000000000000039, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000014-0000000000000000039 of size 1359 edits # 26 loaded in 0 seconds
2020-04-02 05:06:28,321 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:28,322 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 40
2020-04-02 05:06:28,333 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:28,333 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 25 msecs
2020-04-02 05:06:28,334 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:28,334 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:28,335 [Socket Reader #1 for port 44940] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 44940
2020-04-02 05:06:28,339 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:44940 to access this namenode/service.
2020-04-02 05:06:28,339 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:28,369 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:28,370 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:28,371 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:28,371 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:28,371 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:28,375 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:28,376 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:28,376 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:28,376 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:28,376 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:28,376 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:28,380 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:28,380 [IPC Server listener on 44940] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 44940: starting
2020-04-02 05:06:28,384 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:44940
2020-04-02 05:06:28,385 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:28,385 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:28,394 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 7 milliseconds
name space=7
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:28,397 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 44940 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:28,400 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:28,401 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:28,401 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:28,405 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:28,405 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:28,406 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:28,407 [CacheReplicationMonitor(1338775717)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:28,407 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:28,409 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:28,409 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:28,409 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:28,410 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:35546
2020-04-02 05:06:28,410 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:28,410 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:28,411 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:28,413 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:28,415 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:28,415 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:28,417 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:28,418 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:28,418 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:28,418 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:28,419 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36912
2020-04-02 05:06:28,419 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:28,420 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@133e019b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:28,421 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7dac3fd8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:28,426 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@54ec8cc9{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:28,427 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@52eacb4b{HTTP/1.1,[http/1.1]}{localhost:36912}
2020-04-02 05:06:28,427 [main] INFO  server.Server (Server.java:doStart(419)) - Started @10076ms
2020-04-02 05:06:28,443 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:35466
2020-04-02 05:06:28,443 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:28,443 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a551a63] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:28,444 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:28,444 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:28,445 [Socket Reader #1 for port 42675] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 42675
2020-04-02 05:06:28,454 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:42675
2020-04-02 05:06:28,459 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:28,459 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:28,460 [Thread-293] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44940 starting to offer service
2020-04-02 05:06:28,463 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:28,481 [IPC Server listener on 42675] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 42675: starting
2020-04-02 05:06:28,481 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 42675 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:28,503 [IPC Server handler 1 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,503 [Thread-293] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:44940
2020-04-02 05:06:28,505 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:28,505 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:28,505 [Thread-293] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:28,510 [Thread-293] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:28,517 [Thread-293] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:28,542 [Thread-293] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,542 [Thread-293] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,551 [Thread-293] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,551 [Thread-293] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,552 [Thread-293] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:28,554 [Thread-293] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:28,554 [Thread-293] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:28,557 [Thread-293] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:28,557 [Thread-293] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:28,559 [Thread-293] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:28,560 [Thread-293] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:28,561 [Thread-293] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:28,563 [Thread-293] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:28,574 [Thread-293] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:28,574 [Thread-293] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,575 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:28,577 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:28,579 [Thread-309] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:28,584 [Thread-308] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:28,588 [Thread-309] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 11ms
2020-04-02 05:06:28,600 [Thread-308] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 26ms
2020-04-02 05:06:28,602 [Thread-293] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 27ms
2020-04-02 05:06:28,603 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:28,603 [Thread-311] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:28,603 [Thread-310] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:28,603 [Thread-311] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:28,603 [Thread-311] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2020-04-02 05:06:28,603 [Thread-310] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms
2020-04-02 05:06:28,605 [Thread-293] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 3ms
2020-04-02 05:06:28,608 [Thread-293] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 10:25 AM with interval of 21600000ms
2020-04-02 05:06:28,608 [IPC Server handler 2 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,610 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814395992 ms.
2020-04-02 05:06:28,611 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:44940 beginning handshake with NN
2020-04-02 05:06:28,610 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814395992 ms.
2020-04-02 05:06:28,613 [IPC Server handler 4 on 44940] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:35546, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35466, infoSecurePort=0, ipcPort=42675, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:28,613 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:28,613 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:28,613 [IPC Server handler 4 on 44940] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:35546
2020-04-02 05:06:28,614 [IPC Server handler 4 on 44940] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:35546).
2020-04-02 05:06:28,619 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:44940 successfully registered with NN
2020-04-02 05:06:28,619 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:44940 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:28,623 [IPC Server handler 5 on 44940] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:35546
2020-04-02 05:06:28,623 [IPC Server handler 5 on 44940] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:35546
2020-04-02 05:06:28,630 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xccffaa0f1326e6ba: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:28,632 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xccffaa0f1326e6ba: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:35546, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35466, infoSecurePort=0, ipcPort=42675, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:28,632 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0xccffaa0f1326e6ba: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:28,633 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0xccffaa0f1326e6ba: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:35546, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=35466, infoSecurePort=0, ipcPort=42675, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:06:28,636 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0xccffaa0f1326e6ba,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 8 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:28,636 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:28,715 [IPC Server handler 8 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,715 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:28,720 [IPC Server handler 9 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,720 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:28,724 [IPC Server handler 7 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,729 [IPC Server handler 3 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,738 [IPC Server handler 0 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,738 [IPC Server handler 0 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 44940, call Call#159 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:49654: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/file1":root:supergroup:-r-xrwx---
2020-04-02 05:06:28,747 [IPC Server handler 1 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,758 [IPC Server handler 2 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,760 [IPC Server handler 4 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,763 [IPC Server handler 5 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,764 [IPC Server handler 5 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 44940, call Call#163 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:49654: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:28,769 [IPC Server handler 6 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,770 [IPC Server handler 6 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 44940, call Call#164 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:49654: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:28,774 [IPC Server handler 8 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,778 [IPC Server handler 7 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,780 [IPC Server handler 3 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,782 [IPC Server handler 0 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,783 [IPC Server handler 1 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,784 [IPC Server handler 1 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 44940, call Call#170 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:49656: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:28,787 [IPC Server handler 2 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,793 [IPC Server handler 4 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,794 [IPC Server handler 5 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,798 [IPC Server handler 8 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,798 [IPC Server handler 8 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 44940, call Call#175 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:49656: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:28,802 [IPC Server handler 9 on 44940] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:28,802 [IPC Server handler 9 on 44940] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 44940, call Call#176 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:49656: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:28,804 [main] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:06:28,805 [main] INFO  namenode.FSImage (FSImage.java:saveNamespace(1101)) - Save namespace ...
2020-04-02 05:06:28,805 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 40, 40
2020-04-02 05:06:28,807 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 5 Number of transactions batched in Syncs: 39 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:06:28,808 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000040 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000040-0000000000000000041
2020-04-02 05:06:28,809 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000040 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000040-0000000000000000041
2020-04-02 05:06:28,813 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000041 using no compression
2020-04-02 05:06:28,813 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000041 using no compression
2020-04-02 05:06:28,838 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000041 of size 1322 bytes saved in 0 seconds .
2020-04-02 05:06:28,838 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000041 of size 1322 bytes saved in 0 seconds .
2020-04-02 05:06:28,842 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 2 images with txid >= 11
2020-04-02 05:06:28,842 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-04-02 05:06:28,843 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-04-02 05:06:28,847 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 42
2020-04-02 05:06:28,857 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:saveNamespace(4419)) - New namespace image has been created
2020-04-02 05:06:28,858 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:28,858 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:28,858 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 42675 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:28,858 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:28,858 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@443dbe42] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:28,860 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:28,860 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:28,878 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@54ec8cc9{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:28,879 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@52eacb4b{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:28,879 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7dac3fd8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:28,880 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@133e019b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:28,886 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 42675
2020-04-02 05:06:28,887 [IPC Server listener on 42675] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 42675
2020-04-02 05:06:28,899 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:28,899 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:28,900 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:44940
2020-04-02 05:06:29,006 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:29,006 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:44940] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,018 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:29,026 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:29,035 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:29,035 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:29,036 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:29,036 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:29,038 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:29,039 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:29,039 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 44940 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:29,039 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:29,051 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 42, 42
2020-04-02 05:06:29,051 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@6ab72419] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:29,053 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@7957dc72] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:29,059 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:06:29,060 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000042 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043
2020-04-02 05:06:29,060 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000042 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000042-0000000000000000043
2020-04-02 05:06:29,061 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:29,061 [CacheReplicationMonitor(1338775717)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:29,068 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 44940
2020-04-02 05:06:29,069 [IPC Server listener on 44940] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 44940
2020-04-02 05:06:29,070 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:29,069 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:29,069 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:29,078 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:29,079 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:29,080 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@499b2a5c{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:29,081 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@596df867{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:29,081 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7646731d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:29,082 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@173b9122{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:29,082 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:29,085 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:29,086 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:29,091 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:29,093 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:29,097 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:29,099 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:29,099 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:29,099 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:29,100 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:29,104 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37d3d232] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:29,104 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:29,104 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:29,105 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:29,106 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:29,106 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:29,107 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:29,107 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:29,108 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:29,108 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:29,109 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:29,109 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:29,109 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 39234
2020-04-02 05:06:29,109 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:29,111 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@29caf222{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:29,111 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5851bd4f{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:29,117 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@1cfd1875{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:29,117 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@28c0b664{HTTP/1.1,[http/1.1]}{localhost:39234}
2020-04-02 05:06:29,117 [main] INFO  server.Server (Server.java:doStart(419)) - Started @10766ms
2020-04-02 05:06:29,119 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:29,120 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:29,121 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:29,121 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:29,122 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:29,122 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:29,122 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:29,122 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:29
2020-04-02 05:06:29,123 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:29,123 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:29,123 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:29,123 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:29,130 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:29,131 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:29,131 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:29,132 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:29,132 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:29,132 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:29,132 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:29,132 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:29,132 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:29,134 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:29,134 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:29,134 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:29,134 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:29,134 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:29,134 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:29,134 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:29,134 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:29,135 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:29,135 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:29,136 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:29,136 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:29,136 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:29,136 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:29,136 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:29,137 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:29,137 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:29,137 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:29,137 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:29,139 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:29,140 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:29,143 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:29,143 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:29,145 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000041, cpktTxId=0000000000000000041)
2020-04-02 05:06:29,147 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 7 INodes.
2020-04-02 05:06:29,149 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:29,149 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 41 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000041
2020-04-02 05:06:29,150 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6d1310f6 expecting start txid #42
2020-04-02 05:06:29,150 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000042-0000000000000000043 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:29,150 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043' to transaction ID 42
2020-04-02 05:06:29,150 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000042-0000000000000000043 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:29,150 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:29,151 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 44
2020-04-02 05:06:29,164 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:29,164 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 26 msecs
2020-04-02 05:06:29,165 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:29,165 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:29,166 [Socket Reader #1 for port 36954] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 36954
2020-04-02 05:06:29,171 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:36954 to access this namenode/service.
2020-04-02 05:06:29,172 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:29,204 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:29,206 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:29,206 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:29,206 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:29,207 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:29,211 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:29,218 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:29,218 [IPC Server listener on 36954] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 36954: starting
2020-04-02 05:06:29,220 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:36954
2020-04-02 05:06:29,223 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:29,223 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:29,228 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 4 milliseconds
name space=7
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:29,240 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 36954 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:29,240 [CacheReplicationMonitor(1587653704)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:29,243 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:29,246 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:29,247 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:29,248 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:29,248 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:29,248 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:29,248 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:29,252 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:29,252 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:29,252 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:29,253 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:37629
2020-04-02 05:06:29,253 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:29,254 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:29,255 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:29,256 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:29,257 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:29,257 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:29,259 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:29,260 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:29,260 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:29,260 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:29,262 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36479
2020-04-02 05:06:29,262 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:29,263 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1dfd5f51{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:29,263 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@24855019{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:29,268 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@42deb43a{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:29,275 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1deb2c43{HTTP/1.1,[http/1.1]}{localhost:36479}
2020-04-02 05:06:29,275 [main] INFO  server.Server (Server.java:doStart(419)) - Started @10924ms
2020-04-02 05:06:29,284 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:44531
2020-04-02 05:06:29,285 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:29,285 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1cefc4b3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:29,285 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:29,285 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:29,286 [Socket Reader #1 for port 37330] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 37330
2020-04-02 05:06:29,290 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:37330
2020-04-02 05:06:29,297 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:29,297 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:29,298 [Thread-367] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36954 starting to offer service
2020-04-02 05:06:29,302 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:29,302 [IPC Server listener on 37330] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 37330: starting
2020-04-02 05:06:29,308 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 37330 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:29,324 [Thread-367] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36954
2020-04-02 05:06:29,328 [Thread-367] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:29,328 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,329 [Thread-367] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:29,331 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:29,331 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:29,332 [Thread-367] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:29,342 [Thread-367] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,343 [Thread-367] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,356 [Thread-367] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,356 [Thread-367] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,357 [Thread-367] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:29,360 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:29,361 [Thread-367] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:29,364 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:29,366 [Thread-367] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:29,367 [Thread-367] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:29,371 [Thread-367] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:29,374 [Thread-367] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:29,374 [Thread-367] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:29,374 [Thread-367] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:29,385 [Thread-367] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,386 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:29,386 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:29,388 [Thread-383] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:29,388 [Thread-382] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:29,396 [Thread-382] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 10ms
2020-04-02 05:06:29,396 [Thread-383] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 10ms
2020-04-02 05:06:29,396 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 11ms
2020-04-02 05:06:29,397 [Thread-384] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:29,397 [Thread-385] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:29,397 [Thread-384] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:29,397 [Thread-385] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:29,397 [Thread-384] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-04-02 05:06:29,398 [Thread-385] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 1ms
2020-04-02 05:06:29,398 [Thread-367] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 1ms
2020-04-02 05:06:29,398 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814395204 ms.
2020-04-02 05:06:29,399 [Thread-367] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 7:31 AM with interval of 21600000ms
2020-04-02 05:06:29,399 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814395203 ms.
2020-04-02 05:06:29,406 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:36954 beginning handshake with NN
2020-04-02 05:06:29,407 [IPC Server handler 5 on 36954] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:37629, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44531, infoSecurePort=0, ipcPort=37330, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:29,407 [IPC Server handler 5 on 36954] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:37629
2020-04-02 05:06:29,407 [IPC Server handler 5 on 36954] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:37629).
2020-04-02 05:06:29,412 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:36954 successfully registered with NN
2020-04-02 05:06:29,412 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:36954 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:29,417 [IPC Server handler 2 on 36954] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:37629
2020-04-02 05:06:29,417 [IPC Server handler 2 on 36954] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:37629
2020-04-02 05:06:29,425 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x80cbcc51f84eb139: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:29,427 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x80cbcc51f84eb139: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:37629, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44531, infoSecurePort=0, ipcPort=37330, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:29,427 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x80cbcc51f84eb139: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:29,427 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x80cbcc51f84eb139: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:37629, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44531, infoSecurePort=0, ipcPort=37330, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:29,431 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x80cbcc51f84eb139,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:29,431 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:29,435 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,438 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:29,442 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,443 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:29,447 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,448 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,456 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,456 [IPC Server handler 8 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 36954, call Call#186 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/file1":root:supergroup:-r-xrwx---
2020-04-02 05:06:29,484 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,487 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,489 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,491 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,492 [IPC Server handler 2 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 36954, call Call#190 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:29,503 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,503 [IPC Server handler 6 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 36954, call Call#191 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p4/subdir1":root:supergroup:dr-xrwx---
2020-04-02 05:06:29,509 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,511 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,513 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,515 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,517 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p4/.snapshot/snapshot4/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,517 [IPC Server handler 1 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 36954, call Call#197 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:29,530 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,532 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,533 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,540 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,541 [IPC Server handler 4 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 36954, call Call#202 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:29,542 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p4/.snapshot/snapshot4/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,543 [IPC Server handler 3 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 36954, call Call#203 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p4/.snapshot/snapshot4/subdir1":root:supergroup:dr-xr-x---
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterChange
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterChange
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclEntriesSnapshotPath
[msx] perform reset as unitTestCounterInClass 4 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,552 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p5	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,562 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p5	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,563 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot5 for /p5
2020-04-02 05:06:29,564 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p5	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,566 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p5	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,568 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p5	dst=/p5/.snapshot/snapshot5	perm=null	proto=rpc
2020-04-02 05:06:29,570 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p5	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,573 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAclEntries	src=/p5/.snapshot/snapshot5	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,574 [IPC Server handler 2 on 36954] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 2 on 36954, call Call#210 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.removeAclEntries from 127.0.0.1:57668
org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException: Modification on a read-only snapshot is disallowed
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.removeAclEntries(FSDirAclOp.java:70)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeAclEntries(FSNamesystem.java:7161)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeAclEntries(NameNodeRpcServer.java:2052)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.removeAclEntries(ClientNamenodeProtocolServerSideTranslatorPB.java:1454)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclEntriesSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclEntriesSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveDefaultAclSnapshotPath
[msx] perform reset as unitTestCounterInClass 5 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,577 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p6	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,579 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p6	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,579 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot6 for /p6
2020-04-02 05:06:29,580 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p6	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,581 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p6	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,583 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p6	dst=/p6/.snapshot/snapshot6	perm=null	proto=rpc
2020-04-02 05:06:29,584 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p6	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,587 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeDefaultAcl	src=/p6/.snapshot/snapshot6	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,587 [IPC Server handler 1 on 36954] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 1 on 36954, call Call#217 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.removeDefaultAcl from 127.0.0.1:57668
org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException: Modification on a read-only snapshot is disallowed
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.removeDefaultAcl(FSDirAclOp.java:93)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.removeDefaultAcl(FSNamesystem.java:7181)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.removeDefaultAcl(NameNodeRpcServer.java:2058)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.removeDefaultAcl(ClientNamenodeProtocolServerSideTranslatorPB.java:1467)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveDefaultAclSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveDefaultAclSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveReadsCurrentState
[msx] perform reset as unitTestCounterInClass 6 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,590 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p7	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,591 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p7	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,592 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot7 for /p7
2020-04-02 05:06:29,598 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,600 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,601 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p7	dst=/p7/.snapshot/snapshot7	perm=null	proto=rpc
2020-04-02 05:06:29,602 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,606 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p7	dst=null	perm=root:supergroup:rwxrwx---	proto=rpc
2020-04-02 05:06:29,609 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p7	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,615 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,616 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,618 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,618 [IPC Server handler 0 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 36954, call Call#228 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p7":root:supergroup:drwx------
2020-04-02 05:06:29,620 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,620 [IPC Server handler 5 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 36954, call Call#229 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p7":root:supergroup:drwx------
2020-04-02 05:06:29,622 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,622 [IPC Server handler 2 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 36954, call Call#230 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p7":root:supergroup:drwx------
2020-04-02 05:06:29,627 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p7	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,627 [IPC Server handler 6 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 36954, call Call#231 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p7":root:supergroup:drwx------
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveReadsCurrentState
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveReadsCurrentState
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testChangeAclExceedsQuota
[msx] perform reset as unitTestCounterInClass 7 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,631 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p8	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,636 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p8	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,637 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p8	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,641 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setQuota	src=/p8	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,643 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p8/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:29,647 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p8/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:29,653 [IPC Server handler 0 on 36954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p8/file1 is closed by DFSClient_NONMAPREDUCE_-723953606_1
2020-04-02 05:06:29,659 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p8/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:29,662 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p8/file1	dst=null	perm=root:supergroup:rw-rw----	proto=rpc
2020-04-02 05:06:29,664 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p8	dst=/p8/.snapshot/snapshot8	perm=null	proto=rpc
2020-04-02 05:06:29,668 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p8/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,670 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p8/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,672 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p8/.snapshot/snapshot8/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,673 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p8/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,675 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p8/file1	dst=null	perm=root:supergroup:rw-r-----	proto=rpc
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testChangeAclExceedsQuota
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testChangeAclExceedsQuota
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDeDuplication
[msx] perform reset as unitTestCounterInClass 8 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,677 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p9	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,679 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,681 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,683 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p9/file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:29,685 [IPC Server handler 6 on 36954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p9/file is closed by DFSClient_NONMAPREDUCE_-723953606_1
2020-04-02 05:06:29,692 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,694 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,695 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,696 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,698 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,707 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,709 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,711 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,711 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,712 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,713 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,715 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,716 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,717 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,719 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,721 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/file	dst=null	perm=root:supergroup:rw-rwxr--	proto=rpc
2020-04-02 05:06:29,722 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,723 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,724 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,725 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,726 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,727 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,729 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9/file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:29,730 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,731 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,732 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,733 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,734 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,736 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,738 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,740 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,741 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,742 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,743 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,744 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,745 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,746 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,747 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,749 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,751 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,752 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9/sub-dir	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,754 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/file	dst=null	perm=root:supergroup:rw-rwxr--	proto=rpc
2020-04-02 05:06:29,754 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,755 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,756 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,758 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,759 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,761 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/file	dst=null	perm=root:supergroup:rw-rwxr--	proto=rpc
2020-04-02 05:06:29,762 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,764 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p9/file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:29,771 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,777 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p9/sub-dir/dir	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,779 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/sub-dir/dir	dst=null	perm=root:supergroup:rwxrwxr-x	proto=rpc
2020-04-02 05:06:29,781 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p9/sub-dir/file	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:29,783 [IPC Server handler 2 on 36954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p9/sub-dir/file is closed by DFSClient_NONMAPREDUCE_-723953606_1
2020-04-02 05:06:29,785 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p9/sub-dir/file	dst=null	perm=root:supergroup:rw-rwxr--	proto=rpc
2020-04-02 05:06:29,785 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot9 for /p9
2020-04-02 05:06:29,786 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,788 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,790 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p9	dst=/p9/.snapshot/snapshot9	perm=null	proto=rpc
2020-04-02 05:06:29,791 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p9	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,794 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=delete	src=/p9/sub-dir	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,796 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=deleteSnapshot	src=/p9/.snapshot/snapshot9	dst=null	perm=null	proto=rpc
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDeDuplication
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDeDuplication
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterRemoval
[msx] perform reset as unitTestCounterInClass 9 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:29,799 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p10	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:29,800 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p10	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,801 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p10	dst=null	perm=root:supergroup:rwxr-x---	proto=rpc
2020-04-02 05:06:29,803 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,805 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,805 [IPC Server handler 3 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 36954, call Call#313 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,807 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,807 [IPC Server handler 7 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 36954, call Call#314 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,808 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot10 for /p10
2020-04-02 05:06:29,809 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,811 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,813 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p10	dst=/p10/.snapshot/snapshot10	perm=null	proto=rpc
2020-04-02 05:06:29,821 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,822 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,824 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,825 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,826 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,828 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,830 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,830 [IPC Server handler 9 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 36954, call Call#325 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,832 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,832 [IPC Server handler 8 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 36954, call Call#326 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,833 [IPC Server handler 1 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p10	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:29,835 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,835 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,837 [IPC Server handler 2 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,838 [IPC Server handler 6 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,839 [IPC Server handler 4 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,839 [IPC Server handler 4 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 36954, call Call#332 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:29,840 [IPC Server handler 3 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,841 [IPC Server handler 3 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 36954, call Call#333 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57684: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:29,842 [IPC Server handler 7 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,842 [IPC Server handler 7 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 36954, call Call#334 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:29,843 [IPC Server handler 9 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,844 [IPC Server handler 9 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 36954, call Call#335 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:29,845 [IPC Server handler 8 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,847 [IPC Server handler 0 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,847 [IPC Server handler 0 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 36954, call Call#338 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,848 [IPC Server handler 5 on 36954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:29,849 [IPC Server handler 5 on 36954] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 36954, call Call#339 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:57686: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:29,849 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:29,849 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:29,850 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 37330 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:29,850 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:29,850 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@28a2a3e7] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:29,852 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:29,852 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:29,898 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@42deb43a{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:29,898 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1deb2c43{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:29,899 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@24855019{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:29,899 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1dfd5f51{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:29,900 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 37330
2020-04-02 05:06:29,903 [IPC Server listener on 37330] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 37330
2020-04-02 05:06:29,904 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:29,904 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:29,904 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:36954
2020-04-02 05:06:30,006 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:30,006 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:36954] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,019 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:30,027 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:30,042 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:30,042 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:30,045 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:30,045 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:30,049 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:30,049 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:30,049 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 36954 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:30,049 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:30,049 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 44, 130
2020-04-02 05:06:30,050 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@751e664e] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:30,050 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@160c3ec1] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:30,050 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 88 Total time for transactions(ms): 22 Number of transactions batched in Syncs: 45 Number of syncs: 87 SyncTimes(ms): 5 6 
2020-04-02 05:06:30,051 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000044 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000044-0000000000000000131
2020-04-02 05:06:30,054 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000044 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000044-0000000000000000131
2020-04-02 05:06:30,054 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:30,055 [CacheReplicationMonitor(1587653704)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:30,060 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 36954
2020-04-02 05:06:30,062 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:30,062 [IPC Server listener on 36954] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 36954
2020-04-02 05:06:30,064 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:30,064 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:30,078 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:30,078 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:30,080 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1cfd1875{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:30,081 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@28c0b664{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:30,082 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5851bd4f{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:30,083 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@29caf222{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:30,084 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:30,089 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:30,090 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:30,098 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:30,100 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:30,106 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:30,108 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:30,108 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:30,109 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:30,110 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:30,118 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1968a49c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:30,118 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:30,118 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,119 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:30,120 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:30,120 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,121 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:30,122 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:30,122 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:30,122 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:30,124 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:30,124 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:30,124 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 33802
2020-04-02 05:06:30,125 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:30,126 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3ef41c66{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:30,127 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@622ef26a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:30,133 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@1e287667{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:30,136 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2e6ee0bc{HTTP/1.1,[http/1.1]}{localhost:33802}
2020-04-02 05:06:30,137 [main] INFO  server.Server (Server.java:doStart(419)) - Started @11785ms
2020-04-02 05:06:30,139 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:30,139 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:30,140 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:30,142 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:30,143 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:30,143 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:30,143 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:30,144 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:30
2020-04-02 05:06:30,144 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:30,144 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,145 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:30,145 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:30,156 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:30,157 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:30,157 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:30,158 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:30,158 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:30,158 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:30,158 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:30,158 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:30,158 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,159 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:30,159 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:30,163 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:30,164 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:30,164 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:30,164 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:30,165 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:30,165 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:30,165 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:30,165 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,165 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:30,165 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:30,167 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:30,167 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:30,167 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:30,168 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:30,168 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:30,168 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:30,168 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,169 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:30,169 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:30,173 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,174 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,177 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:30,177 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:30,179 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000041, cpktTxId=0000000000000000041)
2020-04-02 05:06:30,181 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 7 INodes.
2020-04-02 05:06:30,184 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:30,184 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 41 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000041
2020-04-02 05:06:30,185 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@66b72664 expecting start txid #42
2020-04-02 05:06:30,185 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000042-0000000000000000043 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:30,185 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043' to transaction ID 42
2020-04-02 05:06:30,186 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000042-0000000000000000043, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000042-0000000000000000043 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:30,186 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@7a34b7b8 expecting start txid #44
2020-04-02 05:06:30,186 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000044-0000000000000000131, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000044-0000000000000000131 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:30,186 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000044-0000000000000000131' to transaction ID 42
2020-04-02 05:06:30,205 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000044-0000000000000000131, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000044-0000000000000000131 of size 5290 edits # 88 loaded in 0 seconds
2020-04-02 05:06:30,206 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:30,207 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 132
2020-04-02 05:06:30,239 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:30,240 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 70 msecs
2020-04-02 05:06:30,240 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:30,241 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:30,243 [Socket Reader #1 for port 46731] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 46731
2020-04-02 05:06:30,252 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:46731 to access this namenode/service.
2020-04-02 05:06:30,253 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:30,288 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:30,290 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:30,291 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:30,291 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:30,291 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:30,298 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 8 msec
2020-04-02 05:06:30,300 [IPC Server listener on 46731] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 46731: starting
2020-04-02 05:06:30,300 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:30,306 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:46731
2020-04-02 05:06:30,306 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:30,306 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:30,310 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 4 milliseconds
name space=14
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:30,313 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 46731 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:30,314 [CacheReplicationMonitor(1303608163)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:30,316 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:30,317 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:30,317 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:30,318 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:30,319 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:30,319 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:30,320 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:30,320 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:30,320 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:30,320 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:30,321 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:43028
2020-04-02 05:06:30,321 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:30,321 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:30,324 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,326 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:30,326 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:30,326 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,328 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:30,328 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:30,329 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:30,329 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:30,329 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 45970
2020-04-02 05:06:30,330 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:30,331 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@117632cf{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:30,331 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@d71adc2{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:30,336 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@49bf29c6{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:30,340 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7ee55e70{HTTP/1.1,[http/1.1]}{localhost:45970}
2020-04-02 05:06:30,355 [main] INFO  server.Server (Server.java:doStart(419)) - Started @12004ms
2020-04-02 05:06:30,377 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45335
2020-04-02 05:06:30,378 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:30,378 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7668d560] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:30,378 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:30,379 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:30,380 [Socket Reader #1 for port 42951] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 42951
2020-04-02 05:06:30,385 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:42951
2020-04-02 05:06:30,394 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:30,394 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:30,395 [Thread-445] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46731 starting to offer service
2020-04-02 05:06:30,398 [IPC Server listener on 42951] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 42951: starting
2020-04-02 05:06:30,398 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:30,404 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 42951 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:30,409 [Thread-445] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46731
2020-04-02 05:06:30,413 [Thread-445] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:30,417 [Thread-445] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,417 [IPC Server handler 1 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,418 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:30,418 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:30,419 [Thread-445] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,434 [Thread-445] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,434 [Thread-445] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,442 [Thread-445] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,443 [Thread-445] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,444 [Thread-445] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:30,445 [Thread-445] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:30,447 [Thread-445] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:30,448 [Thread-445] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:30,449 [Thread-445] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:30,450 [Thread-445] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:30,451 [Thread-445] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:30,452 [Thread-445] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:30,452 [Thread-445] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:30,452 [Thread-445] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:30,452 [Thread-445] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,452 [Thread-460] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:30,452 [Thread-461] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:30,454 [Thread-461] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:30,454 [Thread-460] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:30,465 [Thread-461] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 12ms
2020-04-02 05:06:30,470 [Thread-460] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 17ms
2020-04-02 05:06:30,470 [Thread-445] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 18ms
2020-04-02 05:06:30,470 [Thread-462] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:30,471 [Thread-463] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:30,471 [Thread-462] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:30,471 [Thread-463] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:30,471 [Thread-463] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2020-04-02 05:06:30,471 [Thread-462] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms
2020-04-02 05:06:30,473 [Thread-445] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 3ms
2020-04-02 05:06:30,475 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814394127 ms.
2020-04-02 05:06:30,475 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814394127 ms.
2020-04-02 05:06:30,476 [Thread-445] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 8:06 AM with interval of 21600000ms
2020-04-02 05:06:30,478 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46731 beginning handshake with NN
2020-04-02 05:06:30,480 [IPC Server handler 3 on 46731] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43028, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45335, infoSecurePort=0, ipcPort=42951, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:30,480 [IPC Server handler 3 on 46731] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43028
2020-04-02 05:06:30,480 [IPC Server handler 3 on 46731] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:43028).
2020-04-02 05:06:30,484 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46731 successfully registered with NN
2020-04-02 05:06:30,484 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:46731 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:30,488 [IPC Server handler 6 on 46731] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:43028
2020-04-02 05:06:30,488 [IPC Server handler 6 on 46731] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:43028
2020-04-02 05:06:30,491 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x79a23f813af36c04: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:30,492 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x79a23f813af36c04: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:43028, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45335, infoSecurePort=0, ipcPort=42951, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:06:30,492 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x79a23f813af36c04: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:30,492 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x79a23f813af36c04: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:43028, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=45335, infoSecurePort=0, ipcPort=42951, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:30,493 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x79a23f813af36c04,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 3 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:30,493 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,521 [IPC Server handler 2 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,522 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:30,527 [IPC Server handler 5 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,528 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:30,534 [IPC Server handler 8 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,536 [IPC Server handler 9 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,538 [IPC Server handler 7 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,539 [IPC Server handler 0 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,551 [IPC Server handler 1 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,552 [IPC Server handler 1 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 46731, call Call#351 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36402: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:30,554 [IPC Server handler 3 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,554 [IPC Server handler 3 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 46731, call Call#352 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36402: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:30,560 [IPC Server handler 6 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,560 [IPC Server handler 6 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 46731, call Call#353 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36406: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:30,563 [IPC Server handler 4 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,564 [IPC Server handler 4 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 46731, call Call#354 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36406: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:30,565 [IPC Server handler 2 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,568 [IPC Server handler 8 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,568 [IPC Server handler 8 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 46731, call Call#357 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36406: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:30,570 [IPC Server handler 9 on 46731] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:30,570 [IPC Server handler 9 on 46731] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 46731, call Call#358 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36406: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:30,571 [main] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:06:30,571 [main] INFO  namenode.FSImage (FSImage.java:saveNamespace(1101)) - Save namespace ...
2020-04-02 05:06:30,571 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 132, 132
2020-04-02 05:06:30,575 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 131 Number of syncs: 3 SyncTimes(ms): 5 3 
2020-04-02 05:06:30,576 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000132 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000132-0000000000000000133
2020-04-02 05:06:30,577 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000132 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000132-0000000000000000133
2020-04-02 05:06:30,580 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000133 using no compression
2020-04-02 05:06:30,581 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000133 using no compression
2020-04-02 05:06:30,591 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000133 of size 2275 bytes saved in 0 seconds .
2020-04-02 05:06:30,595 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000133 of size 2275 bytes saved in 0 seconds .
2020-04-02 05:06:30,597 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 2 images with txid >= 41
2020-04-02 05:06:30,597 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-04-02 05:06:30,598 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage_0000000000000000011, cpktTxId=0000000000000000011)
2020-04-02 05:06:30,603 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 134
2020-04-02 05:06:30,617 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:saveNamespace(4419)) - New namespace image has been created
2020-04-02 05:06:30,617 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:30,617 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:30,617 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 42951 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:30,618 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:30,618 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@79ab3a71] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:30,620 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:30,621 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:30,647 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@49bf29c6{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:30,648 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7ee55e70{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:30,648 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@d71adc2{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:30,649 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@117632cf{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:30,649 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 42951
2020-04-02 05:06:30,651 [IPC Server listener on 42951] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 42951
2020-04-02 05:06:30,651 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:30,652 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:30,654 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:46731
2020-04-02 05:06:30,758 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:30,758 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:46731] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:30,771 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:30,782 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:30,786 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:30,790 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:30,791 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:30,791 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:30,806 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:30,806 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:30,806 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 46731 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:30,807 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:30,807 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@e27ba81] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:30,807 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@54336c81] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:30,807 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 134, 134
2020-04-02 05:06:30,813 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 2 3 
2020-04-02 05:06:30,814 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000134 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135
2020-04-02 05:06:30,815 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000134 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000134-0000000000000000135
2020-04-02 05:06:30,815 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:30,818 [CacheReplicationMonitor(1303608163)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:30,825 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 46731
2020-04-02 05:06:30,828 [IPC Server listener on 46731] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 46731
2020-04-02 05:06:30,828 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:30,828 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:30,829 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:30,837 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:30,837 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:30,838 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1e287667{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:30,847 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2e6ee0bc{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:30,848 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@622ef26a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:30,848 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3ef41c66{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:30,849 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:30,856 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:30,856 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:30,864 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:30,870 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:30,874 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:30,876 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:30,877 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:30,877 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:30,878 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:30,885 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@38d5b107] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:30,885 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:30,885 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,887 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:30,887 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:30,888 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:30,889 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:30,891 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:30,891 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:30,891 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:30,892 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:30,892 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:30,892 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 45926
2020-04-02 05:06:30,892 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:30,896 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@30457e14{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:30,899 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@632aa1a3{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:30,904 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@61019f59{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:30,917 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@62e8f862{HTTP/1.1,[http/1.1]}{localhost:45926}
2020-04-02 05:06:30,918 [main] INFO  server.Server (Server.java:doStart(419)) - Started @12566ms
2020-04-02 05:06:30,920 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:30,920 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:30,921 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:30,921 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:30,921 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:30,921 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:30,921 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:30,922 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:30,922 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:30,922 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:30,922 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:30,923 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:30,923 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:30
2020-04-02 05:06:30,923 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:30,923 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,924 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:30,924 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:30,938 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:30,938 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:30,939 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:30,940 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:30,940 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,940 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:30,940 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:30,946 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:30,946 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:30,946 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:30,946 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:30,946 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:30,946 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:30,946 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:30,947 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,947 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:30,947 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:30,949 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:30,949 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:30,949 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:30,950 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:30,951 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:30,951 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:30,951 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:30,951 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:30,952 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:30,954 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,955 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:30,956 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:30,956 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:30,957 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000133, cpktTxId=0000000000000000133)
2020-04-02 05:06:30,959 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 14 INodes.
2020-04-02 05:06:30,962 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:30,962 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 133 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000133
2020-04-02 05:06:30,962 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@14c053c6 expecting start txid #134
2020-04-02 05:06:30,962 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000134-0000000000000000135 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:30,962 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135' to transaction ID 134
2020-04-02 05:06:30,963 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000134-0000000000000000135 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:30,963 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:30,964 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 136
2020-04-02 05:06:30,976 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:30,977 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 24 msecs
2020-04-02 05:06:30,977 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:30,977 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:30,982 [Socket Reader #1 for port 37171] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 37171
2020-04-02 05:06:31,009 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:37171 to access this namenode/service.
2020-04-02 05:06:31,010 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:31,040 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:31,044 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:31,044 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:31,045 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:31,045 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:31,049 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 4 msec
2020-04-02 05:06:31,053 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:31,053 [IPC Server listener on 37171] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 37171: starting
2020-04-02 05:06:31,149 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:37171
2020-04-02 05:06:31,150 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:31,150 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:31,178 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 28 milliseconds
name space=14
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:31,183 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 37171 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:31,184 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:31,184 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:31,190 [CacheReplicationMonitor(439104464)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:31,203 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:31,207 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:31,207 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:31,207 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:31,207 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:31,207 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:31,210 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:31,210 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:31,211 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:35021
2020-04-02 05:06:31,211 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:31,211 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:31,212 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:31,213 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:31,215 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:31,215 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:31,216 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:31,217 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:31,217 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:31,217 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:31,218 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 39356
2020-04-02 05:06:31,218 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:31,219 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6594402a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:31,220 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@405325cf{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:31,224 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@678040b3{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:31,225 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@17f460bb{HTTP/1.1,[http/1.1]}{localhost:39356}
2020-04-02 05:06:31,225 [main] INFO  server.Server (Server.java:doStart(419)) - Started @12874ms
2020-04-02 05:06:31,263 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:44992
2020-04-02 05:06:31,264 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:31,264 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:31,264 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:31,265 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7d2a6eac] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:31,265 [Socket Reader #1 for port 39203] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 39203
2020-04-02 05:06:31,291 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:39203
2020-04-02 05:06:31,296 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:31,297 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:31,302 [Thread-523] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37171 starting to offer service
2020-04-02 05:06:31,303 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:31,303 [IPC Server listener on 39203] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 39203: starting
2020-04-02 05:06:31,318 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 39203 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:31,377 [Thread-523] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37171
2020-04-02 05:06:31,377 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,378 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:31,378 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:31,386 [Thread-523] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:31,388 [Thread-523] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:31,390 [Thread-523] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:31,410 [Thread-523] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,410 [Thread-523] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,437 [Thread-523] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,437 [Thread-523] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,440 [Thread-523] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:31,444 [Thread-523] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:31,444 [Thread-523] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:31,445 [Thread-523] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:31,446 [Thread-523] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:31,446 [Thread-523] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:31,447 [Thread-523] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:31,448 [Thread-523] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:31,448 [Thread-523] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:31,448 [Thread-523] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:31,470 [Thread-523] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,474 [Thread-538] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:31,474 [Thread-539] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:31,481 [Thread-538] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:31,482 [Thread-539] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:31,526 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,528 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:31,528 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:31,741 [Thread-538] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 267ms
2020-04-02 05:06:31,770 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,771 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:31,771 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:31,795 [Thread-539] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 321ms
2020-04-02 05:06:31,796 [Thread-523] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 326ms
2020-04-02 05:06:31,796 [Thread-540] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:31,796 [Thread-540] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:31,796 [Thread-541] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:31,797 [Thread-541] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:31,801 [Thread-541] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 5ms
2020-04-02 05:06:31,805 [Thread-540] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 10ms
2020-04-02 05:06:31,807 [Thread-523] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 11ms
2020-04-02 05:06:31,807 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814392795 ms.
2020-04-02 05:06:31,807 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814392795 ms.
2020-04-02 05:06:31,808 [Thread-523] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 9:17 AM with interval of 21600000ms
2020-04-02 05:06:31,809 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:37171 beginning handshake with NN
2020-04-02 05:06:31,818 [IPC Server handler 8 on 37171] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:35021, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44992, infoSecurePort=0, ipcPort=39203, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:31,818 [IPC Server handler 8 on 37171] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:35021
2020-04-02 05:06:31,819 [IPC Server handler 8 on 37171] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:35021).
2020-04-02 05:06:31,834 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:37171 successfully registered with NN
2020-04-02 05:06:31,834 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:37171 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:31,838 [IPC Server handler 9 on 37171] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:35021
2020-04-02 05:06:31,838 [IPC Server handler 9 on 37171] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:35021
2020-04-02 05:06:31,840 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x75cc3c0678684f69: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:31,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x75cc3c0678684f69: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:35021, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44992, infoSecurePort=0, ipcPort=39203, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:31,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x75cc3c0678684f69: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:31,841 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x75cc3c0678684f69: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:35021, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=44992, infoSecurePort=0, ipcPort=39203, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:31,841 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x75cc3c0678684f69,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:31,842 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:31,872 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,875 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:31,882 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,883 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:31,888 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,889 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,891 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,892 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,898 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,898 [IPC Server handler 7 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 37171, call Call#372 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36664: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:31,903 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,903 [IPC Server handler 8 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 37171, call Call#373 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36664: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:31,908 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,909 [IPC Server handler 9 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 37171, call Call#374 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:31,922 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,922 [IPC Server handler 3 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 37171, call Call#375 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p10":root:supergroup:drwx------
2020-04-02 05:06:31,925 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,927 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,927 [IPC Server handler 5 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 37171, call Call#378 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
2020-04-02 05:06:31,928 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p10/.snapshot/snapshot10	dst=null	perm=null	proto=rpc
2020-04-02 05:06:31,929 [IPC Server handler 0 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 0 on 37171, call Call#379 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=EXECUTE, inode="/p10":root:supergroup:drwxr-x---
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterRemoval
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotRootAfterRemoval
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterRemoval
[msx] perform reset as unitTestCounterInClass 10 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:31,932 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p11	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:31,941 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p11	dst=null	perm=root:supergroup:rwxrwxrwx	proto=rpc
2020-04-02 05:06:31,942 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p11/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:31,976 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p11/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:31,978 [IPC Server handler 9 on 37171] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p11/file1 is closed by DFSClient_NONMAPREDUCE_606852050_1
2020-04-02 05:06:31,983 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p11/subdir1	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:31,994 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p11/subdir1	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:31,995 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p11/file1	dst=null	perm=root:supergroup:r-xr-x---	proto=rpc
2020-04-02 05:06:32,002 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setAcl	src=/p11/subdir1	dst=null	perm=root:supergroup:r-xr-x---	proto=rpc
2020-04-02 05:06:32,003 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,005 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,005 [IPC Server handler 1 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 37171, call Call#390 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:32,006 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,010 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,010 [IPC Server handler 8 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 37171, call Call#393 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,011 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,011 [IPC Server handler 9 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 37171, call Call#394 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,012 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot11 for /p11
2020-04-02 05:06:32,012 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,014 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p11	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,015 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p11	dst=/p11/.snapshot/snapshot11	perm=null	proto=rpc
2020-04-02 05:06:32,017 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p11	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,018 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,019 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,020 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,021 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,022 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,023 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,024 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,025 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,026 [IPC Server handler 6 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 37171, call Call#406 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:32,027 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,029 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,030 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,035 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,035 [IPC Server handler 2 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 37171, call Call#411 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,036 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,036 [IPC Server handler 7 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 37171, call Call#412 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,038 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p11/file1	dst=null	perm=root:supergroup:r-x------	proto=rpc
2020-04-02 05:06:32,039 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p11/subdir1	dst=null	perm=root:supergroup:r-x------	proto=rpc
2020-04-02 05:06:32,042 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,046 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,047 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,047 [IPC Server handler 4 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 37171, call Call#417 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36664: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:32,048 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,049 [IPC Server handler 5 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 37171, call Call#418 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:32,050 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,051 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,053 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,053 [IPC Server handler 2 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 37171, call Call#421 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36664: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,054 [IPC Server handler 7 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,054 [IPC Server handler 7 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 7 on 37171, call Call#422 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36664: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,056 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,057 [IPC Server handler 8 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 37171, call Call#423 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,058 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,058 [IPC Server handler 9 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 37171, call Call#424 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,059 [IPC Server handler 3 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,060 [IPC Server handler 6 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,062 [IPC Server handler 4 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,062 [IPC Server handler 5 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,063 [IPC Server handler 5 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 37171, call Call#428 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:32,066 [IPC Server handler 0 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,070 [IPC Server handler 1 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,078 [IPC Server handler 2 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,080 [IPC Server handler 8 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,080 [IPC Server handler 8 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 37171, call Call#433 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,081 [IPC Server handler 9 on 37171] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,081 [IPC Server handler 9 on 37171] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 37171, call Call#434 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:36668: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,082 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:32,082 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:32,082 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 39203 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:32,082 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:32,087 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2392212b] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:32,087 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:32,087 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:32,120 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@678040b3{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:32,128 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@17f460bb{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:32,129 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@405325cf{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:32,129 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6594402a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:32,139 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 39203
2020-04-02 05:06:32,143 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:32,143 [IPC Server listener on 39203] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 39203
2020-04-02 05:06:32,145 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:32,146 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:37171
2020-04-02 05:06:32,246 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:32,247 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:37171] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,257 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:32,270 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:32,273 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:32,273 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:32,274 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:32,274 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:32,280 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:32,281 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:32,281 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 37171 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:32,281 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:32,282 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 136, 150
2020-04-02 05:06:32,282 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@2a415aa9] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:32,282 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@55a8dc49] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:32,282 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 16 Total time for transactions(ms): 11 Number of transactions batched in Syncs: 135 Number of syncs: 17 SyncTimes(ms): 2 1 
2020-04-02 05:06:32,283 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000136 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000136-0000000000000000151
2020-04-02 05:06:32,283 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000136 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000136-0000000000000000151
2020-04-02 05:06:32,284 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:32,285 [CacheReplicationMonitor(439104464)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:32,291 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 37171
2020-04-02 05:06:32,292 [IPC Server listener on 37171] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 37171
2020-04-02 05:06:32,293 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:32,293 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:32,293 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:32,313 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:32,313 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:32,318 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@61019f59{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:32,326 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@62e8f862{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:32,326 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@632aa1a3{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:32,326 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@30457e14{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:32,327 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:32,328 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:32,328 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:32,336 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:32,337 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:32,342 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:32,353 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:32,354 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:32,354 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:32,355 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:32,359 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@737edcfa] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:32,359 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:32,359 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:32,361 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:32,361 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:32,361 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:32,362 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:32,363 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:32,363 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:32,363 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:32,365 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:32,365 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:32,365 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36544
2020-04-02 05:06:32,365 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:32,372 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@73386d72{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:32,373 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@125c082e{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:32,386 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6759f091{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:32,387 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@33a053d{HTTP/1.1,[http/1.1]}{localhost:36544}
2020-04-02 05:06:32,388 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14036ms
2020-04-02 05:06:32,390 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:32,390 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:32,390 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:32,390 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:32,390 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:32,391 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:32,391 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:32,391 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:32,391 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:32,392 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:32,392 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:32,392 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:32,392 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:32
2020-04-02 05:06:32,392 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:32,392 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:32,393 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:32,393 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:32,398 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:32,399 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:32,399 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:32,400 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:32,400 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:32,400 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:32,400 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:32,401 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:32,402 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:32,402 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:32,402 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:32,402 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:32,402 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:32,402 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:32,403 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:32,403 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:32,403 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:32,403 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:32,403 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:32,403 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:32,404 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:32,404 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:32,404 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:32,405 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:32,405 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:32,405 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:32,405 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:32,412 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:32,413 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:32,415 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:32,415 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:32,416 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000133, cpktTxId=0000000000000000133)
2020-04-02 05:06:32,417 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 14 INodes.
2020-04-02 05:06:32,420 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:32,421 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 133 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000133
2020-04-02 05:06:32,421 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@3f3c7bdb expecting start txid #134
2020-04-02 05:06:32,421 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000134-0000000000000000135 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:32,421 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135' to transaction ID 134
2020-04-02 05:06:32,421 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000134-0000000000000000135, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000134-0000000000000000135 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:32,421 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@456abb66 expecting start txid #136
2020-04-02 05:06:32,422 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000136-0000000000000000151, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000136-0000000000000000151 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:32,422 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000136-0000000000000000151' to transaction ID 134
2020-04-02 05:06:32,423 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000136-0000000000000000151, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000136-0000000000000000151 of size 863 edits # 16 loaded in 0 seconds
2020-04-02 05:06:32,423 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:32,424 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 152
2020-04-02 05:06:32,433 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:32,434 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 23 msecs
2020-04-02 05:06:32,434 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:32,434 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:32,438 [Socket Reader #1 for port 38787] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 38787
2020-04-02 05:06:32,444 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:38787 to access this namenode/service.
2020-04-02 05:06:32,445 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:32,467 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:32,468 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:32,468 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:32,468 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:32,468 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:32,472 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:32,474 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:32,474 [IPC Server listener on 38787] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 38787: starting
2020-04-02 05:06:32,477 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:38787
2020-04-02 05:06:32,505 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:32,505 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:32,552 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 48 milliseconds
name space=17
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:32,554 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 38787 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:32,569 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:32,570 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:32,574 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:32,575 [CacheReplicationMonitor(2105375946)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:32,578 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:32,578 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:32,581 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:32,581 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:32,582 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:32,582 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:32,582 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:32,583 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:43656
2020-04-02 05:06:32,583 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:32,583 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:32,584 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:32,586 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:32,587 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:32,587 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:32,589 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:32,592 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:32,593 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:32,593 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:32,594 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 34219
2020-04-02 05:06:32,594 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:32,603 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5b69d40d{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:32,603 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@15f193b8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:32,608 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@bf71cec{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:32,609 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@22d6cac2{HTTP/1.1,[http/1.1]}{localhost:34219}
2020-04-02 05:06:32,610 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14259ms
2020-04-02 05:06:32,625 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34203
2020-04-02 05:06:32,626 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:32,626 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:32,626 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1654a892] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:32,627 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:32,628 [Socket Reader #1 for port 44188] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 44188
2020-04-02 05:06:32,633 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:44188
2020-04-02 05:06:32,639 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:32,639 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:32,640 [Thread-603] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38787 starting to offer service
2020-04-02 05:06:32,640 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:32,640 [IPC Server listener on 44188] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 44188: starting
2020-04-02 05:06:32,641 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 44188 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:32,662 [Thread-603] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38787
2020-04-02 05:06:32,675 [IPC Server handler 0 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,676 [Thread-603] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:32,679 [Thread-603] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:32,681 [Thread-603] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:32,688 [Thread-603] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,688 [Thread-603] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,691 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:32,691 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:32,695 [Thread-603] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,696 [Thread-603] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,697 [Thread-603] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:32,698 [Thread-603] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:32,698 [Thread-603] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:32,702 [Thread-603] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:32,703 [Thread-603] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:32,703 [Thread-603] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:32,704 [Thread-603] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:32,705 [Thread-603] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:32,705 [Thread-603] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:32,705 [Thread-603] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:32,705 [Thread-603] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,705 [Thread-618] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:32,705 [Thread-619] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:32,706 [Thread-618] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:32,708 [Thread-619] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:32,713 [Thread-618] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 7ms
2020-04-02 05:06:32,720 [Thread-619] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 15ms
2020-04-02 05:06:32,721 [Thread-603] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 16ms
2020-04-02 05:06:32,721 [Thread-620] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:32,721 [Thread-621] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:32,721 [Thread-620] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:32,722 [Thread-621] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:32,722 [Thread-620] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 1ms
2020-04-02 05:06:32,722 [Thread-621] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2020-04-02 05:06:32,722 [Thread-603] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 1ms
2020-04-02 05:06:32,724 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814391878 ms.
2020-04-02 05:06:32,725 [Thread-603] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 8:11 AM with interval of 21600000ms
2020-04-02 05:06:32,725 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814391877 ms.
2020-04-02 05:06:32,731 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:38787 beginning handshake with NN
2020-04-02 05:06:32,734 [IPC Server handler 3 on 38787] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43656, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34203, infoSecurePort=0, ipcPort=44188, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:32,734 [IPC Server handler 3 on 38787] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43656
2020-04-02 05:06:32,735 [IPC Server handler 3 on 38787] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:43656).
2020-04-02 05:06:32,738 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:38787 successfully registered with NN
2020-04-02 05:06:32,738 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:38787 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:32,742 [IPC Server handler 2 on 38787] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:43656
2020-04-02 05:06:32,742 [IPC Server handler 2 on 38787] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:43656
2020-04-02 05:06:32,747 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x6b4835c8a454df49: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:32,747 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x6b4835c8a454df49: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:43656, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34203, infoSecurePort=0, ipcPort=44188, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:32,747 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x6b4835c8a454df49: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:32,747 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x6b4835c8a454df49: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:43656, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=34203, infoSecurePort=0, ipcPort=44188, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:32,748 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x6b4835c8a454df49,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 2 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:32,748 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:32,799 [IPC Server handler 5 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,802 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:32,806 [IPC Server handler 6 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,807 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:32,812 [IPC Server handler 7 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,813 [IPC Server handler 8 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,820 [IPC Server handler 9 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,821 [IPC Server handler 9 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 38787, call Call#444 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:43774: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:32,830 [IPC Server handler 1 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,831 [IPC Server handler 1 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 38787, call Call#445 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:32,838 [IPC Server handler 0 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,839 [IPC Server handler 3 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,871 [IPC Server handler 2 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,871 [IPC Server handler 2 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 2 on 38787, call Call#448 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:43774: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,889 [IPC Server handler 4 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,889 [IPC Server handler 4 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 38787, call Call#449 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:43774: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,894 [IPC Server handler 5 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,894 [IPC Server handler 5 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 38787, call Call#450 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,902 [IPC Server handler 6 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,902 [IPC Server handler 6 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 38787, call Call#451 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:32,903 [IPC Server handler 7 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,905 [IPC Server handler 8 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,905 [IPC Server handler 9 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,906 [IPC Server handler 1 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,906 [IPC Server handler 1 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 38787, call Call#455 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:32,907 [IPC Server handler 0 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,908 [IPC Server handler 3 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,909 [IPC Server handler 2 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,910 [IPC Server handler 5 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,910 [IPC Server handler 5 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 38787, call Call#460 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,911 [IPC Server handler 6 on 38787] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:32,912 [IPC Server handler 6 on 38787] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 38787, call Call#461 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:43776: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:32,923 [main] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:06:32,923 [main] INFO  namenode.FSImage (FSImage.java:saveNamespace(1101)) - Save namespace ...
2020-04-02 05:06:32,924 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 152, 152
2020-04-02 05:06:32,927 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 151 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:06:32,928 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000152 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000152-0000000000000000153
2020-04-02 05:06:32,928 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000152 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000152-0000000000000000153
2020-04-02 05:06:32,944 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000153 using no compression
2020-04-02 05:06:32,944 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000153 using no compression
2020-04-02 05:06:32,961 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage.ckpt_0000000000000000153 of size 2679 bytes saved in 0 seconds .
2020-04-02 05:06:32,968 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage.ckpt_0000000000000000153 of size 2679 bytes saved in 0 seconds .
2020-04-02 05:06:32,970 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 2 images with txid >= 133
2020-04-02 05:06:32,970 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000041, cpktTxId=0000000000000000041)
2020-04-02 05:06:32,970 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:purgeImage(225)) - Purging old image FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/fsimage_0000000000000000041, cpktTxId=0000000000000000041)
2020-04-02 05:06:32,975 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 154
2020-04-02 05:06:32,983 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:saveNamespace(4419)) - New namespace image has been created
2020-04-02 05:06:32,983 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:32,983 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:32,983 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 44188 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:32,983 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:32,984 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:32,984 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:32,988 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@41c89d2f] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:33,027 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@bf71cec{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:33,028 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@22d6cac2{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:33,030 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@15f193b8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:33,030 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5b69d40d{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:33,037 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 44188
2020-04-02 05:06:33,049 [IPC Server listener on 44188] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 44188
2020-04-02 05:06:33,049 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:33,049 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:33,049 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:38787
2020-04-02 05:06:33,150 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:33,150 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:38787] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,164 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:33,174 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:33,179 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:33,179 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:33,181 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:33,181 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:33,185 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:33,185 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:33,185 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 38787 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:33,185 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:33,185 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 154, 154
2020-04-02 05:06:33,185 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@771158fb] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:33,185 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@91c4a3f] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:33,186 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 0 Number of transactions batched in Syncs: 0 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:06:33,186 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000154 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000154-0000000000000000155
2020-04-02 05:06:33,187 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000154 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000154-0000000000000000155
2020-04-02 05:06:33,191 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:33,191 [CacheReplicationMonitor(2105375946)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:33,214 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 38787
2020-04-02 05:06:33,226 [IPC Server listener on 38787] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 38787
2020-04-02 05:06:33,226 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:33,228 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:33,229 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:33,238 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:33,239 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:33,240 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6759f091{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:33,254 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@33a053d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:33,254 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@125c082e{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:33,254 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@73386d72{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:33,255 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:33,267 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:33,267 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:06:33,272 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:06:33,279 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:06:33,283 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:06:33,285 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:06:33,286 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:06:33,286 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:06:33,287 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:06:33,290 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@100f9bbe] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:33,291 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:06:33,291 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:33,292 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:33,297 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:06:33,297 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:33,298 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:33,299 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:06:33,299 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:33,299 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:33,300 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:06:33,300 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:06:33,301 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 34551
2020-04-02 05:06:33,301 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:33,311 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@680362a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:33,318 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1f651cd8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:33,326 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7dc51783{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:06:33,327 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4b61d0c6{HTTP/1.1,[http/1.1]}{localhost:34551}
2020-04-02 05:06:33,327 [main] INFO  server.Server (Server.java:doStart(419)) - Started @14976ms
2020-04-02 05:06:33,335 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:06:33,336 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:06:33,337 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:06:33,337 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:33,337 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:06:33,337 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:06:33,338 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:06:33,338 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:06:33
2020-04-02 05:06:33,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:06:33,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:33,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:06:33,338 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:06:33,341 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:06:33,342 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:06:33,342 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:06:33,343 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:06:33,343 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:06:33,343 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:06:33,343 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:06:33,343 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:33,343 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:06:33,343 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:06:33,345 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? true
2020-04-02 05:06:33,345 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:06:33,345 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:06:33,345 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:06:33,345 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:06:33,345 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:06:33,346 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:06:33,346 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:33,346 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:06:33,346 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:06:33,347 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:06:33,347 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:06:33,347 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:06:33,347 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:06:33,347 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:06:33,348 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:06:33,348 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:06:33,348 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:06:33,348 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:06:33,359 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:33,360 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:33,362 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current
2020-04-02 05:06:33,362 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current
2020-04-02 05:06:33,363 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000153, cpktTxId=0000000000000000153)
2020-04-02 05:06:33,364 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 17 INodes.
2020-04-02 05:06:33,382 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:06:33,383 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 153 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/fsimage_0000000000000000153
2020-04-02 05:06:33,383 [main] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@263558c9 expecting start txid #154
2020-04-02 05:06:33,383 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000154-0000000000000000155, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000154-0000000000000000155 maxTxnsToRead = 9223372036854775807
2020-04-02 05:06:33,383 [main] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000154-0000000000000000155' to transaction ID 154
2020-04-02 05:06:33,384 [main] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000154-0000000000000000155, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000154-0000000000000000155 of size 42 edits # 2 loaded in 0 seconds
2020-04-02 05:06:33,384 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:06:33,424 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 156
2020-04-02 05:06:33,433 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:06:33,433 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 84 msecs
2020-04-02 05:06:33,434 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:06:33,435 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:33,442 [Socket Reader #1 for port 41282] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 41282
2020-04-02 05:06:33,444 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:41282 to access this namenode/service.
2020-04-02 05:06:33,445 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:06:33,473 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:06:33,490 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:06:33,490 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:06:33,491 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:06:33,491 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:06:33,496 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:06:33,499 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:33,513 [IPC Server listener on 41282] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 41282: starting
2020-04-02 05:06:33,526 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:41282
2020-04-02 05:06:33,527 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:06:33,527 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:06:33,539 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 12 milliseconds
name space=17
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:06:33,554 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 41282 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:33,554 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1,[DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:33,555 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:33,555 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:33,556 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:06:33,556 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:06:33,556 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:33,556 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:06:33,557 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:06:33,557 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:06:33,557 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:06:33,557 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:43540
2020-04-02 05:06:33,570 [CacheReplicationMonitor(588289854)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:06:33,587 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:06:33,595 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:06:33,596 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:33,598 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:06:33,602 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:06:33,602 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:06:33,603 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:06:33,604 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:06:33,604 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:06:33,604 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:06:33,604 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36207
2020-04-02 05:06:33,605 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:06:33,611 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@470a9030{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:06:33,612 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@27494e46{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:06:33,619 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3e598df9{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:06:33,620 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7e31ce0f{HTTP/1.1,[http/1.1]}{localhost:36207}
2020-04-02 05:06:33,620 [main] INFO  server.Server (Server.java:doStart(419)) - Started @15269ms
2020-04-02 05:06:33,645 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41593
2020-04-02 05:06:33,650 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:06:33,650 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:06:33,650 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:06:33,650 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3088660d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:06:33,654 [Socket Reader #1 for port 44814] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 44814
2020-04-02 05:06:33,657 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:44814
2020-04-02 05:06:33,660 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:06:33,661 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:06:33,661 [Thread-681] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:41282 starting to offer service
2020-04-02 05:06:33,673 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:06:33,673 [IPC Server listener on 44814] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 44814: starting
2020-04-02 05:06:33,676 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 44814 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:33,697 [Thread-681] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:41282
2020-04-02 05:06:33,699 [Thread-681] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:06:33,699 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,702 [Thread-681] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:33,702 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:06:33,702 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:06:33,704 [Thread-681] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/in_use.lock acquired by nodename 8103@b4bea1744e38
2020-04-02 05:06:33,712 [Thread-681] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,712 [Thread-681] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,720 [Thread-681] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,720 [Thread-681] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,721 [Thread-681] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=976002151;bpid=BP-792146049-172.17.0.8-1585803980740;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=976002151;c=1585803980740;bpid=BP-792146049-172.17.0.8-1585803980740;dnuuid=72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:33,723 [Thread-681] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b8a7c1a2-f459-4326-b870-1cd7b976f954
2020-04-02 05:06:33,725 [Thread-681] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, StorageType: DISK
2020-04-02 05:06:33,727 [Thread-681] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-b272f152-1868-4df9-9efd-758e32857e3b
2020-04-02 05:06:33,735 [Thread-681] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, StorageType: DISK
2020-04-02 05:06:33,735 [Thread-681] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:06:33,736 [Thread-681] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:33,738 [Thread-681] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1
2020-04-02 05:06:33,738 [Thread-681] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:33,738 [Thread-681] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2
2020-04-02 05:06:33,738 [Thread-681] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,742 [Thread-696] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:33,743 [Thread-697] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:33,743 [Thread-696] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:33,744 [Thread-697] INFO  impl.FsDatasetImpl (BlockPoolSlice.java:loadDfsUsed(291)) - Cached dfsUsed found for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current: 24576
2020-04-02 05:06:33,754 [Thread-696] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 12ms
2020-04-02 05:06:33,754 [Thread-697] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-792146049-172.17.0.8-1585803980740 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 11ms
2020-04-02 05:06:33,759 [Thread-681] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-792146049-172.17.0.8-1585803980740: 21ms
2020-04-02 05:06:33,766 [Thread-698] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1...
2020-04-02 05:06:33,766 [Thread-698] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:33,766 [Thread-698] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1: 0ms
2020-04-02 05:06:33,766 [Thread-699] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2...
2020-04-02 05:06:33,766 [Thread-699] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740/current/replicas doesn't exist 
2020-04-02 05:06:33,767 [Thread-699] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-792146049-172.17.0.8-1585803980740 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2: 0ms
2020-04-02 05:06:33,774 [Thread-681] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-792146049-172.17.0.8-1585803980740: 15ms
2020-04-02 05:06:33,776 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954): no suitable block pools found to scan.  Waiting 1814390826 ms.
2020-04-02 05:06:33,776 [Thread-681] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 7:18 AM with interval of 21600000ms
2020-04-02 05:06:33,778 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:41282 beginning handshake with NN
2020-04-02 05:06:33,778 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b): no suitable block pools found to scan.  Waiting 1814390824 ms.
2020-04-02 05:06:33,780 [IPC Server handler 2 on 41282] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:43540, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=41593, infoSecurePort=0, ipcPort=44814, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740) storage 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:33,780 [IPC Server handler 2 on 41282] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:43540
2020-04-02 05:06:33,780 [IPC Server handler 2 on 41282] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 72a2c15c-2d0a-420b-b5fc-42392302d006 (127.0.0.1:43540).
2020-04-02 05:06:33,794 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:41282 successfully registered with NN
2020-04-02 05:06:33,794 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:41282 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:06:33,799 [IPC Server handler 3 on 41282] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 for DN 127.0.0.1:43540
2020-04-02 05:06:33,800 [IPC Server handler 3 on 41282] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-b272f152-1868-4df9-9efd-758e32857e3b for DN 127.0.0.1:43540
2020-04-02 05:06:33,805 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x6d4f8f704953b476: Processing first storage report for DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:33,806 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x6d4f8f704953b476: from storage DS-b8a7c1a2-f459-4326-b870-1cd7b976f954 node DatanodeRegistration(127.0.0.1:43540, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=41593, infoSecurePort=0, ipcPort=44814, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:06:33,807 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x6d4f8f704953b476: Processing first storage report for DS-b272f152-1868-4df9-9efd-758e32857e3b from datanode 72a2c15c-2d0a-420b-b5fc-42392302d006
2020-04-02 05:06:33,807 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x6d4f8f704953b476: from storage DS-b272f152-1868-4df9-9efd-758e32857e3b node DatanodeRegistration(127.0.0.1:43540, datanodeUuid=72a2c15c-2d0a-420b-b5fc-42392302d006, infoPort=41593, infoSecurePort=0, ipcPort=44814, storageInfo=lv=-57;cid=testClusterID;nsid=976002151;c=1585803980740), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2020-04-02 05:06:33,808 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x6d4f8f704953b476,  containing 2 storage report(s), of which we sent 2. The reports had 0 total blocks and used 1 RPC(s). This took 0 msec to generate and 6 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:06:33,808 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:33,813 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,814 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:33,817 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,817 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:06:33,822 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,826 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,841 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,842 [IPC Server handler 9 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 41282, call Call#471 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:44640: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:33,853 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,854 [IPC Server handler 1 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 41282, call Call#472 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/file1":root:supergroup:-r-x------
2020-04-02 05:06:33,855 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,858 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,862 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,863 [IPC Server handler 3 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 3 on 41282, call Call#475 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:44640: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:33,863 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,864 [IPC Server handler 4 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 4 on 41282, call Call#476 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:44640: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:33,866 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,867 [IPC Server handler 5 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 41282, call Call#477 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:33,870 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,870 [IPC Server handler 6 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 41282, call Call#478 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/subdir1":root:supergroup:dr-x------
2020-04-02 05:06:33,873 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,876 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,880 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,881 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=open	src=/p11/.snapshot/snapshot11/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,881 [IPC Server handler 1 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 1 on 41282, call Call#482 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getBlockLocations from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/file1":root:supergroup:-r-xr-x---
2020-04-02 05:06:33,886 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,889 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,890 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,895 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,895 [IPC Server handler 5 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 5 on 41282, call Call#487 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ_EXECUTE, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
2020-04-02 05:06:33,896 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p11/.snapshot/snapshot11/subdir1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,896 [IPC Server handler 6 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 6 on 41282, call Call#488 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:44644: org.apache.hadoop.security.AccessControlException: Permission denied: user=diana, access=READ, inode="/p11/.snapshot/snapshot11/subdir1":root:supergroup:dr-xr-x---
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterRemoval
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testOriginalAclEnforcedForSnapshotContentsAfterRemoval
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDefaultAclNotCopiedToAccessAclOfNewSnapshot
[msx] perform reset as unitTestCounterInClass 11 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,899 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p12	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,900 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p12	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:33,904 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p12	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:33,905 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot12 for /p12
2020-04-02 05:06:33,906 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,907 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,908 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p12	dst=/p12/.snapshot/snapshot12	perm=null	proto=rpc
2020-04-02 05:06:33,909 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,909 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,910 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,911 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p12/.snapshot/snapshot12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,911 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p12/.snapshot/snapshot12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,912 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p12/.snapshot/snapshot12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,912 [IPC Server handler 8 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 8 on 41282, call Call#500 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.getListing from 127.0.0.1:44640: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=EXECUTE, inode="/p12":root:supergroup:drwx------
2020-04-02 05:06:33,913 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=checkAccess	src=/p12/.snapshot/snapshot12	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,913 [IPC Server handler 9 on 41282] INFO  ipc.Server (Server.java:logException(2715)) - IPC Server handler 9 on 41282, call Call#501 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.checkAccess from 127.0.0.1:44640: org.apache.hadoop.security.AccessControlException: Permission denied: user=bruce, access=EXECUTE, inode="/p12":root:supergroup:drwx------
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDefaultAclNotCopiedToAccessAclOfNewSnapshot
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testDefaultAclNotCopiedToAccessAclOfNewSnapshot
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclExceedsQuota
[msx] perform reset as unitTestCounterInClass 12 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,915 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p13	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,915 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p13	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,916 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p13	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,916 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setQuota	src=/p13	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,918 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/p13/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:06:33,923 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p13/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:33,924 [IPC Server handler 6 on 41282] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /p13/file1 is closed by DFSClient_NONMAPREDUCE_1616463848_1
2020-04-02 05:06:33,929 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p13/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
2020-04-02 05:06:33,931 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p13/file1	dst=null	perm=root:supergroup:rw-rw----	proto=rpc
2020-04-02 05:06:33,933 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p13	dst=/p13/.snapshot/snapshot13	perm=null	proto=rpc
2020-04-02 05:06:33,934 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p13/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,934 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p13/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,936 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p13/.snapshot/snapshot13/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,937 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p13/file1	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,939 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=removeAcl	src=/p13/file1	dst=null	perm=root:supergroup:rw-------	proto=rpc
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclExceedsQuota
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testRemoveAclExceedsQuota
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyAclEntriesSnapshotPath
[msx] perform reset as unitTestCounterInClass 13 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,941 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p14	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,942 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p14	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:33,942 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot14 for /p14
2020-04-02 05:06:33,943 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p14	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,944 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p14	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,944 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p14	dst=/p14/.snapshot/snapshot14	perm=null	proto=rpc
2020-04-02 05:06:33,946 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p14	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,947 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=false	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p14/.snapshot/snapshot14	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,947 [IPC Server handler 0 on 41282] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 0 on 41282, call Call#523 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.modifyAclEntries from 127.0.0.1:44636
org.apache.hadoop.hdfs.protocol.SnapshotAccessControlException: Modification on a read-only snapshot is disallowed
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkTraverse(FSDirectory.java:1822)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.resolvePath(FSDirectory.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSDirAclOp.modifyAclEntries(FSDirAclOp.java:46)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.modifyAclEntries(FSNamesystem.java:7140)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.modifyAclEntries(NameNodeRpcServer.java:2045)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.modifyAclEntries(ClientNamenodeProtocolServerSideTranslatorPB.java:1442)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyAclEntriesSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyAclEntriesSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testGetAclStatusDotSnapshotPath
[msx] perform reset as unitTestCounterInClass 14 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,949 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p15	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,949 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot15 for /p15
2020-04-02 05:06:33,951 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p15	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,951 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p15	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,952 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p15	dst=/p15/.snapshot/snapshot15	perm=null	proto=rpc
2020-04-02 05:06:33,953 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p15	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,954 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p15/.snapshot	dst=null	perm=null	proto=rpc
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testGetAclStatusDotSnapshotPath
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testGetAclStatusDotSnapshotPath
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyReadsCurrentState
[msx] perform reset as unitTestCounterInClass 15 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,956 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/p16	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:06:33,956 [IPC Server handler 9 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setPermission	src=/p16	dst=null	perm=root:supergroup:rwx------	proto=rpc
2020-04-02 05:06:33,957 [main] INFO  snapshot.SnapshotTestHelper (SnapshotTestHelper.java:createSnapshot(133)) - createSnapshot snapshot16 for /p16
2020-04-02 05:06:33,957 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,958 [IPC Server handler 0 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=allowSnapshot	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,959 [IPC Server handler 2 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=createSnapshot	src=/p16	dst=/p16/.snapshot/snapshot16	perm=null	proto=rpc
2020-04-02 05:06:33,959 [IPC Server handler 3 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=setSpaceQuota	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,960 [IPC Server handler 4 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p16	dst=null	perm=root:supergroup:rwxrwx---	proto=rpc
2020-04-02 05:06:33,961 [IPC Server handler 5 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=modifyAclEntries	src=/p16	dst=null	perm=root:supergroup:rwxrwx---	proto=rpc
2020-04-02 05:06:33,962 [IPC Server handler 6 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getAclStatus	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,963 [IPC Server handler 7 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=getfileinfo	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,965 [IPC Server handler 8 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=bruce (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p16	dst=null	perm=null	proto=rpc
2020-04-02 05:06:33,969 [IPC Server handler 1 on 41282] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=diana (auth:SIMPLE)	ip=/127.0.0.1	cmd=listStatus	src=/p16	dst=null	perm=null	proto=rpc
[msx] test Finished org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyReadsCurrentState
[msx] writeFile testName = org.apache.hadoop.hdfs.server.namenode.snapshot.TestAclWithSnapshot#testModifyReadsCurrentState
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:06:33,972 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:06:33,972 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:06:33,972 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 44814 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:33,972 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:06:33,973 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5339bbad] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:06:33,974 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1, DS-b8a7c1a2-f459-4326-b870-1cd7b976f954) exiting.
2020-04-02 05:06:33,975 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2, DS-b272f152-1868-4df9-9efd-758e32857e3b) exiting.
2020-04-02 05:06:34,015 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3e598df9{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:06:34,017 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7e31ce0f{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:34,018 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@27494e46{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:34,018 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@470a9030{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:34,021 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 44814
2020-04-02 05:06:34,023 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:34,023 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:06:34,023 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006) service to localhost/127.0.0.1:41282
2020-04-02 05:06:34,027 [IPC Server listener on 44814] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 44814
2020-04-02 05:06:34,126 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-792146049-172.17.0.8-1585803980740 (Datanode Uuid 72a2c15c-2d0a-420b-b5fc-42392302d006)
2020-04-02 05:06:34,126 [BP-792146049-172.17.0.8-1585803980740 heartbeating to localhost/127.0.0.1:41282] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-792146049-172.17.0.8-1585803980740
2020-04-02 05:06:34,137 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data1/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:34,142 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data/data2/current/BP-792146049-172.17.0.8-1585803980740] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:06:34,151 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:06:34,151 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:06:34,152 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:06:34,152 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:06:34,156 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:06:34,156 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:06:34,156 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 41282 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:06:34,156 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:34,157 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 156, 189
2020-04-02 05:06:34,157 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@39ce27f2] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:06:34,157 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 35 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 155 Number of syncs: 36 SyncTimes(ms): 6 2 
2020-04-02 05:06:34,157 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3a43d133] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:06:34,158 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_inprogress_0000000000000000156 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-1/current/edits_0000000000000000156-0000000000000000190
2020-04-02 05:06:34,159 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_inprogress_0000000000000000156 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name-0-2/current/edits_0000000000000000156-0000000000000000190
2020-04-02 05:06:34,159 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:06:34,159 [CacheReplicationMonitor(588289854)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:06:34,166 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 41282
2020-04-02 05:06:34,172 [IPC Server listener on 41282] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 41282
2020-04-02 05:06:34,172 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:06:34,172 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:06:34,176 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:06:34,183 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:06:34,184 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:06:34,186 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7dc51783{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:06:34,187 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4b61d0c6{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:06:34,188 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1f651cd8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:06:34,188 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@680362a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:06:34,189 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:06:34,193 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:06:34,194 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
[msx] all testRunFinished
