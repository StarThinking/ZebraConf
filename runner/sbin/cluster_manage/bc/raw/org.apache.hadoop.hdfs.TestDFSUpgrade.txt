[msx] before_class
2020-04-02 05:07:50,662 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:50,666 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
Formatting using clusterid: testClusterID
2020-04-02 05:07:50,696 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:50,714 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:50,715 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:50,718 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:50,725 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:50,725 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:50,725 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:50,726 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:50,786 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:50,809 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:50,810 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:50,816 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:50,816 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:50
2020-04-02 05:07:50,818 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:50,820 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:50,822 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:07:50,822 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:50,845 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:50,854 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS
2020-04-02 05:07:50,855 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:50,855 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:50,856 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 30000
2020-04-02 05:07:50,856 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 3
2020-04-02 05:07:50,856 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:50,856 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:50,857 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:50,857 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:50,857 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:50,857 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:50,884 [main] INFO  namenode.FSDirectory (SerialNumberManager.java:initialize(77)) - GLOBAL serial map: bits=24 maxEntries=16777215
2020-04-02 05:07:50,902 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:50,902 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:50,903 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:07:50,903 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:50,909 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:50,909 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:50,909 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:50,910 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:50,921 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:50,924 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:50,929 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:50,929 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:50,930 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:07:50,930 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:50,940 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:50,940 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:50,940 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:50,944 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:50,944 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:50,946 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:50,946 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:50,947 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:07:50,947 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:50,979 [main] INFO  namenode.FSImage (FSImage.java:format(170)) - Allocated new BlockPoolId: BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:50,995 [main] INFO  common.Storage (NNStorage.java:format(583)) - Storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster has been successfully formatted.
2020-04-02 05:07:51,007 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage.ckpt_0000000000000000000 using no compression
2020-04-02 05:07:51,131 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage.ckpt_0000000000000000000 of size 391 bytes saved in 0 seconds .
2020-04-02 05:07:51,146 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 1 images with txid >= 0
2020-04-02 05:07:51,153 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=1
2020-04-02 05:07:51,300 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:51,301 [main] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode []
2020-04-02 05:07:51,435 [main] INFO  beanutils.FluentPropertyBeanIntrospector (FluentPropertyBeanIntrospector.java:introspect(147)) - Error when creating PropertyDescriptor for public final void org.apache.commons.configuration2.AbstractConfiguration.setProperty(java.lang.String,java.lang.Object)! Ignoring this property.
2020-04-02 05:07:51,483 [main] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:07:51,771 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:07:51,771 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:07:51,776 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:07:51,803 [main] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:07:51,846 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4c1909a3] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:51,864 [main] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:07:51,870 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:51,884 [main] INFO  util.log (Log.java:initialized(192)) - Logging initialized @2794ms
2020-04-02 05:07:51,989 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:51,993 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:07:51,994 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:52,001 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:52,004 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:07:52,004 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:52,004 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:52,030 [main] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:07:52,031 [main] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:07:52,039 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 41237
2020-04-02 05:07:52,041 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:52,088 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@9816741{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:52,089 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@13d73f29{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:52,138 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@766653e6{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:07:52,147 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5a37d3ed{HTTP/1.1,[http/1.1]}{localhost:41237}
2020-04-02 05:07:52,148 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3058ms
2020-04-02 05:07:52,148 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:52,149 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:52,149 [main] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:52,150 [main] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:52,150 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:52,151 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:52,159 [main] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:52,159 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:52,160 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:52,160 [main] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:52,161 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:52,161 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:52,161 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:52,162 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:52,163 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:52,164 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - hadoop.configured.node.mapping is deprecated. Instead, use net.topology.configured.node.mapping
2020-04-02 05:07:52,164 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:52,164 [main] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:52,165 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:52,166 [main] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:52
2020-04-02 05:07:52,166 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:52,166 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:52,167 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.8 GB = 36.4 MB
2020-04-02 05:07:52,167 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:52,189 [main] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:52,189 [main] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:07:52,190 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:52,190 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:52,191 [main] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:07:52,191 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 1
2020-04-02 05:07:52,191 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:52,191 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:52,192 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:52,192 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:52,192 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:52,192 [main] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:52,193 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:52,193 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:52,194 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.8 GB = 18.2 MB
2020-04-02 05:07:52,194 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:52,197 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:52,197 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:52,197 [main] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:52,197 [main] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:52,198 [main] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:52,198 [main] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:52,198 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:52,198 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:52,199 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.8 GB = 4.6 MB
2020-04-02 05:07:52,199 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:52,200 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:52,200 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:52,201 [main] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:52,201 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:52,201 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:52,201 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:52,201 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:52,202 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.8 GB = 559.3 KB
2020-04-02 05:07:52,202 [main] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:52,209 [main] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:52,213 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current
2020-04-02 05:07:52,214 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(718)) - No edit log streams selected.
2020-04-02 05:07:52,214 [main] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-04-02 05:07:52,241 [main] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 1 INodes.
2020-04-02 05:07:52,265 [main] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:07:52,266 [main] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 0 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage_0000000000000000000
2020-04-02 05:07:52,270 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:07:52,271 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 1
2020-04-02 05:07:52,294 [main] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:07:52,295 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 90 msecs
2020-04-02 05:07:52,463 [main] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:07:52,474 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:52,489 [Socket Reader #1 for port 42954] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 42954
2020-04-02 05:07:52,746 [main] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:42954 to access this namenode/service.
2020-04-02 05:07:52,755 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:07:52,757 [main] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster in configuration.
2020-04-02 05:07:52,768 [main] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:07:52,783 [main] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:07:52,784 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:07:52,784 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 0 racks and 0 datanodes
2020-04-02 05:07:52,784 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:07:52,788 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 0
2020-04-02 05:07:52,789 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:07:52,789 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:07:52,789 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:07:52,789 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:07:52,789 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-04-02 05:07:52,839 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:52,839 [IPC Server listener on 42954] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 42954: starting
2020-04-02 05:07:52,841 [main] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:42954
2020-04-02 05:07:52,845 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:07:52,845 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:07:52,848 [main] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 3 milliseconds
name space=1
storage space=0
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:07:52,853 [CacheReplicationMonitor(1831679008)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:07:52,853 [main] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 42954 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:52,863 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster
2020-04-02 05:07:52,938 [main] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster
2020-04-02 05:07:52,961 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:07:52,961 [main] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:07:52,965 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:52,967 [main] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:07:52,970 [main] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:07:52,971 [main] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:52,975 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:07:52,980 [main] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:36548
2020-04-02 05:07:52,982 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:07:52,982 [main] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:07:52,995 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:52,996 [main] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:52,997 [main] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:07:52,997 [main] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:52,999 [main] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:52,999 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:07:53,000 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:53,000 [main] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:53,003 [main] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 33771
2020-04-02 05:07:53,003 [main] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:53,004 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@78fbff54{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:53,005 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7e22550a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:53,010 [main] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@4044fb95{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:07:53,011 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1688d828{HTTP/1.1,[http/1.1]}{localhost:33771}
2020-04-02 05:07:53,012 [main] INFO  server.Server (Server.java:doStart(419)) - Started @3921ms
2020-04-02 05:07:53,615 [main] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:45846
2020-04-02 05:07:53,615 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4c168660] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:53,616 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:07:53,617 [main] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:07:53,634 [main] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:53,635 [Socket Reader #1 for port 36693] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 36693
2020-04-02 05:07:53,650 [main] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:36693
2020-04-02 05:07:53,675 [main] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:07:53,678 [main] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:07:54,007 [Thread-58] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:42954 starting to offer service
2020-04-02 05:07:54,036 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:54,050 [IPC Server listener on 36693] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 36693: starting
2020-04-02 05:07:54,053 [main] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 36693 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:54,364 [Thread-58] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:42954
2020-04-02 05:07:54,374 [Thread-58] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-04-02 05:07:54,375 [Thread-58] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:54,376 [Thread-58] INFO  common.Storage (DataStorage.java:loadStorageDirectory(282)) - Storage directory with location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster is not formatted for namespace 1840904636. Formatting...
2020-04-02 05:07:54,377 [Thread-58] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-3ae460eb-233b-484a-9581-15c95ae12a12 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster 
2020-04-02 05:07:54,389 [Thread-58] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:54,389 [Thread-58] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:54,390 [Thread-58] INFO  common.Storage (BlockPoolSliceStorage.java:loadStorageDirectory(168)) - Block pool storage directory for location [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster and block pool id BP-306798092-172.17.0.14-1585804070968 is not formatted. Formatting ...
2020-04-02 05:07:54,390 [Thread-58] INFO  common.Storage (BlockPoolSliceStorage.java:format(280)) - Formatting block pool BP-306798092-172.17.0.14-1585804070968 directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster/current/BP-306798092-172.17.0.14-1585804070968/current
2020-04-02 05:07:54,392 [Thread-58] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=1840904636;bpid=BP-306798092-172.17.0.14-1585804070968;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1840904636;c=1585804070968;bpid=BP-306798092-172.17.0.14-1585804070968;dnuuid=null
2020-04-02 05:07:54,393 [Thread-58] INFO  datanode.DataNode (DataNode.java:checkDatanodeUuid(1556)) - Generated and persisted new Datanode UUID 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:54,521 [Thread-58] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-3ae460eb-233b-484a-9581-15c95ae12a12
2020-04-02 05:07:54,522 [Thread-58] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster, StorageType: DISK
2020-04-02 05:07:54,525 [Thread-58] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:07:54,532 [Thread-58] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster
2020-04-02 05:07:54,540 [Thread-58] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster
2020-04-02 05:07:54,542 [Thread-58] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:54,543 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster...
2020-04-02 05:07:54,558 [IPC Server handler 4 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:54,564 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:07:54,565 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:07:54,587 [Thread-75] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster: 43ms
2020-04-02 05:07:54,588 [Thread-58] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-306798092-172.17.0.14-1585804070968: 45ms
2020-04-02 05:07:54,593 [Thread-77] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster...
2020-04-02 05:07:54,594 [Thread-77] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:07:54,596 [Thread-77] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster: 3ms
2020-04-02 05:07:54,597 [Thread-58] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-306798092-172.17.0.14-1585804070968: 6ms
2020-04-02 05:07:54,599 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(386)) - Now scanning bpid BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster
2020-04-02 05:07:54,600 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster)] INFO  datanode.VolumeScanner (VolumeScanner.java:runLoop(544)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster, DS-3ae460eb-233b-484a-9581-15c95ae12a12): finished scanning block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:54,609 [Thread-58] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 5:32 AM with interval of 21600000ms
2020-04-02 05:07:54,616 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:42954 beginning handshake with NN
2020-04-02 05:07:54,629 [IPC Server handler 9 on 42954] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36548, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=45846, infoSecurePort=0, ipcPort=36693, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804070968) storage 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:54,631 [IPC Server handler 9 on 42954] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36548
2020-04-02 05:07:54,632 [IPC Server handler 9 on 42954] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7f97fdc0-fc66-438e-852d-d4bb971aa9c2 (127.0.0.1:36548).
2020-04-02 05:07:54,638 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:42954 successfully registered with NN
2020-04-02 05:07:54,638 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:42954 using BLOCKREPORT_INTERVAL of 21600000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:07:54,639 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster, DS-3ae460eb-233b-484a-9581-15c95ae12a12): no suitable block pools found to scan.  Waiting 1814399960 ms.
2020-04-02 05:07:54,663 [IPC Server handler 5 on 42954] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3ae460eb-233b-484a-9581-15c95ae12a12 for DN 127.0.0.1:36548
2020-04-02 05:07:54,672 [IPC Server handler 2 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:54,688 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:54,697 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x826237d5eb950cac: Processing first storage report for DS-3ae460eb-233b-484a-9581-15c95ae12a12 from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:54,699 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x826237d5eb950cac: from storage DS-3ae460eb-233b-484a-9581-15c95ae12a12 node DatanodeRegistration(127.0.0.1:36548, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=45846, infoSecurePort=0, ipcPort=36693, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804070968), blocks: 0, hasStaleStorage: false, processing time: 2 msecs, invalidatedBlocks: 0
2020-04-02 05:07:54,722 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x826237d5eb950cac,  containing 1 storage report(s), of which we sent 1. The reports had 0 total blocks and used 1 RPC(s). This took 4 msec to generate and 41 msecs for RPC and NN processing. Got back one command: FinalizeCommand/5.
2020-04-02 05:07:54,722 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BPOfferService.java:processCommandFromActive(759)) - Got finalize command for block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:54,725 [IPC Server handler 3 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=mkdirs	src=/TestUpgrade	dst=null	perm=root:supergroup:rwxr-xr-x	proto=rpc
2020-04-02 05:07:54,777 [IPC Server handler 6 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/TestUpgrade/file1	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:07:54,815 [IPC Server handler 1 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741825_1001, replicas=127.0.0.1:36548 for /TestUpgrade/file1
2020-04-02 05:07:54,887 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53074 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001 src: /127.0.0.1:53074 dest: /127.0.0.1:36548
2020-04-02 05:07:54,956 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53074, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001, duration(ns): 12915827
2020-04-02 05:07:54,957 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741825_1001, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:54,998 [IPC Server handler 0 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741826_1002, replicas=127.0.0.1:36548 for /TestUpgrade/file1
2020-04-02 05:07:55,001 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53112 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002 src: /127.0.0.1:53112 dest: /127.0.0.1:36548
2020-04-02 05:07:55,009 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53112, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002, duration(ns): 5272188
2020-04-02 05:07:55,010 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741826_1002, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,023 [IPC Server handler 5 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741827_1003, replicas=127.0.0.1:36548 for /TestUpgrade/file1
2020-04-02 05:07:55,031 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53142 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003 src: /127.0.0.1:53142 dest: /127.0.0.1:36548
2020-04-02 05:07:55,052 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53142, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003, duration(ns): 18878431
2020-04-02 05:07:55,053 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741827_1003, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,060 [IPC Server handler 2 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741828_1004, replicas=127.0.0.1:36548 for /TestUpgrade/file1
2020-04-02 05:07:55,064 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53164 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004 src: /127.0.0.1:53164 dest: /127.0.0.1:36548
2020-04-02 05:07:55,071 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53164, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004, duration(ns): 3279411
2020-04-02 05:07:55,072 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741828_1004, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,078 [IPC Server handler 6 on 42954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /TestUpgrade/file1 is closed by DFSClient_NONMAPREDUCE_1483371133_1
2020-04-02 05:07:55,087 [IPC Server handler 1 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/TestUpgrade/file2	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:07:55,092 [IPC Server handler 7 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741829_1005, replicas=127.0.0.1:36548 for /TestUpgrade/file2
2020-04-02 05:07:55,099 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53182 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005 src: /127.0.0.1:53182 dest: /127.0.0.1:36548
2020-04-02 05:07:55,135 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53182, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005, duration(ns): 8523188
2020-04-02 05:07:55,135 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741829_1005, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,143 [IPC Server handler 4 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741830_1006, replicas=127.0.0.1:36548 for /TestUpgrade/file2
2020-04-02 05:07:55,151 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53192 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006 src: /127.0.0.1:53192 dest: /127.0.0.1:36548
2020-04-02 05:07:55,163 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53192, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006, duration(ns): 8730987
2020-04-02 05:07:55,163 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741830_1006, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,167 [IPC Server handler 5 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741831_1007, replicas=127.0.0.1:36548 for /TestUpgrade/file2
2020-04-02 05:07:55,170 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53198 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007 src: /127.0.0.1:53198 dest: /127.0.0.1:36548
2020-04-02 05:07:55,189 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53198, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007, duration(ns): 15993241
2020-04-02 05:07:55,189 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741831_1007, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,195 [IPC Server handler 2 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741832_1008, replicas=127.0.0.1:36548 for /TestUpgrade/file2
2020-04-02 05:07:55,210 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53200 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008 src: /127.0.0.1:53200 dest: /127.0.0.1:36548
2020-04-02 05:07:55,219 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53200, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008, duration(ns): 6454453
2020-04-02 05:07:55,219 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741832_1008, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,230 [IPC Server handler 6 on 42954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /TestUpgrade/file2 is closed by DFSClient_NONMAPREDUCE_1483371133_1
2020-04-02 05:07:55,239 [main] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:07:55,240 [main] INFO  namenode.FSImage (FSImage.java:saveNamespace(1101)) - Save namespace ...
2020-04-02 05:07:55,240 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 1, 30
2020-04-02 05:07:55,241 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 31 Total time for transactions(ms): 17 Number of transactions batched in Syncs: 8 Number of syncs: 24 SyncTimes(ms): 4 
2020-04-02 05:07:55,242 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/edits_inprogress_0000000000000000001 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/edits_0000000000000000001-0000000000000000031
2020-04-02 05:07:55,244 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage.ckpt_0000000000000000031 using no compression
2020-04-02 05:07:55,266 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/fsimage.ckpt_0000000000000000031 of size 696 bytes saved in 0 seconds .
2020-04-02 05:07:55,281 [main] INFO  namenode.NNStorageRetentionManager (NNStorageRetentionManager.java:getImageTxIdToRetain(203)) - Going to retain 2 images with txid >= 0
2020-04-02 05:07:55,291 [main] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 32
2020-04-02 05:07:55,296 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:saveNamespace(4419)) - New namespace image has been created
2020-04-02 05:07:55,297 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 2 secs
2020-04-02 05:07:55,297 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 1 racks and 1 datanodes
2020-04-02 05:07:55,297 [main] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:07:55,300 [IPC Server handler 1 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/TestUpgrade/file3	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:07:55,304 [IPC Server handler 7 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741833_1009, replicas=127.0.0.1:36548 for /TestUpgrade/file3
2020-04-02 05:07:55,308 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53206 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009 src: /127.0.0.1:53206 dest: /127.0.0.1:36548
2020-04-02 05:07:55,313 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53206, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009, duration(ns): 3844168
2020-04-02 05:07:55,314 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741833_1009, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,319 [IPC Server handler 0 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741834_1010, replicas=127.0.0.1:36548 for /TestUpgrade/file3
2020-04-02 05:07:55,330 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53208 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010 src: /127.0.0.1:53208 dest: /127.0.0.1:36548
2020-04-02 05:07:55,347 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53208, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010, duration(ns): 14291591
2020-04-02 05:07:55,347 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741834_1010, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,350 [IPC Server handler 5 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741835_1011, replicas=127.0.0.1:36548 for /TestUpgrade/file3
2020-04-02 05:07:55,358 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53214 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011 src: /127.0.0.1:53214 dest: /127.0.0.1:36548
2020-04-02 05:07:55,385 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53214, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011, duration(ns): 20099456
2020-04-02 05:07:55,391 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741835_1011, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,397 [IPC Server handler 2 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741836_1012, replicas=127.0.0.1:36548 for /TestUpgrade/file3
2020-04-02 05:07:55,406 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53216 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012 src: /127.0.0.1:53216 dest: /127.0.0.1:36548
2020-04-02 05:07:55,443 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53216, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012, duration(ns): 20527550
2020-04-02 05:07:55,444 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741836_1012, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,448 [IPC Server handler 3 on 42954] INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2908)) - BLOCK* blk_1073741836_1012 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /TestUpgrade/file3
2020-04-02 05:07:55,851 [IPC Server handler 1 on 42954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /TestUpgrade/file3 is closed by DFSClient_NONMAPREDUCE_1483371133_1
2020-04-02 05:07:55,858 [IPC Server handler 7 on 42954] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=create	src=/TestUpgrade/file4	dst=null	perm=root:supergroup:rw-r--r--	proto=rpc
2020-04-02 05:07:55,863 [IPC Server handler 4 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741837_1013, replicas=127.0.0.1:36548 for /TestUpgrade/file4
2020-04-02 05:07:55,868 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53254 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013 src: /127.0.0.1:53254 dest: /127.0.0.1:36548
2020-04-02 05:07:55,899 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53254, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013, duration(ns): 18091475
2020-04-02 05:07:55,899 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741837_1013, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,907 [IPC Server handler 9 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741838_1014, replicas=127.0.0.1:36548 for /TestUpgrade/file4
2020-04-02 05:07:55,922 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53262 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014 src: /127.0.0.1:53262 dest: /127.0.0.1:36548
2020-04-02 05:07:55,940 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53262, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014, duration(ns): 12097953
2020-04-02 05:07:55,941 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741838_1014, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,948 [IPC Server handler 8 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741839_1015, replicas=127.0.0.1:36548 for /TestUpgrade/file4
2020-04-02 05:07:55,952 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53270 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015 src: /127.0.0.1:53270 dest: /127.0.0.1:36548
2020-04-02 05:07:55,966 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53270, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015, duration(ns): 12384456
2020-04-02 05:07:55,967 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741839_1015, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:55,978 [IPC Server handler 3 on 42954] INFO  hdfs.StateChange (FSDirWriteFileOp.java:logAllocatedBlock(804)) - BLOCK* allocate blk_1073741840_1016, replicas=127.0.0.1:36548 for /TestUpgrade/file4
2020-04-02 05:07:55,984 [DataXceiver for client DFSClient_NONMAPREDUCE_1483371133_1 at /127.0.0.1:53278 [Receiving block BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016]] INFO  datanode.DataNode (DataXceiver.java:writeBlock(738)) - Receiving BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016 src: /127.0.0.1:53278 dest: /127.0.0.1:36548
2020-04-02 05:07:56,011 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016, type=LAST_IN_PIPELINE] INFO  DataNode.clienttrace (BlockReceiver.java:finalizeBlock(1533)) - src: /127.0.0.1:53278, dest: /127.0.0.1:36548, bytes: 1024, op: HDFS_WRITE, cliID: DFSClient_NONMAPREDUCE_1483371133_1, offset: 0, srvID: 7f97fdc0-fc66-438e-852d-d4bb971aa9c2, blockid: BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016, duration(ns): 24511347
2020-04-02 05:07:56,014 [PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016, type=LAST_IN_PIPELINE] INFO  datanode.DataNode (BlockReceiver.java:run(1506)) - PacketResponder: BP-306798092-172.17.0.14-1585804070968:blk_1073741840_1016, type=LAST_IN_PIPELINE terminating
2020-04-02 05:07:56,020 [IPC Server handler 1 on 42954] INFO  namenode.FSNamesystem (FSNamesystem.java:checkBlocksComplete(2908)) - BLOCK* blk_1073741840_1016 is COMMITTED but not COMPLETE(numNodes= 0 <  minimum = 1) in file /TestUpgrade/file4
2020-04-02 05:07:56,428 [IPC Server handler 7 on 42954] INFO  hdfs.StateChange (FSNamesystem.java:completeFile(2861)) - DIR* completeFile: /TestUpgrade/file4 is closed by DFSClient_NONMAPREDUCE_1483371133_1
2020-04-02 05:07:56,429 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:07:56,429 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:07:56,429 [main] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 36693 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:56,430 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@aeab9a1] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:07:56,430 [main] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:07:56,431 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster, DS-3ae460eb-233b-484a-9581-15c95ae12a12) exiting.
2020-04-02 05:07:56,485 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@4044fb95{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:07:56,490 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1688d828{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:56,490 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7e22550a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:56,491 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@78fbff54{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:56,494 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 36693
2020-04-02 05:07:56,503 [IPC Server listener on 36693] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 36693
2020-04-02 05:07:56,504 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:56,505 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:07:56,505 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:42954
2020-04-02 05:07:56,506 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2)
2020-04-02 05:07:56,506 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:42954] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:56,527 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/datanodeMaster/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:07:56,531 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:07:56,531 [main] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:07:56,532 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:07:56,532 [main] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:07:56,545 [main] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:07:56,545 [main] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:07:56,545 [main] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 42954 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:56,545 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:56,546 [main] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 32, 60
2020-04-02 05:07:56,546 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@4097cac] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:07:56,546 [main] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 30 Total time for transactions(ms): 4 Number of transactions batched in Syncs: 10 Number of syncs: 21 SyncTimes(ms): 1 
2020-04-02 05:07:56,547 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@ec2cc4] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:07:56,547 [main] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/edits_inprogress_0000000000000000032 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/namenodeMaster/current/edits_0000000000000000032-0000000000000000061
2020-04-02 05:07:56,548 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:07:56,549 [CacheReplicationMonitor(1831679008)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:07:56,570 [main] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 42954
2020-04-02 05:07:56,571 [IPC Server listener on 42954] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 42954
2020-04-02 05:07:56,571 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:56,578 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:07:56,578 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:07:56,631 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:56,631 [main] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:07:56,653 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@766653e6{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:07:56,658 [main] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5a37d3ed{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:56,659 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@13d73f29{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:56,659 [main] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@9816741{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:56,660 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:07:56,664 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:07:56,664 [main] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
[msx] test Started org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgrade
[msx] unitTestCounterInClass = 0
2020-04-02 05:07:56,688 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:07:56,688 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 0*** Normal NameNode upgrade: numDirs=1
2020-04-02 05:07:56,729 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:07:56,730 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,731 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:07:56,736 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:07:56,738 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:07:56,738 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:07:56,739 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:07:56,739 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:07:56,746 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@79d10c3d] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:56,747 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:07:56,747 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:56,749 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:56,750 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:07:56,750 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:56,752 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:56,753 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:07:56,753 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:56,753 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:56,754 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:07:56,754 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:07:56,755 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 37873
2020-04-02 05:07:56,755 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:56,757 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3356f9a6{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:56,758 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4800ad00{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:56,763 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@8dba177{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:07:56,773 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4188e165{HTTP/1.1,[http/1.1]}{localhost:37873}
2020-04-02 05:07:56,773 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @7683ms
2020-04-02 05:07:56,774 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,774 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,774 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:56,774 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:56,775 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,775 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,776 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:56,777 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:56,778 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:56,778 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:56,778 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:56,779 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:56,779 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:56,779 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:56
2020-04-02 05:07:56,779 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:56,780 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:56,780 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:07:56,780 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:56,798 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:56,812 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:56,813 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:56,816 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:56,816 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:56,816 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:56,817 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:56,817 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:56,817 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:56,818 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:07:56,818 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:56,832 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:56,832 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:56,832 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:56,833 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:56,833 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:56,833 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:56,833 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:56,833 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:56,838 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:07:56,838 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:56,840 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:56,840 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:56,840 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:56,841 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:56,841 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:56,841 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:56,841 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:56,842 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:07:56,842 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:56,845 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:56,845 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:07:56,849 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:07:56,850 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:07:56,852 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:07:56,855 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:07:56,855 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:07:56,856 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@408309be expecting start txid #32
2020-04-02 05:07:56,856 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:07:56,857 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:07:56,882 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:07:56,883 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804076883
2020-04-02 05:07:56,884 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:56,890 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:07:56,901 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:07:56,905 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:56,909 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:56,909 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:56,918 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:07:56,922 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:07:56,928 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:07:56,929 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 86 msecs
2020-04-02 05:07:56,930 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:07:56,930 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:56,931 [Socket Reader #1 for port 37503] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 37503
2020-04-02 05:07:56,939 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:37503 to access this namenode/service.
2020-04-02 05:07:56,940 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:07:56,940 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:56,965 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:07:56,980 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:07:56,991 [IPC Server listener on 37503] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 37503: starting
2020-04-02 05:07:56,994 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:56,995 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:37503
2020-04-02 05:07:56,996 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:07:56,999 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:07:57,001 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 2 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:07:57,004 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 37503 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:57,006 [CacheReplicationMonitor(1690175251)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:07:57,029 [IPC Server handler 3 on 37503] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:57,031 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:57,040 [IPC Server handler 1 on 37503] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:07:57,047 [IPC Server handler 1 on 37503] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=safemode_enter	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:57,064 [IPC Server handler 4 on 37503] INFO  namenode.NameNode (NameNodeRpcServer.java:rollingUpgrade(1333)) - rollingUpgrade PREPARE
2020-04-02 05:07:57,066 [IPC Server handler 4 on 37503] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 4 on 37503, call Call#52 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.rollingUpgrade from 127.0.0.1:54260
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
2020-04-02 05:07:57,084 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:testUpgrade(248)) - The exception is expected.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.common.InconsistentFSStateException): Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy24.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rollingUpgrade(ClientNamenodeProtocolTranslatorPB.java:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy25.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.rollingUpgrade(DFSClient.java:2368)
	at org.apache.hadoop.hdfs.DistributedFileSystem.rollingUpgrade(DistributedFileSystem.java:1555)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:243)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:07:57,088 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:57,088 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:07:57,089 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
2020-04-02 05:07:57,090 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:07:57,090 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:07:57,090 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 37503 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:57,090 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:57,094 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3e31d580] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:07:57,094 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:07:57,095 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@7367ee4] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:07:57,096 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 1 
2020-04-02 05:07:57,097 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:07:57,097 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:07:57,100 [CacheReplicationMonitor(1690175251)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:07:57,101 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 37503
2020-04-02 05:07:57,110 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:07:57,110 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:07:57,112 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:57,113 [IPC Server listener on 37503] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 37503
2020-04-02 05:07:57,128 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:57,129 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:07:57,131 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@8dba177{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:07:57,146 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4188e165{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:57,147 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4800ad00{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:57,147 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3356f9a6{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:57,155 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:07:57,156 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:07:57,157 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:07:57,163 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:07:57,164 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 1*** Normal DataNode upgrade: numDirs=1
2020-04-02 05:07:57,214 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:07:57,215 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,216 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:07:57,225 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:07:57,227 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:07:57,228 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:07:57,228 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:07:57,229 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:07:57,240 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@19772cfc] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:57,241 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:07:57,241 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:57,243 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:57,244 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:07:57,244 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:57,246 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:57,247 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:07:57,247 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:57,247 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:57,249 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:07:57,249 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:07:57,250 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36562
2020-04-02 05:07:57,250 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:57,252 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5fa8836f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:57,253 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@231b4fe6{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:57,258 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3cd345d5{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:07:57,259 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@7dfbca40{HTTP/1.1,[http/1.1]}{localhost:36562}
2020-04-02 05:07:57,259 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @8169ms
2020-04-02 05:07:57,260 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,261 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,261 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:57,261 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:57,262 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,262 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,263 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:57,263 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:57,264 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:57,264 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:57,264 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:57,265 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:57,265 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:57,265 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:57,267 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:57,268 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:57,268 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:57,269 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:57,269 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:57
2020-04-02 05:07:57,269 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:57,269 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:57,270 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:07:57,270 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:57,307 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:57,308 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:07:57,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:57,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:57,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:07:57,308 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:57,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:57,309 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:57,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:57,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:07:57,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:57,316 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:57,317 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:57,317 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:57,317 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:57,317 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:57,317 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:57,317 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:57,318 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:57,318 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:07:57,318 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:57,320 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:57,320 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:57,320 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:57,321 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:57,321 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:57,322 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:57,322 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:57,322 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:07:57,322 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:57,325 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:57,325 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:07:57,326 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:07:57,327 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:07:57,328 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:07:57,330 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:07:57,330 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:07:57,330 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@40cf592c expecting start txid #32
2020-04-02 05:07:57,330 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:07:57,330 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:07:57,339 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:07:57,340 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804077340
2020-04-02 05:07:57,340 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:57,353 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:07:57,360 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:07:57,365 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:57,367 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:57,367 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:57,368 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:07:57,369 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:07:57,376 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:07:57,377 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 54 msecs
2020-04-02 05:07:57,377 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:07:57,378 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:57,379 [Socket Reader #1 for port 46533] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 46533
2020-04-02 05:07:57,394 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:46533 to access this namenode/service.
2020-04-02 05:07:57,395 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:07:57,396 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:57,414 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:07:57,418 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:07:57,423 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:57,423 [IPC Server listener on 46533] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 46533: starting
2020-04-02 05:07:57,454 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:46533
2020-04-02 05:07:57,454 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:07:57,454 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:07:57,455 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 0 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:07:57,490 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 46533 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:57,506 [CacheReplicationMonitor(1552802776)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:07:57,536 [IPC Server handler 0 on 46533] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:57,553 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:57,676 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:57,679 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:57,680 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:07:57,684 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:07:57,684 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:57,685 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:07:57,685 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:07:57,685 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:57,686 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:07:57,686 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:46703
2020-04-02 05:07:57,687 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:07:57,687 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:07:57,690 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:57,692 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:57,693 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:07:57,693 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:57,696 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:57,697 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:07:57,697 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:57,697 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:57,698 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 44794
2020-04-02 05:07:57,698 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:57,701 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4355107f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:57,702 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6c95f05a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:57,709 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@311636d9{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:07:57,711 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@4022fa9e{HTTP/1.1,[http/1.1]}{localhost:44794}
2020-04-02 05:07:57,711 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @8621ms
2020-04-02 05:07:57,822 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:46482
2020-04-02 05:07:57,822 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:07:57,822 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:07:57,823 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:57,823 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4ff59fdc] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:57,823 [Socket Reader #1 for port 43586] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 43586
2020-04-02 05:07:57,828 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:43586
2020-04-02 05:07:57,832 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:07:57,833 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:07:57,834 [Thread-221] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46533 starting to offer service
2020-04-02 05:07:57,836 [IPC Server listener on 43586] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 43586: starting
2020-04-02 05:07:57,836 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:57,854 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 43586 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:57,857 [IPC Server handler 1 on 46533] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:57,863 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:07:57,863 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:07:57,871 [Thread-221] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:46533
2020-04-02 05:07:57,874 [Thread-221] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-04-02 05:07:57,876 [Thread-221] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:57,888 [Thread-221] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:57,888 [Thread-221] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:57,888 [Thread-221] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804077340
2020-04-02 05:07:57,890 [Thread-221] INFO  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(472)) - loadBlockPoolSliceStorage: 1 upgrade tasks
2020-04-02 05:07:57,892 [pool-33-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:07:57,896 [pool-33-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:07:57,896 [pool-33-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:07:57,898 [pool-33-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:07:57,900 [Thread-221] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=1840904636;bpid=BP-306798092-172.17.0.14-1585804070968;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1840904636;c=1585804077340;bpid=BP-306798092-172.17.0.14-1585804070968;dnuuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:57,914 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-3ae460eb-233b-484a-9581-15c95ae12a12
2020-04-02 05:07:57,915 [Thread-221] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, StorageType: DISK
2020-04-02 05:07:57,916 [Thread-221] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:07:57,917 [Thread-221] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:57,919 [Thread-221] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:57,919 [Thread-221] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:57,920 [Thread-234] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:07:57,947 [Thread-234] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 26ms
2020-04-02 05:07:57,947 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-306798092-172.17.0.14-1585804070968: 28ms
2020-04-02 05:07:57,948 [Thread-236] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:07:57,948 [Thread-236] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:07:57,956 [Thread-236] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 8ms
2020-04-02 05:07:57,956 [Thread-221] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-306798092-172.17.0.14-1585804070968: 9ms
2020-04-02 05:07:57,966 [IPC Server handler 3 on 46533] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:57,967 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:07:57,968 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:07:57,972 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-3ae460eb-233b-484a-9581-15c95ae12a12): no suitable block pools found to scan.  Waiting 1814396627 ms.
2020-04-02 05:07:57,973 [Thread-221] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 7:50 AM with interval of 21600000ms
2020-04-02 05:07:57,975 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:46533 beginning handshake with NN
2020-04-02 05:07:57,977 [IPC Server handler 5 on 46533] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46703, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=46482, infoSecurePort=0, ipcPort=43586, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804077340) storage 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:57,978 [IPC Server handler 5 on 46533] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46703
2020-04-02 05:07:57,978 [IPC Server handler 5 on 46533] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7f97fdc0-fc66-438e-852d-d4bb971aa9c2 (127.0.0.1:46703).
2020-04-02 05:07:57,995 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:46533 successfully registered with NN
2020-04-02 05:07:58,002 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:46533 using BLOCKREPORT_INTERVAL of 10000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:07:58,009 [IPC Server handler 4 on 46533] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-3ae460eb-233b-484a-9581-15c95ae12a12 for DN 127.0.0.1:46703
2020-04-02 05:07:58,018 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x4a8458bae90bfb00: Processing first storage report for DS-3ae460eb-233b-484a-9581-15c95ae12a12 from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:58,019 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:07:58,021 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(391)) - STATE* Safe mode is OFF
2020-04-02 05:07:58,021 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:07:58,021 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 1 racks and 1 datanodes
2020-04-02 05:07:58,021 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:07:58,034 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x4a8458bae90bfb00: from storage DS-3ae460eb-233b-484a-9581-15c95ae12a12 node DatanodeRegistration(127.0.0.1:46703, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=46482, infoSecurePort=0, ipcPort=43586, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804077340), blocks: 16, hasStaleStorage: false, processing time: 15 msecs, invalidatedBlocks: 0
2020-04-02 05:07:58,044 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 16
2020-04-02 05:07:58,044 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:07:58,044 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:07:58,044 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:07:58,045 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:07:58,045 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2020-04-02 05:07:58,048 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x4a8458bae90bfb00,  containing 1 storage report(s), of which we sent 1. The reports had 16 total blocks and used 1 RPC(s). This took 1 msec to generate and 30 msecs for RPC and NN processing. Got back no commands.
2020-04-02 05:07:58,073 [IPC Server handler 7 on 46533] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:58,077 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:58,080 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:07:58,082 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:07:58,082 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 43586 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:58,083 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6eb742e] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:07:58,084 [Thread-134] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:07:58,085 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-3ae460eb-233b-484a-9581-15c95ae12a12) exiting.
2020-04-02 05:07:58,502 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@311636d9{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:07:58,507 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@4022fa9e{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:58,508 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6c95f05a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:58,509 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4355107f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:58,570 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 43586
2020-04-02 05:07:58,574 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:58,574 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:07:58,576 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:46533
2020-04-02 05:07:58,586 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2)
2020-04-02 05:07:58,586 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:46533] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:58,582 [IPC Server listener on 43586] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 43586
2020-04-02 05:07:58,607 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:07:58,612 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:07:58,620 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:07:58,622 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:07:58,623 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:07:58,626 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:07:58,626 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:07:58,627 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 46533 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:58,627 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:58,627 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:07:58,630 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@1a40df0d] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:07:58,630 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@544ce934] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:07:58,641 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 9 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 1 
2020-04-02 05:07:58,662 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:07:58,663 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:07:58,663 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 46533
2020-04-02 05:07:58,663 [CacheReplicationMonitor(1552802776)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:07:58,716 [IPC Server listener on 46533] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 46533
2020-04-02 05:07:58,716 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:58,716 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:07:58,716 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:07:58,742 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:58,742 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:07:58,744 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3cd345d5{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:07:58,755 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@7dfbca40{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:58,756 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@231b4fe6{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:58,757 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5fa8836f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:58,761 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:07:58,767 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:07:58,767 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:07:58,781 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:07:58,781 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 2*** NameNode upgrade with existing previous dir: numDirs=1
2020-04-02 05:07:58,819 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:07:58,820 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,820 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:07:58,825 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:07:58,826 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:07:58,826 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:07:58,827 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:07:58,827 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:07:58,832 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:07:58,832 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:58,832 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@714e72b8] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:58,833 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:58,834 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:07:58,834 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:58,841 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:58,841 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:07:58,842 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:58,842 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:58,843 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:07:58,843 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:07:58,844 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 35139
2020-04-02 05:07:58,844 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:58,849 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@585d4073{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:58,850 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@115b301a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:58,857 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@1b8797c5{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:07:58,858 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@493fffb3{HTTP/1.1,[http/1.1]}{localhost:35139}
2020-04-02 05:07:58,858 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @9768ms
2020-04-02 05:07:58,859 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,859 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,859 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:58,859 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:58,863 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,863 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,865 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:58,866 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:58,866 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:58,866 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:58,867 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:58,867 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:58,867 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:58,867 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:58,868 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:58,868 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:58,869 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:58,869 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:58,869 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:58
2020-04-02 05:07:58,870 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:58,870 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:58,870 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:07:58,870 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:58,884 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:58,885 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:07:58,885 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:58,885 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:58,886 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:58,887 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:58,887 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:58,887 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:58,887 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:58,888 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:07:58,888 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:58,890 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:58,890 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:58,890 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:58,890 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:58,890 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:58,890 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:58,890 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:58,891 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:58,891 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:07:58,891 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:58,892 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:58,892 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:58,892 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:58,893 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:58,893 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:58,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:58,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:58,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:07:58,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:58,899 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:58,899 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:07:58,900 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:438)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:07:58,900 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:58,902 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@1b8797c5{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:07:58,919 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@493fffb3{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:58,919 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@115b301a{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:58,920 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@585d4073{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:58,925 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:07:58,927 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:07:58,927 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:07:58,928 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:438)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:07:58,929 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:07:58,931 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:07:58,932 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:07:58,932 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 3*** DataNode upgrade with existing previous dir: numDirs=1
2020-04-02 05:07:58,958 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:07:58,958 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:58,959 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:07:58,964 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:07:58,966 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:07:58,967 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:07:58,967 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:07:58,968 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:07:58,973 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1f7d7ca1] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:58,978 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:07:58,978 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:58,980 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:58,986 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:07:58,986 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:58,988 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:58,989 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:07:58,989 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:58,989 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:58,991 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:07:58,991 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:07:58,992 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 37973
2020-04-02 05:07:58,992 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:59,000 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@28ea88eb{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:59,001 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3b481839{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:59,015 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@5e1fa9e2{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:07:59,016 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@47d7fa37{HTTP/1.1,[http/1.1]}{localhost:37973}
2020-04-02 05:07:59,016 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @9926ms
2020-04-02 05:07:59,016 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:59,017 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:59,017 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:59,017 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:07:59,017 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:59,017 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:59,018 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:07:59,019 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:07:59,019 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:07:59,019 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:07:59,019 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:07:59,020 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:07:59,020 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:07:59,020 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:07:59,021 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:59,021 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:07:59,021 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:07:59,022 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:07:59,022 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:07:59
2020-04-02 05:07:59,022 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:07:59,023 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:59,023 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:07:59,023 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:07:59,027 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:07:59,027 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:07:59,027 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:07:59,028 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:07:59,028 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:07:59,028 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:07:59,028 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:07:59,029 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:07:59,029 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:07:59,029 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:07:59,029 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:07:59,029 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:07:59,030 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:07:59,044 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:59,044 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:07:59,045 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:07:59,046 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:07:59,046 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:07:59,046 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:07:59,046 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:07:59,047 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:07:59,047 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:07:59,047 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:07:59,047 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:59,047 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:07:59,047 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:07:59,048 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:07:59,048 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:07:59,048 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:07:59,049 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:07:59,049 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:07:59,050 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:07:59,050 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:07:59,050 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:07:59,050 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:07:59,054 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:59,054 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:07:59,055 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:07:59,066 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:07:59,068 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:07:59,070 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:07:59,070 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:07:59,070 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@819a0c1 expecting start txid #32
2020-04-02 05:07:59,070 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:07:59,073 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:07:59,079 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:07:59,080 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804079080
2020-04-02 05:07:59,080 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:59,098 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:07:59,108 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:07:59,114 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:59,115 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:59,115 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:07:59,120 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:07:59,122 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:07:59,130 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:07:59,131 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 80 msecs
2020-04-02 05:07:59,131 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:07:59,131 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:59,132 [Socket Reader #1 for port 38097] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 38097
2020-04-02 05:07:59,141 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:38097 to access this namenode/service.
2020-04-02 05:07:59,143 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:07:59,143 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:07:59,153 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:07:59,158 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:07:59,167 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:59,169 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:38097
2020-04-02 05:07:59,181 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:07:59,181 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:07:59,182 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 0 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:07:59,168 [IPC Server listener on 38097] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 38097: starting
2020-04-02 05:07:59,186 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 38097 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:59,205 [CacheReplicationMonitor(1156499346)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:07:59,212 [IPC Server handler 0 on 38097] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:59,214 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:59,396 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:59,397 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:59,398 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:07:59,398 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:07:59,399 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:59,399 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:07:59,399 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:07:59,399 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:07:59,400 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:07:59,400 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:46011
2020-04-02 05:07:59,401 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:07:59,401 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:07:59,402 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:59,420 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:07:59,431 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:07:59,431 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:07:59,433 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:07:59,433 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:07:59,433 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:07:59,433 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:07:59,434 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 32883
2020-04-02 05:07:59,434 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:07:59,461 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@68a60925{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:07:59,478 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2956be8d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:07:59,483 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@50b416ee{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:07:59,484 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@32e544d{HTTP/1.1,[http/1.1]}{localhost:32883}
2020-04-02 05:07:59,485 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @10394ms
2020-04-02 05:07:59,532 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:43268
2020-04-02 05:07:59,533 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:07:59,533 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@20fd409b] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:07:59,533 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:07:59,533 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:07:59,534 [Socket Reader #1 for port 43075] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 43075
2020-04-02 05:07:59,545 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:43075
2020-04-02 05:07:59,550 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:07:59,550 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:07:59,562 [Thread-307] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38097 starting to offer service
2020-04-02 05:07:59,575 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:07:59,577 [IPC Server listener on 43075] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 43075: starting
2020-04-02 05:07:59,602 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 43075 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:59,624 [IPC Server handler 1 on 38097] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:59,630 [Thread-307] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:38097
2020-04-02 05:07:59,630 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:07:59,633 [Thread-307] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-04-02 05:07:59,635 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:07:59,636 [Thread-307] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:07:59,658 [Thread-307] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:59,659 [Thread-307] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:59,670 [Thread-307] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804079080
2020-04-02 05:07:59,671 [Thread-307] INFO  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(472)) - loadBlockPoolSliceStorage: 1 upgrade tasks
2020-04-02 05:07:59,671 [pool-50-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:07:59,674 [pool-50-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:07:59,674 [pool-50-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:07:59,676 [pool-50-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:07:59,676 [Thread-307] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=1840904636;bpid=BP-306798092-172.17.0.14-1585804070968;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1840904636;c=1585804079080;bpid=BP-306798092-172.17.0.14-1585804070968;dnuuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:59,684 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b
2020-04-02 05:07:59,694 [Thread-307] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, StorageType: DISK
2020-04-02 05:07:59,694 [Thread-307] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:07:59,696 [Thread-307] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:59,698 [Thread-307] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:07:59,698 [Thread-307] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:59,698 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:07:59,741 [Thread-320] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 42ms
2020-04-02 05:07:59,745 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-306798092-172.17.0.14-1585804070968: 48ms
2020-04-02 05:07:59,746 [IPC Server handler 3 on 38097] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:59,752 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:07:59,752 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:07:59,754 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:07:59,754 [Thread-322] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:07:59,761 [Thread-322] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 7ms
2020-04-02 05:07:59,762 [Thread-307] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-306798092-172.17.0.14-1585804070968: 16ms
2020-04-02 05:07:59,763 [Thread-307] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 9:10 AM with interval of 21600000ms
2020-04-02 05:07:59,764 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b): no suitable block pools found to scan.  Waiting 1814394835 ms.
2020-04-02 05:07:59,766 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:38097 beginning handshake with NN
2020-04-02 05:07:59,768 [IPC Server handler 4 on 38097] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:46011, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=43268, infoSecurePort=0, ipcPort=43075, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804079080) storage 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:59,768 [IPC Server handler 4 on 38097] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:46011
2020-04-02 05:07:59,769 [IPC Server handler 4 on 38097] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7f97fdc0-fc66-438e-852d-d4bb971aa9c2 (127.0.0.1:46011).
2020-04-02 05:07:59,772 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:38097 successfully registered with NN
2020-04-02 05:07:59,773 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:38097 using BLOCKREPORT_INTERVAL of 10000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:07:59,778 [IPC Server handler 5 on 38097] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b for DN 127.0.0.1:46011
2020-04-02 05:07:59,786 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x9330678f4d33803f: Processing first storage report for DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:07:59,795 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:07:59,797 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(391)) - STATE* Safe mode is OFF
2020-04-02 05:07:59,797 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:07:59,797 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 1 racks and 1 datanodes
2020-04-02 05:07:59,798 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:07:59,803 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x9330678f4d33803f: from storage DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b node DatanodeRegistration(127.0.0.1:46011, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=43268, infoSecurePort=0, ipcPort=43075, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804079080), blocks: 16, hasStaleStorage: false, processing time: 17 msecs, invalidatedBlocks: 0
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 16
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:07:59,808 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 12 msec
2020-04-02 05:07:59,814 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x9330678f4d33803f,  containing 1 storage report(s), of which we sent 1. The reports had 16 total blocks and used 1 RPC(s). This took 1 msec to generate and 30 msecs for RPC and NN processing. Got back no commands.
2020-04-02 05:07:59,859 [IPC Server handler 6 on 38097] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:07:59,863 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:07:59,867 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:07:59,867 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:07:59,872 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 43075 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:59,872 [Thread-134] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:07:59,873 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5865c4e4] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:07:59,874 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-abdbdda0-2d7d-4a42-9beb-7e77982f8c9b) exiting.
2020-04-02 05:07:59,903 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@50b416ee{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:07:59,920 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@32e544d{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:07:59,920 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2956be8d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:07:59,921 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@68a60925{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:07:59,943 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 43075
2020-04-02 05:07:59,946 [IPC Server listener on 43075] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 43075
2020-04-02 05:07:59,946 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:59,946 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:07:59,946 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:38097
2020-04-02 05:07:59,946 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2)
2020-04-02 05:07:59,947 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:38097] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:07:59,963 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:07:59,972 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:07:59,972 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:07:59,974 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:07:59,974 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:07:59,976 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:07:59,976 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:07:59,976 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 38097 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:07:59,976 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:07:59,976 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:07:59,977 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@3a9e7992] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:07:59,979 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 3 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 2 
2020-04-02 05:07:59,979 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:07:59,980 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:07:59,980 [CacheReplicationMonitor(1156499346)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:07:59,981 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 38097
2020-04-02 05:07:59,982 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@219c88bf] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:07:59,987 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:07:59,987 [IPC Server listener on 38097] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 38097
2020-04-02 05:08:00,000 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:00,005 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:00,028 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:00,029 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:00,043 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@5e1fa9e2{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:00,053 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@47d7fa37{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:00,054 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3b481839{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:00,055 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@28ea88eb{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:00,058 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:00,063 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:00,063 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:00,083 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:00,083 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 4*** DataNode upgrade with future stored layout version in current: numDirs=1
2020-04-02 05:08:00,102 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:00,103 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,103 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:00,108 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:00,110 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:00,111 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:00,111 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:00,112 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:00,126 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@221a2266] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:00,127 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:00,127 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,129 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:00,130 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:00,130 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,132 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:00,133 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:00,133 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:00,133 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:00,135 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:00,135 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:00,136 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 45508
2020-04-02 05:08:00,136 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:00,148 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@120e84ec{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:00,151 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@31b44285{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:00,157 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@14dabeca{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:00,159 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@76b6cda0{HTTP/1.1,[http/1.1]}{localhost:45508}
2020-04-02 05:08:00,159 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @11069ms
2020-04-02 05:08:00,160 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,160 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,160 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:00,160 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:00,161 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,165 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,166 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:00,167 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:00,173 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:00,174 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:00,174 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:00,174 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:00,175 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:00,175 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:00,175 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:00,176 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:00,176 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:00,177 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:00,177 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:00
2020-04-02 05:08:00,177 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:00,177 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:00,178 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:00,178 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:00,190 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:00,191 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:00,191 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:00,198 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:00,198 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:00,198 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:00,198 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:00,199 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:00,199 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:00,199 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:00,199 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:00,199 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:00,199 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:00,200 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:00,200 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:00,200 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:00,204 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:00,204 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:00,205 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:00,205 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:00,205 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:00,205 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:00,205 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:00,205 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:00,206 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:00,206 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:00,207 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:00,207 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:00,207 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:00,208 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:00,208 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:00,208 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:00,208 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:00,209 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:00,209 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:00,213 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:00,213 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:00,214 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:00,215 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:00,217 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:00,219 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:00,219 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:00,220 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@585ccf78 expecting start txid #32
2020-04-02 05:08:00,220 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:00,221 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:00,227 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:00,227 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804080227
2020-04-02 05:08:00,227 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:00,231 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:00,240 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:00,242 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:00,242 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:00,242 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:00,243 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:00,244 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:00,251 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:00,251 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 42 msecs
2020-04-02 05:08:00,252 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:00,252 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:00,253 [Socket Reader #1 for port 37611] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 37611
2020-04-02 05:08:00,261 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:37611 to access this namenode/service.
2020-04-02 05:08:00,262 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:00,263 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,279 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:00,289 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:00,296 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:00,297 [IPC Server listener on 37611] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 37611: starting
2020-04-02 05:08:00,300 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:37611
2020-04-02 05:08:00,306 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:00,306 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:00,306 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 1 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:00,323 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 37611 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:00,327 [CacheReplicationMonitor(227026288)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:00,366 [IPC Server handler 0 on 37611] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:00,370 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:00,450 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-e684fe0a-6a43-4729-bb0c-fab67bd501f9 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 
2020-04-02 05:08:00,451 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:00,452 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:00,453 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:00,454 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:00,455 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:00,455 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:00,455 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:00,455 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:00,455 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:00,456 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:41633
2020-04-02 05:08:00,456 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:00,456 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:00,457 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,458 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:00,459 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:00,459 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,460 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:00,461 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:00,461 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:00,461 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:00,462 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 39997
2020-04-02 05:08:00,462 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:00,465 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@489e1d12{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:00,466 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5fb7e471{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:00,469 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@3f7990f4{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:00,471 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5db8ec07{HTTP/1.1,[http/1.1]}{localhost:39997}
2020-04-02 05:08:00,471 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @11381ms
2020-04-02 05:08:00,494 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34574
2020-04-02 05:08:00,494 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:00,495 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:00,495 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:00,494 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6a3af85] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:00,495 [Socket Reader #1 for port 36719] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 36719
2020-04-02 05:08:00,498 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:36719
2020-04-02 05:08:00,502 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:00,502 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:00,503 [Thread-376] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37611 starting to offer service
2020-04-02 05:08:00,503 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:00,503 [IPC Server listener on 36719] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 36719: starting
2020-04-02 05:08:00,504 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 36719 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:00,518 [IPC Server handler 1 on 37611] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:00,542 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:00,542 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:00,555 [Thread-376] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37611
2020-04-02 05:08:00,557 [Thread-376] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-04-02 05:08:00,559 [Thread-376] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:00,560 [Thread-376] WARN  common.Storage (DataStorage.java:loadDataStorage(418)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1. Reported: -2147483648. Expecting = -57.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:616)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:605)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:134)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:294)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:407)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:387)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:00,561 [Thread-376] ERROR datanode.DataNode (BPServiceActor.java:run(829)) - Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37611. Exiting. 
java.io.IOException: All specified directories have failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:552)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:00,561 [Thread-376] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:37611
2020-04-02 05:08:00,561 [Thread-376] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool <registering> (Datanode Uuid unassigned)
2020-04-02 05:08:00,650 [IPC Server handler 2 on 37611] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:00,651 [Thread-134] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2694)) - BPOfferService in datanode DataNode{data=null, localName='127.0.0.1:41633', datanodeUuid='null', xmitsInProgress=0} failed to connect to namenode at localhost/127.0.0.1:37611
2020-04-02 05:08:00,651 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:00,651 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:00,651 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:00,651 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 36719 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:00,652 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@3b185f9f] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:00,764 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@3f7990f4{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:00,765 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5db8ec07{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:00,765 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5fb7e471{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:00,766 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@489e1d12{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:00,771 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 36719
2020-04-02 05:08:00,774 [IPC Server listener on 36719] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 36719
2020-04-02 05:08:00,775 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:00,776 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:00,776 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:00,776 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 37611 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:00,776 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:00,776 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:00,776 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@4ca858d] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:00,776 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@b07c390] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:00,778 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 2 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 2 
2020-04-02 05:08:00,778 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:00,779 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:00,780 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 37611
2020-04-02 05:08:00,783 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:00,783 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:00,783 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:00,786 [CacheReplicationMonitor(227026288)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:00,787 [IPC Server listener on 37611] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 37611
2020-04-02 05:08:00,803 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:00,806 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:00,814 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@14dabeca{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:00,825 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@76b6cda0{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:00,826 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@31b44285{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:00,828 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@120e84ec{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:00,858 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:00,860 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:00,860 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:00,883 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:00,884 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 5*** DataNode upgrade with newer fsscTime in current: numDirs=1
2020-04-02 05:08:00,902 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:00,902 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,903 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:00,906 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:00,918 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:00,918 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:00,918 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:00,919 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:00,941 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2aeeea59] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:00,942 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:00,943 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,944 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:00,946 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:00,946 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:00,947 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:00,952 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:00,956 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:00,956 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:00,958 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:00,958 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:00,959 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 43332
2020-04-02 05:08:00,959 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:00,964 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3e446e77{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:00,980 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6f8c1600{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:00,986 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@39bd5920{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:00,988 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3d83c3d6{HTTP/1.1,[http/1.1]}{localhost:43332}
2020-04-02 05:08:00,988 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @11898ms
2020-04-02 05:08:00,990 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,991 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,992 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:00,992 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:00,992 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,992 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:00,993 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:00,994 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:00,994 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:00,999 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:01,000 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:01,000 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:01,000 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:01,001 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:01
2020-04-02 05:08:01,001 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:01,001 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:01,001 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:01,001 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:01,008 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:01,008 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:01,008 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:01,008 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:01,008 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:01,009 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:01,010 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:01,010 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:01,010 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:01,010 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:01,012 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:01,012 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:01,012 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:01,012 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:01,012 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:01,012 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:01,012 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:01,012 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:01,013 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:01,013 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:01,014 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:01,014 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:01,014 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:01,015 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:01,015 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:01,015 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:01,015 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:01,016 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:01,016 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:01,033 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:01,033 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:01,034 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:01,035 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:01,036 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:01,036 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:01,036 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:01,036 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@252b545 expecting start txid #32
2020-04-02 05:08:01,037 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:01,037 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:01,042 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:01,043 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804081043
2020-04-02 05:08:01,043 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:01,054 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:01,067 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:01,074 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:01,075 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:01,075 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:01,076 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:01,077 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:01,102 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:01,103 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 86 msecs
2020-04-02 05:08:01,103 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:01,103 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:01,108 [Socket Reader #1 for port 35890] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 35890
2020-04-02 05:08:01,120 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:35890 to access this namenode/service.
2020-04-02 05:08:01,121 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:01,121 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:01,149 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:01,158 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:01,171 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:01,171 [IPC Server listener on 35890] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 35890: starting
2020-04-02 05:08:01,201 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:35890
2020-04-02 05:08:01,202 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:01,202 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:01,220 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 19 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:01,242 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 35890 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:01,243 [CacheReplicationMonitor(1896704107)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:01,262 [IPC Server handler 0 on 35890] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:01,269 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:01,351 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-2d2006ed-b6a0-4968-9077-cff362961ee4 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 
2020-04-02 05:08:01,353 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:01,354 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:01,356 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:01,359 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:01,359 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:01,359 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:01,360 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:01,360 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:01,361 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:01,362 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:36132
2020-04-02 05:08:01,362 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:01,362 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:01,363 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:01,365 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:01,367 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:01,367 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:01,368 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:01,369 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:01,370 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:01,370 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:01,370 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 40767
2020-04-02 05:08:01,371 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:01,372 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@594d400f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:01,373 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@14f16c0f{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:01,395 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@73fea7ff{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:01,396 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@759eac91{HTTP/1.1,[http/1.1]}{localhost:40767}
2020-04-02 05:08:01,396 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @12306ms
2020-04-02 05:08:01,432 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:41059
2020-04-02 05:08:01,433 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:01,433 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@400c81e9] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:01,433 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:01,434 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:01,435 [Socket Reader #1 for port 38847] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 38847
2020-04-02 05:08:01,439 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:38847
2020-04-02 05:08:01,460 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:01,464 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:01,465 [Thread-441] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35890 starting to offer service
2020-04-02 05:08:01,469 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:01,474 [IPC Server listener on 38847] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 38847: starting
2020-04-02 05:08:01,492 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 38847 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:01,522 [IPC Server handler 1 on 35890] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:01,525 [Thread-441] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:35890
2020-04-02 05:08:01,528 [Thread-441] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 1 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=1, dataDirs=1)
2020-04-02 05:08:01,528 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:01,528 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:01,531 [Thread-441] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:01,547 [Thread-441] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:01,548 [Thread-441] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:01,548 [Thread-441] WARN  common.Storage (BlockPoolSliceStorage.java:loadBpStorageDirectories(228)) - Failed to analyze storage directories for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804081043
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:01,548 [Thread-441] WARN  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(466)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804081043
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:01,549 [Thread-441] ERROR datanode.DataNode (BPServiceActor.java:run(829)) - Initialization failed for Block pool <registering> (Datanode Uuid FixedDatanodeUuid) service to localhost/127.0.0.1:35890. Exiting. 
java.io.IOException: All specified directories have failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:552)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:01,549 [Thread-441] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool <registering> (Datanode Uuid FixedDatanodeUuid) service to localhost/127.0.0.1:35890
2020-04-02 05:08:01,549 [Thread-441] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool <registering> (Datanode Uuid FixedDatanodeUuid)
2020-04-02 05:08:01,632 [IPC Server handler 3 on 35890] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:01,634 [Thread-134] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2694)) - BPOfferService in datanode DataNode{data=null, localName='127.0.0.1:36132', datanodeUuid='FixedDatanodeUuid', xmitsInProgress=0} failed to connect to namenode at localhost/127.0.0.1:35890
2020-04-02 05:08:01,634 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:01,634 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:01,634 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:01,634 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 38847 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:01,641 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@49f38255] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:01,791 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@73fea7ff{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:01,792 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@759eac91{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:01,792 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@14f16c0f{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:01,792 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@594d400f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:01,794 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 38847
2020-04-02 05:08:01,800 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:01,819 [IPC Server listener on 38847] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 38847
2020-04-02 05:08:01,830 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:01,831 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:01,831 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 35890 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:01,831 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:01,831 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:01,834 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 6 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 9 
2020-04-02 05:08:01,835 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:01,846 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:01,846 [CacheReplicationMonitor(1896704107)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:01,838 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@7773db41] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:01,837 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@14b245b] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:01,856 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 35890
2020-04-02 05:08:01,858 [IPC Server listener on 35890] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 35890
2020-04-02 05:08:01,898 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:01,898 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:01,898 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:01,923 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:01,924 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:01,925 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@39bd5920{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:01,968 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3d83c3d6{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:01,969 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6f8c1600{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:01,969 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3e446e77{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:01,978 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:01,978 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:01,979 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:01,998 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,004 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 6*** NameNode upgrade with no edits file: numDirs=1
2020-04-02 05:08:02,023 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:02,023 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,033 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:02,038 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:02,044 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:02,044 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:02,044 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:02,045 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:02,052 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@463cafd4] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:02,052 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:02,053 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,055 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:02,058 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:02,058 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,059 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:02,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:02,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:02,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:02,066 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:02,066 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:02,066 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36346
2020-04-02 05:08:02,066 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:02,070 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4bc3c734{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:02,071 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@62722fa8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:02,074 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@71e2143a{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:02,075 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@73dd4d79{HTTP/1.1,[http/1.1]}{localhost:36346}
2020-04-02 05:08:02,075 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @12985ms
2020-04-02 05:08:02,075 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,075 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,075 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,075 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,076 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,076 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,076 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:02,079 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:02,079 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:02,079 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:02,079 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:02,079 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:02,080 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:02,080 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:02,080 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:02,083 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:02,084 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:02,084 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:02,084 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:02
2020-04-02 05:08:02,085 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:02,085 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,085 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:02,085 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:02,089 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:02,089 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:02,090 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:02,091 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:02,091 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:02,091 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:02,091 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:02,091 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:02,092 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,092 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:02,092 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:02,095 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:02,095 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:02,095 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:02,096 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:02,096 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:02,096 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:02,096 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:02,096 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,097 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:02,097 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:02,106 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:02,107 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:02,107 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:02,108 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:02,108 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:02,108 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:02,108 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,108 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:02,109 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:02,112 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:02,113 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:02,114 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:02,115 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 32 but unable to find any edit logs containing txid 32
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1709)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1684)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:701)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:323)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,116 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:02,117 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@71e2143a{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:02,129 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@73dd4d79{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:02,130 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@62722fa8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:02,130 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4bc3c734{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:02,138 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:02,139 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:02,140 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:02,141 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 32 but unable to find any edit logs containing txid 32
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1709)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1684)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:701)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:323)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,141 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:02,143 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:02,144 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,144 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 7*** NameNode upgrade with no image file: numDirs=1
2020-04-02 05:08:02,175 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:02,176 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,176 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:02,180 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:02,183 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:02,183 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:02,183 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:02,183 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:02,189 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@37709fdf] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:02,189 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:02,205 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,206 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:02,210 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:02,210 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,219 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:02,220 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:02,220 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:02,220 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:02,222 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:02,223 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:02,223 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36667
2020-04-02 05:08:02,226 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:02,283 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1d8ac880{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:02,283 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@41bbed3c{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:02,288 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2dfb0c9c{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:02,289 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@629e7eb7{HTTP/1.1,[http/1.1]}{localhost:36667}
2020-04-02 05:08:02,289 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @13199ms
2020-04-02 05:08:02,289 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,289 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,290 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,290 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,290 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,290 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,290 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:02,291 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:02,292 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:02,292 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:02,292 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:02,292 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:02,292 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:02,293 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:02
2020-04-02 05:08:02,293 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:02,293 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,293 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:02,293 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:02,307 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:02,308 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:02,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:02,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:02,308 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:02,308 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:02,309 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:02,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:02,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:02,310 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:02,313 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:02,313 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:02,313 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:02,314 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:02,314 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:02,314 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:02,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:02,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:02,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:02,315 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:02,315 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:02,315 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:02,316 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:02,316 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:02,316 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:02,316 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,317 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:02,317 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:02,321 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:02,321 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:02,322 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.FileNotFoundException: No valid image files found
	at org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector.getLatestImages(FSImageTransactionalStorageInspector.java:158)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:672)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,323 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:02,324 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2dfb0c9c{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:02,338 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@629e7eb7{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:02,339 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@41bbed3c{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:02,339 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1d8ac880{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:02,341 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:02,343 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:02,343 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:02,343 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.FileNotFoundException: No valid image files found
	at org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector.getLatestImages(FSImageTransactionalStorageInspector.java:158)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:672)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,344 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:02,345 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:02,346 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,346 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 8*** NameNode upgrade with corrupt version file: numDirs=1
2020-04-02 05:08:02,371 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:02,371 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,371 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:02,375 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:02,382 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:02,382 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:02,383 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:02,383 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:02,388 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@70796fe7] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:02,390 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:02,391 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,393 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:02,394 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:02,394 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,395 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:02,396 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:02,396 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:02,396 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:02,397 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:02,398 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:02,398 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 33708
2020-04-02 05:08:02,398 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:02,411 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@247fb913{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:02,412 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@758527be{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:02,419 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@53caf152{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:02,419 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@442753f2{HTTP/1.1,[http/1.1]}{localhost:33708}
2020-04-02 05:08:02,420 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @13330ms
2020-04-02 05:08:02,420 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,425 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,425 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,425 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,426 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,426 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,427 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:02,427 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:02,427 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:02,427 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:02,428 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:02,428 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:02,428 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:02,428 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:02,429 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:02,429 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:02,429 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:02,429 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:02,430 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:02
2020-04-02 05:08:02,430 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:02,430 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,431 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:02,431 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:02,441 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:02,442 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:02,442 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:02,442 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:02,443 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:02,444 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:02,444 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:02,445 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,445 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:02,445 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:02,451 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:02,452 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:02,452 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:02,452 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:02,452 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:02,452 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:02,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:02,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:02,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:02,455 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:02,455 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:02,455 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:02,456 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:02,457 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:02,457 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:02,457 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,457 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:02,457 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:02,460 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:02,461 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: file VERSION has layoutVersion missing.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.getProperty(StorageInfo.java:240)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:203)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:340)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,462 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:02,470 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@53caf152{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:02,478 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@442753f2{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:02,479 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@758527be{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:02,480 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@247fb913{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:02,486 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:02,495 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:02,500 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:02,501 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: file VERSION has layoutVersion missing.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.getProperty(StorageInfo.java:240)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:203)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:340)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,501 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:02,503 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:02,504 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,504 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 9*** NameNode upgrade with old layout version in current: numDirs=1
2020-04-02 05:08:02,526 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:02,526 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,527 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:02,531 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:02,533 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:02,533 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:02,538 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:02,538 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:02,550 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:02,550 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,552 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:02,550 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@40d751d5] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:02,554 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:02,557 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,558 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:02,559 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:02,559 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:02,560 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:02,561 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:02,561 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:02,562 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 39859
2020-04-02 05:08:02,563 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:02,567 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7674d12a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:02,568 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3d66b53e{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:02,581 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6458aa0e{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:02,582 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2e047051{HTTP/1.1,[http/1.1]}{localhost:39859}
2020-04-02 05:08:02,583 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @13492ms
2020-04-02 05:08:02,583 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,588 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,589 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,589 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,589 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,589 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,590 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:02,591 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:02,591 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:02,591 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:02,591 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:02,591 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:02,592 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:02,592 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:02,592 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:02,593 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:02,594 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:02,594 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:02,594 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:02
2020-04-02 05:08:02,595 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:02,595 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,595 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:02,596 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:02,718 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:02,719 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:02,719 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:02,719 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:02,719 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:02,720 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:02,720 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:02,720 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,721 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:02,721 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:02,727 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:02,729 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:02,729 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:02,729 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:02,729 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:02,729 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:02,731 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:02,734 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,734 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:02,734 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:02,735 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:02,735 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:02,735 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:02,736 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:02,736 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:02,737 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:02,737 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,737 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:02,737 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:02,755 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:02,755 [Thread-134] ERROR common.Storage (Storage.java:checkVersionUpgradable(1119)) - *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
2020-04-02 05:08:02,756 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.IOException: *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
	at org.apache.hadoop.hdfs.server.common.Storage.checkVersionUpgradable(Storage.java:1120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:353)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,756 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:02,759 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6458aa0e{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:02,790 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2e047051{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:02,790 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3d66b53e{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:02,791 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7674d12a{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:02,799 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:02,799 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:02,799 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:02,800 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.IOException: *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
	at org.apache.hadoop.hdfs.server.common.Storage.checkVersionUpgradable(Storage.java:1120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:353)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,800 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:02,801 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:02,807 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,807 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 10*** NameNode upgrade with future layout version in current: numDirs=1
2020-04-02 05:08:02,829 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:02,830 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,830 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:02,839 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:02,841 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:02,841 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:02,841 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:02,842 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:02,846 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@36f00287] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:02,846 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:02,848 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,850 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:02,851 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:02,851 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:02,852 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:02,852 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:02,853 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:02,853 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:02,854 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:02,854 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:02,855 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 41797
2020-04-02 05:08:02,855 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:02,857 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@680c720f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:02,857 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@485cd63c{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:02,862 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@62cfb3bc{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:02,863 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@2d425e5a{HTTP/1.1,[http/1.1]}{localhost:41797}
2020-04-02 05:08:02,863 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @13773ms
2020-04-02 05:08:02,863 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,863 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,864 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(680)) - Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,864 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:checkConfiguration(685)) - Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-04-02 05:08:02,864 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,864 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:02,865 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:02,865 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:02,865 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:02,865 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:02,866 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:02,867 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:02,868 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:02,868 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:02,868 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:02,868 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:02,869 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:02,869 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:02,875 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:02
2020-04-02 05:08:02,876 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:02,876 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,876 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:02,876 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:02,879 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:02,880 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:02,880 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:02,880 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:02,880 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:02,881 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:02,882 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:02,882 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,882 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:02,882 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:02,886 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:02,886 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:02,886 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:02,886 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:02,886 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:02,886 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:02,886 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:02,886 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,887 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:02,887 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:02,888 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:02,889 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:02,889 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:02,890 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:02,890 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:02,890 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:02,890 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:02,890 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:02,890 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:02,893 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:02,893 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. Reported: -2147483648. Expecting = -64.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,899 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:02,907 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@62cfb3bc{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:02,938 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@2d425e5a{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:02,966 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@485cd63c{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:02,967 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@680c720f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:02,970 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:02,972 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:02,972 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:02,973 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. Reported: -2147483648. Expecting = -64.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:02,973 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:02,975 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:02,984 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:02,984 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 11*** Normal NameNode upgrade: numDirs=2
2020-04-02 05:08:03,026 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:03,027 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,027 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,027 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:03,031 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:03,032 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:03,033 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:03,038 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:03,038 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:03,048 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@7282f2f2] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:03,059 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:03,059 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:03,061 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:03,062 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:03,062 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:03,063 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:03,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:03,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:03,064 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:03,066 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:03,066 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:03,066 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 37185
2020-04-02 05:08:03,066 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:03,074 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1dbe0df2{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:03,074 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7b45c13d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:03,080 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@11782b52{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:03,080 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@16c2261b{HTTP/1.1,[http/1.1]}{localhost:37185}
2020-04-02 05:08:03,081 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @13991ms
2020-04-02 05:08:03,081 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,088 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,088 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,088 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,098 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,098 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,098 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,098 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,099 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:03,100 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:03,101 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:03,101 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:03,101 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:03,102 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:03,102 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:03
2020-04-02 05:08:03,102 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:03,102 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,103 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:03,103 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:03,134 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:03,135 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:03,135 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:03,135 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:03,135 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:03,135 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:03,136 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:03,137 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:03,137 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,137 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:03,142 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:03,143 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:03,144 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:03,144 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:03,145 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:03,145 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:03,145 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:03,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:03,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:03,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:03,146 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:03,146 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:03,146 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:03,147 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:03,147 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:03,147 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:03,147 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,148 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:03,148 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:03,150 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:03,151 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:03,152 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:03,153 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:03,153 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:03,153 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:03,154 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:03,155 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:03,155 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:03,155 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@59031a46 expecting start txid #32
2020-04-02 05:08:03,155 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:03,155 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:03,158 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:03,158 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804083158
2020-04-02 05:08:03,158 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,159 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,161 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:03,163 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:03,184 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:03,202 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:03,206 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,206 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,207 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,207 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,207 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,208 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,211 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:03,212 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:03,242 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:03,243 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 95 msecs
2020-04-02 05:08:03,243 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:03,243 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:03,244 [Socket Reader #1 for port 33052] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 33052
2020-04-02 05:08:03,252 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:33052 to access this namenode/service.
2020-04-02 05:08:03,253 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:03,253 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,253 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,324 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:03,337 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:03,346 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:03,346 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:33052
2020-04-02 05:08:03,346 [IPC Server listener on 33052] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 33052: starting
2020-04-02 05:08:03,347 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:03,347 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:03,347 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 0 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:03,348 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 33052 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:03,351 [CacheReplicationMonitor(1017266496)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:03,356 [IPC Server handler 1 on 33052] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:03,386 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:03,389 [IPC Server handler 3 on 33052] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:08:03,391 [IPC Server handler 3 on 33052] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=safemode_enter	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:03,393 [IPC Server handler 5 on 33052] INFO  namenode.NameNode (NameNodeRpcServer.java:rollingUpgrade(1333)) - rollingUpgrade PREPARE
2020-04-02 05:08:03,393 [IPC Server handler 5 on 33052] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 5 on 33052, call Call#79 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.rollingUpgrade from 127.0.0.1:40194
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
2020-04-02 05:08:03,395 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:testUpgrade(248)) - The exception is expected.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.common.InconsistentFSStateException): Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy24.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rollingUpgrade(ClientNamenodeProtocolTranslatorPB.java:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy25.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.rollingUpgrade(DFSClient.java:2368)
	at org.apache.hadoop.hdfs.DistributedFileSystem.rollingUpgrade(DistributedFileSystem.java:1555)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:243)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:03,395 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,396 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:03,396 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
2020-04-02 05:08:03,396 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,396 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:03,396 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
md5 of /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000061: 8401ab878af417382fdf96914d80291c
2020-04-02 05:08:03,655 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:03,656 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:03,656 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 33052 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:03,656 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:03,657 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:03,668 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 19 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 1 2 
2020-04-02 05:08:03,669 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:03,669 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:03,670 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:03,670 [CacheReplicationMonitor(1017266496)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:03,671 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 33052
2020-04-02 05:08:03,672 [IPC Server listener on 33052] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 33052
2020-04-02 05:08:03,681 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@4d603181] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:03,681 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@79c4aa6f] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:03,682 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:03,683 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:03,683 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:03,691 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:03,691 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:03,694 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@11782b52{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:03,718 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@16c2261b{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:03,719 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7b45c13d{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:03,719 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1dbe0df2{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:03,738 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:03,739 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:03,739 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:03,758 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:03,758 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 12*** Normal DataNode upgrade: numDirs=2
2020-04-02 05:08:03,792 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:03,793 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,793 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,794 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:03,797 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:03,807 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:03,807 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:03,808 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:03,808 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:03,814 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:03,814 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:03,816 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:03,816 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@4960a645] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:03,819 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:03,819 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:03,820 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:03,821 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:03,821 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:03,821 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:03,822 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:03,822 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:03,822 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 46407
2020-04-02 05:08:03,823 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:03,848 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@623a5d25{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:03,850 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@58e0d954{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:03,866 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2396b7af{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:03,867 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@6d60705{HTTP/1.1,[http/1.1]}{localhost:46407}
2020-04-02 05:08:03,867 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @14777ms
2020-04-02 05:08:03,867 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,867 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,867 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:03,869 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:03,870 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:03,871 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:03,871 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:03,871 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:03,871 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:03,873 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:03,884 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:03
2020-04-02 05:08:03,884 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:03,884 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,884 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:03,884 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:03,890 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:03,891 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:03,891 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:03,891 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:03,891 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:03,891 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:03,891 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:03,892 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:03,892 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:03,892 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:03,893 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:03,893 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:03,893 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:03,893 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:03,894 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:03,899 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:03,899 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:03,899 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:03,900 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:03,900 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:03,900 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:03,900 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:03,900 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,901 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:03,901 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:03,902 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:03,902 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:03,902 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:03,903 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:03,904 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:03,904 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:03,904 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:03,904 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:03,904 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:03,920 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:03,926 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:03,926 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:03,927 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:03,927 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:03,928 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:03,934 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:03,935 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:03,935 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:03,936 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@4cf8c011 expecting start txid #32
2020-04-02 05:08:03,936 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:03,936 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:03,949 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:03,950 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804083950
2020-04-02 05:08:03,950 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,952 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,954 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:03,956 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:03,966 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:03,966 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:03,970 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,970 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,971 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,971 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,971 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:03,973 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:03,974 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:03,981 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:03,987 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:03,987 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 82 msecs
2020-04-02 05:08:03,987 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:03,988 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:03,988 [Socket Reader #1 for port 33477] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 33477
2020-04-02 05:08:03,992 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:33477 to access this namenode/service.
2020-04-02 05:08:03,992 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:03,993 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:03,993 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:04,046 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:04,048 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:04,054 [IPC Server listener on 33477] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 33477: starting
2020-04-02 05:08:04,054 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:33477
2020-04-02 05:08:04,055 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:04,055 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:04,082 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:04,086 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 31 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:04,094 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 33477 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:04,097 [CacheReplicationMonitor(183440737)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:04,104 [IPC Server handler 0 on 33477] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:04,104 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:04,249 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1,/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:04,250 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:04,254 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:04,262 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:04,263 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:04,263 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:04,263 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:04,264 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:04,264 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:04,264 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:04,264 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:36551
2020-04-02 05:08:04,264 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:04,264 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:04,265 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:04,266 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:04,267 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:04,267 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:04,268 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:04,268 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:04,268 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:04,269 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:04,269 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 39313
2020-04-02 05:08:04,269 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:04,270 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4a0be5c6{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:04,271 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@531dbdcc{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:04,275 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@5243f900{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:04,276 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@385fb977{HTTP/1.1,[http/1.1]}{localhost:39313}
2020-04-02 05:08:04,276 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @15186ms
2020-04-02 05:08:04,285 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:34849
2020-04-02 05:08:04,291 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:04,291 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:04,291 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:04,294 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2cc2a809] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:04,294 [Socket Reader #1 for port 36249] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 36249
2020-04-02 05:08:04,296 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:36249
2020-04-02 05:08:04,303 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:04,303 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:04,304 [Thread-638] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:33477 starting to offer service
2020-04-02 05:08:04,338 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:04,338 [IPC Server listener on 36249] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 36249: starting
2020-04-02 05:08:04,356 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 36249 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:04,371 [Thread-638] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:33477
2020-04-02 05:08:04,372 [Thread-638] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:08:04,373 [IPC Server handler 2 on 33477] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:04,374 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:04,375 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:04,381 [Thread-638] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:04,397 [Thread-638] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:04,429 [Thread-638] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,429 [Thread-638] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,429 [Thread-638] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804083950
2020-04-02 05:08:04,434 [pool-98-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:08:04,436 [pool-98-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:08:04,437 [pool-98-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:08:04,439 [pool-98-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:08:04,447 [Thread-638] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,450 [Thread-638] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,451 [Thread-638] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804083950
2020-04-02 05:08:04,454 [Thread-638] INFO  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(472)) - loadBlockPoolSliceStorage: 2 upgrade tasks
2020-04-02 05:08:04,454 [pool-98-thread-2] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:08:04,456 [pool-98-thread-2] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:08:04,457 [pool-98-thread-2] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:08:04,462 [pool-98-thread-2] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:08:04,463 [Thread-638] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=1840904636;bpid=BP-306798092-172.17.0.14-1585804070968;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1840904636;c=1585804083950;bpid=BP-306798092-172.17.0.14-1585804070968;dnuuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:04,466 [Thread-638] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-20f157ac-2998-4a18-9103-38bdc679cf3f
2020-04-02 05:08:04,467 [Thread-638] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, StorageType: DISK
2020-04-02 05:08:04,472 [Thread-638] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-290f964d-f18f-4309-b544-0ceb614296fb
2020-04-02 05:08:04,473 [Thread-638] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, StorageType: DISK
2020-04-02 05:08:04,474 [Thread-638] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:08:04,475 [Thread-638] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:04,476 [Thread-638] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:04,476 [Thread-638] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:04,477 [Thread-638] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:04,484 [IPC Server handler 7 on 33477] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:04,485 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:04,485 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:04,486 [Thread-638] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,487 [Thread-652] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:08:04,487 [Thread-653] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2...
2020-04-02 05:08:04,533 [Thread-652] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 46ms
2020-04-02 05:08:04,541 [Thread-653] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2: 54ms
2020-04-02 05:08:04,541 [Thread-638] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-306798092-172.17.0.14-1585804070968: 55ms
2020-04-02 05:08:04,541 [Thread-656] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:08:04,541 [Thread-657] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2...
2020-04-02 05:08:04,541 [Thread-656] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:08:04,541 [Thread-657] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:08:04,573 [Thread-657] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2: 33ms
2020-04-02 05:08:04,575 [Thread-656] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 34ms
2020-04-02 05:08:04,581 [Thread-638] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-306798092-172.17.0.14-1585804070968: 41ms
2020-04-02 05:08:04,583 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, DS-290f964d-f18f-4309-b544-0ceb614296fb): no suitable block pools found to scan.  Waiting 1814390016 ms.
2020-04-02 05:08:04,583 [Thread-638] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 7:30 AM with interval of 21600000ms
2020-04-02 05:08:04,585 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-20f157ac-2998-4a18-9103-38bdc679cf3f): no suitable block pools found to scan.  Waiting 1814390014 ms.
2020-04-02 05:08:04,587 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:33477 beginning handshake with NN
2020-04-02 05:08:04,588 [IPC Server handler 8 on 33477] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:04,588 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:04,589 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:04,589 [IPC Server handler 8 on 33477] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:36551, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=34849, infoSecurePort=0, ipcPort=36249, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804083950) storage 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:04,589 [IPC Server handler 8 on 33477] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:36551
2020-04-02 05:08:04,589 [IPC Server handler 8 on 33477] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7f97fdc0-fc66-438e-852d-d4bb971aa9c2 (127.0.0.1:36551).
2020-04-02 05:08:04,591 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:33477 successfully registered with NN
2020-04-02 05:08:04,591 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:33477 using BLOCKREPORT_INTERVAL of 10000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:08:04,593 [IPC Server handler 4 on 33477] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-20f157ac-2998-4a18-9103-38bdc679cf3f for DN 127.0.0.1:36551
2020-04-02 05:08:04,593 [IPC Server handler 4 on 33477] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-290f964d-f18f-4309-b544-0ceb614296fb for DN 127.0.0.1:36551
2020-04-02 05:08:04,595 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x4d8827ad677294ca: Processing first storage report for DS-20f157ac-2998-4a18-9103-38bdc679cf3f from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:04,596 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x4d8827ad677294ca: from storage DS-20f157ac-2998-4a18-9103-38bdc679cf3f node DatanodeRegistration(127.0.0.1:36551, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=34849, infoSecurePort=0, ipcPort=36249, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804083950), blocks: 1, hasStaleStorage: true, processing time: 1 msecs, invalidatedBlocks: 0
2020-04-02 05:08:04,596 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x4d8827ad677294ca: Processing first storage report for DS-290f964d-f18f-4309-b544-0ceb614296fb from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:04,596 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:08:04,597 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(391)) - STATE* Safe mode is OFF
2020-04-02 05:08:04,597 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:08:04,597 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 1 racks and 1 datanodes
2020-04-02 05:08:04,597 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:08:04,600 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x4d8827ad677294ca: from storage DS-290f964d-f18f-4309-b544-0ceb614296fb node DatanodeRegistration(127.0.0.1:36551, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=34849, infoSecurePort=0, ipcPort=36249, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804083950), blocks: 15, hasStaleStorage: false, processing time: 4 msecs, invalidatedBlocks: 0
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 16
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:08:04,611 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 14 msec
2020-04-02 05:08:04,612 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x4d8827ad677294ca,  containing 2 storage report(s), of which we sent 2. The reports had 16 total blocks and used 1 RPC(s). This took 0 msec to generate and 18 msecs for RPC and NN processing. Got back no commands.
2020-04-02 05:08:04,691 [IPC Server handler 6 on 33477] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:04,692 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:04,698 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:04,699 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:04,699 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 36249 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:04,699 [Thread-134] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:08:04,700 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-20f157ac-2998-4a18-9103-38bdc679cf3f) exiting.
2020-04-02 05:08:04,701 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@5448ee74] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:04,702 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, DS-290f964d-f18f-4309-b544-0ceb614296fb) exiting.
2020-04-02 05:08:04,751 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@5243f900{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:04,752 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@385fb977{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:04,753 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@531dbdcc{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:04,753 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4a0be5c6{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:04,754 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 36249
2020-04-02 05:08:04,755 [IPC Server listener on 36249] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 36249
2020-04-02 05:08:04,759 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:04,760 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:08:04,760 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:33477
2020-04-02 05:08:04,760 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2)
2020-04-02 05:08:04,760 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:33477] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:04,804 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:08:04,814 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:08:04,817 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:08:04,817 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:08:04,817 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:08:04,818 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:08:04,819 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:04,823 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:04,823 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 33477 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:04,823 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:04,824 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@90d88e4] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:04,824 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@6b144fb8] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:04,830 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:04,850 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 20 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 0 1 
2020-04-02 05:08:04,855 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:04,856 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:04,871 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:04,872 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 33477
2020-04-02 05:08:04,873 [CacheReplicationMonitor(183440737)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:04,887 [IPC Server listener on 33477] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 33477
2020-04-02 05:08:04,889 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:04,887 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:04,887 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:04,918 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:04,918 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:04,920 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2396b7af{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:04,924 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@6d60705{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:04,925 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@58e0d954{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:04,925 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@623a5d25{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:04,928 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:04,931 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:04,931 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:04,967 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:04,968 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 13*** NameNode upgrade with existing previous dir: numDirs=2
2020-04-02 05:08:05,028 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:05,028 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,028 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,029 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:05,035 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:05,037 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:05,037 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:05,037 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:05,038 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:05,042 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@247cc55a] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:05,042 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:05,043 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,044 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:05,045 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:05,045 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,046 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:05,046 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:05,047 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:05,047 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:05,048 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:05,048 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:05,049 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 38995
2020-04-02 05:08:05,049 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:05,062 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@31267ce5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:05,064 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@518b6e5b{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:05,069 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@2678c523{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:05,070 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@3aef8d22{HTTP/1.1,[http/1.1]}{localhost:38995}
2020-04-02 05:08:05,070 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @15980ms
2020-04-02 05:08:05,070 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,070 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,071 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,073 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:05,073 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:05,073 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:05,073 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:05,073 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:05,074 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:05,074 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:05,074 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:05,075 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:05,075 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:05,076 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:05,076 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:05,076 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:05
2020-04-02 05:08:05,076 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:05,076 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,077 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:05,077 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:05,101 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:05,118 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:05,118 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:05,119 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:05,119 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:05,119 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:05,119 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,119 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:05,119 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:05,131 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:05,132 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:05,132 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:05,132 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:05,132 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:05,132 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:05,133 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:05,133 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,133 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:05,133 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:05,143 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:05,143 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:05,143 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:05,144 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:05,144 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:05,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:05,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:05,145 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:05,150 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,151 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,152 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:05,152 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:438)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:05,153 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:05,157 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@2678c523{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:05,174 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@3aef8d22{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:05,175 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@518b6e5b{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:05,175 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@31267ce5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:05,176 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:05,178 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:05,178 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:05,179 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:438)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:270)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:05,179 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:05,181 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:05,183 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:05,184 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 14*** DataNode upgrade with existing previous dir: numDirs=2
2020-04-02 05:08:05,232 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:05,232 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,233 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,233 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:05,251 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:05,253 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:05,253 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:05,254 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:05,254 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:05,257 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@2a6e71ca] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:05,258 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:05,259 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,263 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:05,264 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:05,264 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,266 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:05,266 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:05,266 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:05,266 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:05,268 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:05,268 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:05,268 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 43521
2020-04-02 05:08:05,268 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:05,295 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@25f20744{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:05,297 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@326db0b8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:05,302 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@133f741{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:05,303 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@50d2ae58{HTTP/1.1,[http/1.1]}{localhost:43521}
2020-04-02 05:08:05,303 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @16213ms
2020-04-02 05:08:05,303 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,304 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,304 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,304 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,304 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,305 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,305 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,305 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,306 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:05,307 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:05,311 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:05,312 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:05,312 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:05,312 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:05,313 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:05
2020-04-02 05:08:05,313 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:05,313 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:05,314 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:05,322 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:05,323 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:05,323 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:05,324 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:05,324 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,324 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:05,324 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:05,331 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:05,331 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:05,331 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:05,331 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:05,331 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:05,331 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:05,331 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:05,331 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,331 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:05,332 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:05,332 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:05,332 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:05,332 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:05,333 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:05,333 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:05,333 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:05,333 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:05,333 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:05,333 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:05,336 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,337 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,341 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:05,343 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:05,344 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:05,344 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:05,347 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:05,348 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:05,349 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:05,349 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@d347c25 expecting start txid #32
2020-04-02 05:08:05,349 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:05,349 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:05,355 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:05,355 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804085355
2020-04-02 05:08:05,355 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:05,356 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:05,360 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:05,366 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:05,389 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:05,398 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:05,408 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:05,408 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:05,409 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:05,409 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:05,409 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:05,415 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:05,417 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:05,426 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:05,483 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:05,484 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 150 msecs
2020-04-02 05:08:05,484 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:05,484 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:05,485 [Socket Reader #1 for port 40342] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 40342
2020-04-02 05:08:05,488 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:40342 to access this namenode/service.
2020-04-02 05:08:05,489 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:05,489 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:05,489 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:05,527 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:05,528 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:05,534 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:05,535 [IPC Server listener on 40342] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 40342: starting
2020-04-02 05:08:05,537 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:40342
2020-04-02 05:08:05,538 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:05,538 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:05,539 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 1 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:05,539 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 40342 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:05,549 [CacheReplicationMonitor(112047479)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:05,558 [IPC Server handler 1 on 40342] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:05,558 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:05,852 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1,/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:05,853 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:05,853 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:05,854 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:05,855 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:05,855 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:05,856 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:05,856 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:05,857 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:05,857 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:05,858 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:41439
2020-04-02 05:08:05,858 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:05,859 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:05,860 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,861 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:05,866 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:05,866 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:05,872 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:05,872 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:05,872 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:05,872 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:05,873 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 35762
2020-04-02 05:08:05,873 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:05,876 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6c23962{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:05,876 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@12c030e8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:05,881 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@62c9afd6{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:05,881 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@389d0720{HTTP/1.1,[http/1.1]}{localhost:35762}
2020-04-02 05:08:05,881 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @16791ms
2020-04-02 05:08:05,895 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:42432
2020-04-02 05:08:05,895 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:05,895 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:05,896 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:05,895 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1f7933b1] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:05,897 [Socket Reader #1 for port 42409] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 42409
2020-04-02 05:08:05,901 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:42409
2020-04-02 05:08:05,904 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:05,904 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:05,916 [Thread-728] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40342 starting to offer service
2020-04-02 05:08:05,931 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:05,934 [IPC Server listener on 42409] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 42409: starting
2020-04-02 05:08:05,943 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 42409 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:05,950 [IPC Server handler 3 on 40342] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:05,951 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:05,951 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:05,962 [Thread-728] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40342
2020-04-02 05:08:05,963 [Thread-728] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:08:05,972 [Thread-728] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,975 [Thread-728] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:05,984 [Thread-728] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:05,984 [Thread-728] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:05,985 [Thread-728] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804085355
2020-04-02 05:08:05,995 [Thread-728] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:05,995 [Thread-728] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:05,995 [Thread-728] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(450)) - Upgrading block pool storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968.
   old LV = -57; old CTime = 1585804070968.
   new LV = -57; new CTime = 1585804085355
2020-04-02 05:08:05,996 [Thread-728] INFO  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(472)) - loadBlockPoolSliceStorage: 2 upgrade tasks
2020-04-02 05:08:05,996 [pool-118-thread-2] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:08:05,996 [pool-118-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/finalized to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/finalized
2020-04-02 05:08:05,998 [pool-118-thread-2] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:08:05,999 [pool-118-thread-1] INFO  common.Storage (DataStorage.java:linkBlocks(1074)) - Start linking block files from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp/rbw to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/rbw
2020-04-02 05:08:05,999 [pool-118-thread-2] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:08:05,999 [pool-118-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:linkAllBlocks(696)) - Linked blocks from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/previous.tmp to /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current. HardLinkStats: 3 Directories, including 2 Empty Directories, 0 single Link operations, 1 multi-Link operations, linking 32 files, total 32 linkable files.  Also physically copied 0 other files.
2020-04-02 05:08:06,002 [pool-118-thread-1] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:08:06,009 [pool-118-thread-2] INFO  common.Storage (BlockPoolSliceStorage.java:doUpgrade(509)) - Upgrade of block pool BP-306798092-172.17.0.14-1585804070968 at /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968 is complete
2020-04-02 05:08:06,014 [Thread-728] INFO  datanode.DataNode (DataNode.java:initStorage(1732)) - Setting up storage: nsid=1840904636;bpid=BP-306798092-172.17.0.14-1585804070968;lv=-57;nsInfo=lv=-64;cid=testClusterID;nsid=1840904636;c=1585804085355;bpid=BP-306798092-172.17.0.14-1585804070968;dnuuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:06,017 [Thread-728] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8
2020-04-02 05:08:06,017 [Thread-728] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, StorageType: DISK
2020-04-02 05:08:06,027 [Thread-728] INFO  impl.FsDatasetImpl (FsVolumeList.java:addVolume(306)) - Added new volume: DS-2adc912e-d8aa-411e-bce0-f75c8a56a548
2020-04-02 05:08:06,028 [Thread-728] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addVolume(450)) - Added volume - [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, StorageType: DISK
2020-04-02 05:08:06,029 [Thread-728] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:registerMBean(2280)) - Registered FSDatasetState MBean
2020-04-02 05:08:06,029 [Thread-728] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:06,030 [Thread-728] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:06,030 [Thread-728] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:06,030 [Thread-728] INFO  checker.DatasetVolumeChecker (DatasetVolumeChecker.java:checkAllVolumes(221)) - Scheduled health check for volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:06,030 [Thread-728] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:addBlockPool(2791)) - Adding block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:06,031 [Thread-742] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:08:06,031 [Thread-743] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(409)) - Scanning block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2...
2020-04-02 05:08:06,054 [IPC Server handler 4 on 40342] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:06,055 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:06,056 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:06,058 [Thread-743] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2: 27ms
2020-04-02 05:08:06,067 [Thread-742] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(414)) - Time taken to scan block pool BP-306798092-172.17.0.14-1585804070968 on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 36ms
2020-04-02 05:08:06,067 [Thread-728] INFO  impl.FsDatasetImpl (FsVolumeList.java:addBlockPool(440)) - Total time to scan all replicas for block pool BP-306798092-172.17.0.14-1585804070968: 37ms
2020-04-02 05:08:06,068 [Thread-746] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1...
2020-04-02 05:08:06,068 [Thread-747] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(198)) - Adding replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2...
2020-04-02 05:08:06,068 [Thread-746] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:08:06,068 [Thread-747] INFO  impl.BlockPoolSlice (BlockPoolSlice.java:readReplicasFromCache(871)) - Replica Cache file: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968/current/replicas doesn't exist 
2020-04-02 05:08:06,079 [Thread-746] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1: 11ms
2020-04-02 05:08:06,081 [Thread-747] INFO  impl.FsDatasetImpl (FsVolumeList.java:run(203)) - Time to add replicas to map for block pool BP-306798092-172.17.0.14-1585804070968 on volume /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2: 14ms
2020-04-02 05:08:06,082 [Thread-728] INFO  impl.FsDatasetImpl (FsVolumeList.java:getAllVolumesMap(230)) - Total time to add all replicas to map for block pool BP-306798092-172.17.0.14-1585804070968: 14ms
2020-04-02 05:08:06,085 [Thread-728] INFO  datanode.DirectoryScanner (DirectoryScanner.java:start(283)) - Periodic Directory Tree Verification scan starting at 4/2/20 9:18 AM with interval of 21600000ms
2020-04-02 05:08:06,085 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, DS-2adc912e-d8aa-411e-bce0-f75c8a56a548): no suitable block pools found to scan.  Waiting 1814388514 ms.
2020-04-02 05:08:06,085 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:findNextUsableBlockIter(403)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8): no suitable block pools found to scan.  Waiting 1814388514 ms.
2020-04-02 05:08:06,092 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  datanode.DataNode (BPServiceActor.java:register(764)) - Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:40342 beginning handshake with NN
2020-04-02 05:08:06,095 [IPC Server handler 5 on 40342] INFO  hdfs.StateChange (DatanodeManager.java:registerDatanode(1040)) - BLOCK* registerDatanode: from DatanodeRegistration(127.0.0.1:41439, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=42432, infoSecurePort=0, ipcPort=42409, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804085355) storage 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:06,095 [IPC Server handler 5 on 40342] INFO  net.NetworkTopology (NetworkTopology.java:add(145)) - Adding a new node: /default-rack/127.0.0.1:41439
2020-04-02 05:08:06,096 [IPC Server handler 5 on 40342] INFO  blockmanagement.BlockReportLeaseManager (BlockReportLeaseManager.java:registerNode(204)) - Registered DN 7f97fdc0-fc66-438e-852d-d4bb971aa9c2 (127.0.0.1:41439).
2020-04-02 05:08:06,102 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  datanode.DataNode (BPServiceActor.java:register(783)) - Block pool Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:40342 successfully registered with NN
2020-04-02 05:08:06,102 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  datanode.DataNode (BPServiceActor.java:offerService(612)) - For namenode localhost/127.0.0.1:40342 using BLOCKREPORT_INTERVAL of 10000msec CACHEREPORT_INTERVAL of 10000msec Initial delay: 0msec; heartBeatInterval=3000
2020-04-02 05:08:06,107 [IPC Server handler 6 on 40342] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8 for DN 127.0.0.1:41439
2020-04-02 05:08:06,108 [IPC Server handler 6 on 40342] INFO  blockmanagement.DatanodeDescriptor (DatanodeDescriptor.java:updateStorage(987)) - Adding new storage ID DS-2adc912e-d8aa-411e-bce0-f75c8a56a548 for DN 127.0.0.1:41439
2020-04-02 05:08:06,111 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x4ec365806b541a84: Processing first storage report for DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8 from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:06,111 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x4ec365806b541a84: from storage DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8 node DatanodeRegistration(127.0.0.1:41439, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=42432, infoSecurePort=0, ipcPort=42409, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804085355), blocks: 0, hasStaleStorage: true, processing time: 0 msecs, invalidatedBlocks: 0
2020-04-02 05:08:06,111 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2541)) - BLOCK* processReport 0x4ec365806b541a84: Processing first storage report for DS-2adc912e-d8aa-411e-bce0-f75c8a56a548 from datanode 7f97fdc0-fc66-438e-852d-d4bb971aa9c2
2020-04-02 05:08:06,112 [Block report processor] INFO  blockmanagement.BlockManager (BlockManager.java:initializeReplQueues(4792)) - initializing replication queues
2020-04-02 05:08:06,121 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(391)) - STATE* Safe mode is OFF
2020-04-02 05:08:06,122 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(396)) - STATE* Leaving safe mode after 0 secs
2020-04-02 05:08:06,122 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(402)) - STATE* Network topology has 1 racks and 1 datanodes
2020-04-02 05:08:06,122 [Block report processor] INFO  hdfs.StateChange (BlockManagerSafeMode.java:leaveSafeMode(404)) - STATE* UnderReplicatedBlocks has 0 blocks
2020-04-02 05:08:06,132 [Block report processor] INFO  BlockStateChange (BlockManager.java:processReport(2570)) - BLOCK* processReport 0x4ec365806b541a84: from storage DS-2adc912e-d8aa-411e-bce0-f75c8a56a548 node DatanodeRegistration(127.0.0.1:41439, datanodeUuid=7f97fdc0-fc66-438e-852d-d4bb971aa9c2, infoPort=42432, infoSecurePort=0, ipcPort=42409, storageInfo=lv=-57;cid=testClusterID;nsid=1840904636;c=1585804085355), blocks: 16, hasStaleStorage: false, processing time: 21 msecs, invalidatedBlocks: 0
2020-04-02 05:08:06,142 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3475)) - Total number of blocks            = 16
2020-04-02 05:08:06,143 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3476)) - Number of invalid blocks          = 0
2020-04-02 05:08:06,150 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3477)) - Number of under-replicated blocks = 0
2020-04-02 05:08:06,150 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3478)) - Number of  over-replicated blocks = 0
2020-04-02 05:08:06,150 [Reconstruction Queue Initializer] INFO  blockmanagement.BlockManager (BlockManager.java:processMisReplicatesAsync(3480)) - Number of blocks being written    = 0
2020-04-02 05:08:06,150 [Reconstruction Queue Initializer] INFO  hdfs.StateChange (BlockManager.java:processMisReplicatesAsync(3483)) - STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 18 msec
2020-04-02 05:08:06,154 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  datanode.DataNode (BPServiceActor.java:blockReport(422)) - Successfully sent block report 0x4ec365806b541a84,  containing 2 storage report(s), of which we sent 2. The reports had 16 total blocks and used 1 RPC(s). This took 0 msec to generate and 44 msecs for RPC and NN processing. Got back no commands.
2020-04-02 05:08:06,162 [IPC Server handler 8 on 40342] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:06,163 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:06,178 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:06,179 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:06,179 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 42409 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:06,179 [Thread-134] WARN  datanode.DirectoryScanner (DirectoryScanner.java:shutdown(342)) - DirectoryScanner: shutdown has been called
2020-04-02 05:08:06,181 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@25bb78b4] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:06,185 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1, DS-2ed72541-0ca3-4897-92b3-0b239e7ac2c8) exiting.
2020-04-02 05:08:06,185 [VolumeScannerThread(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2)] INFO  datanode.VolumeScanner (VolumeScanner.java:run(642)) - VolumeScanner(/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2, DS-2adc912e-d8aa-411e-bce0-f75c8a56a548) exiting.
2020-04-02 05:08:06,208 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@62c9afd6{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:06,218 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@389d0720{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:06,218 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@12c030e8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:06,218 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6c23962{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:06,222 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 42409
2020-04-02 05:08:06,231 [IPC Server listener on 42409] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 42409
2020-04-02 05:08:06,236 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:06,237 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] WARN  datanode.IncrementalBlockReportManager (IncrementalBlockReportManager.java:waitTillNextIBR(160)) - IncrementalBlockReportManager interrupted
2020-04-02 05:08:06,237 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2) service to localhost/127.0.0.1:40342
2020-04-02 05:08:06,237 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool BP-306798092-172.17.0.14-1585804070968 (Datanode Uuid 7f97fdc0-fc66-438e-852d-d4bb971aa9c2)
2020-04-02 05:08:06,237 [BP-306798092-172.17.0.14-1585804070968 heartbeating to localhost/127.0.0.1:40342] INFO  impl.FsDatasetImpl (FsDatasetImpl.java:shutdownBlockPool(2802)) - Removing block pool BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:06,262 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:08:06,274 [refreshUsed-/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968] WARN  fs.CachingGetSpaceUsed (CachingGetSpaceUsed.java:run(183)) - Thread Interrupted waiting to refresh disk information: sleep interrupted
2020-04-02 05:08:06,284 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(192)) - Shutting down all async disk service threads
2020-04-02 05:08:06,284 [Thread-134] INFO  impl.FsDatasetAsyncDiskService (FsDatasetAsyncDiskService.java:shutdown(200)) - All async disk service threads have been shut down
2020-04-02 05:08:06,285 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(176)) - Shutting down all async lazy persist service threads
2020-04-02 05:08:06,285 [Thread-134] INFO  impl.RamDiskAsyncLazyPersistService (RamDiskAsyncLazyPersistService.java:shutdown(183)) - All async lazy persist service threads have been shut down
2020-04-02 05:08:06,294 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:06,294 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:06,294 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 40342 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:06,294 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:06,295 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:06,295 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@2ef675c1] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:06,295 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@155a4493] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:06,298 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 14 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 11 8 
2020-04-02 05:08:06,299 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:06,299 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:06,300 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:06,301 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 40342
2020-04-02 05:08:06,301 [CacheReplicationMonitor(112047479)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:06,303 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:06,304 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:06,304 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:06,304 [IPC Server listener on 40342] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 40342
2020-04-02 05:08:06,324 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:06,324 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:06,331 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@133f741{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:06,339 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@50d2ae58{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:06,340 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@326db0b8{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:06,340 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@25f20744{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:06,368 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:06,372 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:06,372 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:06,395 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:06,395 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 15*** DataNode upgrade with future stored layout version in current: numDirs=2
2020-04-02 05:08:06,443 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:06,443 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,444 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,444 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:06,448 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:06,451 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:06,451 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:06,451 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:06,452 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:06,462 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@1afe8a56] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:06,462 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:06,462 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:06,464 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:06,465 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:06,465 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:06,467 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:06,467 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:06,468 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:06,468 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:06,469 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:06,469 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:06,470 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 32823
2020-04-02 05:08:06,470 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:06,472 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2de77979{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:06,472 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@57f00116{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:06,477 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@64182381{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:06,478 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@617ece4b{HTTP/1.1,[http/1.1]}{localhost:32823}
2020-04-02 05:08:06,478 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @17388ms
2020-04-02 05:08:06,478 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,478 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,479 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,480 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:06,480 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:06,481 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:06,482 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:06,482 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:06,482 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:06,482 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:06,482 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:06
2020-04-02 05:08:06,482 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:06,483 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:06,483 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:06,483 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:06,492 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:06,492 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:06,492 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:06,492 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:06,493 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:06,494 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:06,494 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:06,494 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:06,494 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:06,505 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:06,506 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:06,506 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:06,506 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:06,506 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:06,506 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:06,507 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:06,507 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:06,507 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:06,507 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:06,508 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:06,508 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:06,508 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:06,509 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:06,509 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:06,509 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:06,509 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:06,509 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:06,510 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:06,511 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:06,512 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:06,513 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:06,515 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:06,515 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:06,516 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:06,517 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:06,518 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:06,518 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:06,518 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@6f390bea expecting start txid #32
2020-04-02 05:08:06,518 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:06,518 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:06,521 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:06,521 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804086521
2020-04-02 05:08:06,521 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:06,522 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:06,546 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:06,560 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:06,560 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:06,567 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:06,570 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:06,571 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:06,572 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:06,572 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:06,572 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:06,578 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:06,581 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:06,582 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:06,608 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:06,608 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 97 msecs
2020-04-02 05:08:06,609 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:06,609 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:06,610 [Socket Reader #1 for port 40164] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 40164
2020-04-02 05:08:06,615 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:40164 to access this namenode/service.
2020-04-02 05:08:06,615 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:06,615 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:06,616 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:06,646 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:06,649 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:06,662 [IPC Server listener on 40164] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 40164: starting
2020-04-02 05:08:06,683 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:40164
2020-04-02 05:08:06,684 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:06,684 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:06,684 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 1 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:06,707 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:06,718 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 40164 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:06,734 [CacheReplicationMonitor(1247449058)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:06,739 [IPC Server handler 4 on 40164] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:06,740 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:06,877 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-951d014d-e876-46d4-a57b-587eb12a2503 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 
2020-04-02 05:08:06,879 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-5df64b21-8edb-42ef-b886-7434037f2019 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2 
2020-04-02 05:08:06,880 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1,/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:06,884 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:06,884 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:06,891 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:06,892 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:06,892 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:06,892 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:06,893 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:06,893 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:06,894 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:06,895 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:44173
2020-04-02 05:08:06,895 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:06,895 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:06,898 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:06,900 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:06,901 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:06,902 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:06,903 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:06,903 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:06,903 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:06,903 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:06,904 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 41388
2020-04-02 05:08:06,904 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:06,909 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@32dcbb9b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:06,909 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@ef119e4{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:06,913 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6f74d63f{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:06,914 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@47cd2ece{HTTP/1.1,[http/1.1]}{localhost:41388}
2020-04-02 05:08:06,914 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @17824ms
2020-04-02 05:08:06,948 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40695
2020-04-02 05:08:06,973 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5f75012f] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:06,982 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:06,982 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:06,983 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:06,986 [Socket Reader #1 for port 34837] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 34837
2020-04-02 05:08:07,000 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:34837
2020-04-02 05:08:07,005 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:07,005 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:07,006 [Thread-806] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40164 starting to offer service
2020-04-02 05:08:07,028 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:07,028 [IPC Server listener on 34837] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 34837: starting
2020-04-02 05:08:07,040 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 34837 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:07,047 [IPC Server handler 6 on 40164] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:07,054 [Thread-806] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40164
2020-04-02 05:08:07,054 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:07,055 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:07,055 [Thread-806] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:08:07,057 [Thread-806] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:07,057 [Thread-806] WARN  common.Storage (DataStorage.java:loadDataStorage(418)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1. Reported: -2147483648. Expecting = -57.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:616)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:605)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:134)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:294)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:407)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:387)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:07,058 [Thread-806] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:07,059 [Thread-806] WARN  common.Storage (DataStorage.java:loadDataStorage(418)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2. Reported: -2147483648. Expecting = -57.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:616)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.setFieldsFromProperties(DataStorage.java:605)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.readProperties(StorageInfo.java:134)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.doTransition(DataStorage.java:714)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadStorageDirectory(DataStorage.java:294)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadDataStorage(DataStorage.java:407)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:387)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:07,059 [Thread-806] ERROR datanode.DataNode (BPServiceActor.java:run(829)) - Initialization failed for Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40164. Exiting. 
java.io.IOException: All specified directories have failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:552)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:07,059 [Thread-806] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:40164
2020-04-02 05:08:07,059 [Thread-806] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool <registering> (Datanode Uuid unassigned)
2020-04-02 05:08:07,157 [IPC Server handler 7 on 40164] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:07,162 [Thread-134] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2694)) - BPOfferService in datanode DataNode{data=null, localName='127.0.0.1:44173', datanodeUuid='null', xmitsInProgress=0} failed to connect to namenode at localhost/127.0.0.1:40164
2020-04-02 05:08:07,162 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:07,162 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:07,162 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:07,162 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 34837 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:07,164 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@6e2400b0] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:07,195 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6f74d63f{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:07,197 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@47cd2ece{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:07,197 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@ef119e4{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:07,197 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@32dcbb9b{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:07,198 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 34837
2020-04-02 05:08:07,222 [IPC Server listener on 34837] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 34837
2020-04-02 05:08:07,223 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:07,224 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:07,224 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:07,224 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 40164 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:07,224 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:07,224 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@271af6cd] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:07,225 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:07,225 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@68eca0ce] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:07,227 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 10 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:08:07,228 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:07,228 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:07,228 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:07,229 [CacheReplicationMonitor(1247449058)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:07,230 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 40164
2020-04-02 05:08:07,232 [IPC Server listener on 40164] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 40164
2020-04-02 05:08:07,234 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:07,234 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:07,244 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:07,277 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:07,278 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:07,280 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@64182381{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:07,283 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@617ece4b{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:07,283 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@57f00116{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:07,283 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2de77979{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:07,284 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:07,293 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:07,293 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:07,305 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:07,305 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 16*** DataNode upgrade with newer fsscTime in current: numDirs=2
2020-04-02 05:08:07,339 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:07,339 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,339 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,340 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:07,344 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:07,347 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:07,347 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:07,347 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:07,348 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:07,362 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:07,362 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:07,364 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:07,364 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:07,365 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:07,365 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:07,366 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:07,366 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:07,366 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:07,367 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:07,367 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:07,368 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36137
2020-04-02 05:08:07,368 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:07,370 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@681808d5] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:07,370 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@30c5f853{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:07,370 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2fd869ed{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:07,387 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@7f18d28f{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:07,388 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@172500ee{HTTP/1.1,[http/1.1]}{localhost:36137}
2020-04-02 05:08:07,388 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @18298ms
2020-04-02 05:08:07,389 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,389 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,389 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,390 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,390 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,390 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,390 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,390 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,391 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:07,392 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:07,393 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:07,393 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:07,393 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:07,393 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:07,398 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:07
2020-04-02 05:08:07,399 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:07,399 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:07,399 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:07,399 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:07,447 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:07,447 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:07,447 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:07,447 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:07,447 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:07,448 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:07,448 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:07,448 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:07,449 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:07,449 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:07,450 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:07,450 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:07,450 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:07,451 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:07,451 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:07,451 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:07,451 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:07,451 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:07,451 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:07,451 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:07,452 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:07,452 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:07,452 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:07,452 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:07,453 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:07,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:07,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:07,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:07,453 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:07,458 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:07,465 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:07,466 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:07,467 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:07,467 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:07,468 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:07,469 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:07,469 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:07,470 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:07,470 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@20db55a8 expecting start txid #32
2020-04-02 05:08:07,470 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:07,470 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:07,472 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:07,472 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804087472
2020-04-02 05:08:07,472 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:07,473 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:07,499 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:07,505 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:07,507 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:07,515 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:07,523 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:07,523 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:07,524 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:07,524 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:07,524 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:07,526 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:07,527 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:07,528 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:07,537 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:07,538 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 84 msecs
2020-04-02 05:08:07,538 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:07,538 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:07,540 [Socket Reader #1 for port 36534] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 36534
2020-04-02 05:08:07,544 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:36534 to access this namenode/service.
2020-04-02 05:08:07,544 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:07,545 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:07,545 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:07,590 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:07,602 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:07,605 [IPC Server listener on 36534] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 36534: starting
2020-04-02 05:08:07,606 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:07,626 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:36534
2020-04-02 05:08:07,627 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:07,628 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:07,634 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 6 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:07,638 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 36534 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:07,640 [CacheReplicationMonitor(439173733)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:07,656 [IPC Server handler 0 on 36534] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:07,656 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:07,776 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-db9f474a-2e38-4fff-b640-07cc707cb0f8 for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 
2020-04-02 05:08:07,794 [Thread-134] INFO  common.Storage (DataStorage.java:createStorageID(160)) - Generated new storageID DS-6af84bac-2064-49e2-a429-b5df92d81ccc for directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2 
2020-04-02 05:08:07,802 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:startDataNodes(1632)) - Starting DataNode 0 with dfs.datanode.data.dir: /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1,/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:07,803 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1
2020-04-02 05:08:07,806 [Thread-134] INFO  checker.ThrottledAsyncChecker (ThrottledAsyncChecker.java:schedule(137)) - Scheduling a check for [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2
2020-04-02 05:08:07,809 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:init(158)) - DataNode metrics system started (again)
2020-04-02 05:08:07,811 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] DataNode init, vvmode is none, do nothing
2020-04-02 05:08:07,811 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:07,811 [Thread-134] INFO  datanode.BlockScanner (BlockScanner.java:<init>(184)) - Initialized block scanner with targetBytesPerSec 1048576
2020-04-02 05:08:07,812 [Thread-134] INFO  datanode.DataNode (DataNode.java:<init>(510)) - Configured hostname is 127.0.0.1
2020-04-02 05:08:07,812 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:07,812 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1410)) - Starting DataNode with maxLockedMemory = 0
2020-04-02 05:08:07,813 [Thread-134] INFO  datanode.DataNode (DataNode.java:initDataXceiver(1158)) - Opened streaming server at /127.0.0.1:33003
2020-04-02 05:08:07,813 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(78)) - Balancing bandwidth is 10485760 bytes/s
2020-04-02 05:08:07,813 [Thread-134] INFO  datanode.DataNode (DataXceiverServer.java:<init>(79)) - Number threads for balancing is 50
2020-04-02 05:08:07,814 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:07,815 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:07,822 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.datanode is not defined
2020-04-02 05:08:07,822 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:07,823 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:07,823 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context datanode
2020-04-02 05:08:07,823 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:07,824 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:07,824 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 45412
2020-04-02 05:08:07,824 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:07,831 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@77a48038{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:07,832 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@5e334c02{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:07,836 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@72855905{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/datanode/,AVAILABLE}{/datanode}
2020-04-02 05:08:07,836 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@72093c49{HTTP/1.1,[http/1.1]}{localhost:45412}
2020-04-02 05:08:07,837 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @18746ms
2020-04-02 05:08:07,861 [Thread-134] INFO  web.DatanodeHttpServer (DatanodeHttpServer.java:start(255)) - Listening HTTP traffic on /127.0.0.1:40241
2020-04-02 05:08:07,862 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1438)) - dnUserName = root
2020-04-02 05:08:07,862 [Thread-134] INFO  datanode.DataNode (DataNode.java:startDataNode(1439)) - supergroup = supergroup
2020-04-02 05:08:07,862 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@15cd573] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:07,862 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:07,863 [Socket Reader #1 for port 41902] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 41902
2020-04-02 05:08:07,867 [Thread-134] INFO  datanode.DataNode (DataNode.java:initIpcServer(1044)) - Opened IPC server at /127.0.0.1:41902
2020-04-02 05:08:07,871 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:refreshNamenodes(149)) - Refresh request received for nameservices: null
2020-04-02 05:08:07,872 [Thread-134] INFO  datanode.DataNode (BlockPoolManager.java:doRefreshNamenodes(210)) - Starting BPOfferServices for nameservices: <default>
2020-04-02 05:08:07,872 [Thread-869] INFO  datanode.DataNode (BPServiceActor.java:run(810)) - Block pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36534 starting to offer service
2020-04-02 05:08:07,873 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:07,873 [IPC Server listener on 41902] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 41902: starting
2020-04-02 05:08:07,908 [Thread-134] INFO  datanode.DataNode (DataNode.java:runDatanodeDaemon(2674)) - [msx-restart] DataNode 41902 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:07,909 [Thread-869] INFO  datanode.DataNode (BPOfferService.java:verifyAndSetNamespaceInfo(378)) - Acknowledging ACTIVE Namenode during handshakeBlock pool <registering> (Datanode Uuid unassigned) service to localhost/127.0.0.1:36534
2020-04-02 05:08:07,919 [IPC Server handler 3 on 36534] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:07,919 [Thread-869] INFO  common.Storage (DataStorage.java:getParallelVolumeLoadThreadsNum(354)) - Using 2 threads to upgrade data directories (dfs.datanode.parallel.volumes.load.threads.num=2, dataDirs=2)
2020-04-02 05:08:07,921 [Thread-869] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:07,922 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:07,922 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:07,924 [Thread-869] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,196 [Thread-869] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:08,197 [Thread-869] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:08,197 [Thread-869] WARN  common.Storage (BlockPoolSliceStorage.java:loadBpStorageDirectories(228)) - Failed to analyze storage directories for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804087472
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:08,197 [Thread-869] WARN  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(466)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1 for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804087472
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:08,223 [IPC Server handler 4 on 36534] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:08,224 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2702)) - dnInfo.length != numDataNodes
2020-04-02 05:08:08,224 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2623)) - Waiting for cluster to become active
2020-04-02 05:08:08,266 [Thread-869] INFO  common.Storage (BlockPoolSliceStorage.java:recoverTransitionRead(251)) - Analyzing storage directories for bpid BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:08,267 [Thread-869] INFO  common.Storage (Storage.java:lock(864)) - Locking is disabled for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/BP-306798092-172.17.0.14-1585804070968
2020-04-02 05:08:08,267 [Thread-869] WARN  common.Storage (BlockPoolSliceStorage.java:loadBpStorageDirectories(228)) - Failed to analyze storage directories for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804087472
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:08,267 [Thread-869] WARN  common.Storage (DataStorage.java:loadBlockPoolSliceStorage(466)) - Failed to add storage directory [DISK]file:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2 for block pool BP-306798092-172.17.0.14-1585804070968
java.io.IOException: Datanode state: LV = -57 CTime = 9223372036854775807 is newer than the namespace state: LV = -64 CTime = 1585804087472
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.doTransition(BlockPoolSliceStorage.java:415)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadStorageDirectory(BlockPoolSliceStorage.java:181)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.loadBpStorageDirectories(BlockPoolSliceStorage.java:224)
	at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceStorage.recoverTransitionRead(BlockPoolSliceStorage.java:253)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.loadBlockPoolSliceStorage(DataStorage.java:454)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.addStorageLocations(DataStorage.java:389)
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:551)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:08,268 [Thread-869] ERROR datanode.DataNode (BPServiceActor.java:run(829)) - Initialization failed for Block pool <registering> (Datanode Uuid FixedDatanodeUuid) service to localhost/127.0.0.1:36534. Exiting. 
java.io.IOException: All specified directories have failed to load.
	at org.apache.hadoop.hdfs.server.datanode.DataStorage.recoverTransitionRead(DataStorage.java:552)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initStorage(DataNode.java:1729)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.initBlockPool(DataNode.java:1689)
	at org.apache.hadoop.hdfs.server.datanode.BPOfferService.verifyAndSetNamespaceInfo(BPOfferService.java:390)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.connectToNNAndHandshake(BPServiceActor.java:280)
	at org.apache.hadoop.hdfs.server.datanode.BPServiceActor.run(BPServiceActor.java:817)
	at java.lang.Thread.run(Thread.java:748)
2020-04-02 05:08:08,268 [Thread-869] WARN  datanode.DataNode (BPServiceActor.java:run(853)) - Ending block pool service for: Block pool <registering> (Datanode Uuid FixedDatanodeUuid) service to localhost/127.0.0.1:36534
2020-04-02 05:08:08,269 [Thread-869] INFO  datanode.DataNode (BlockPoolManager.java:remove(102)) - Removed Block pool <registering> (Datanode Uuid FixedDatanodeUuid)
2020-04-02 05:08:08,325 [IPC Server handler 6 on 36534] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:08,325 [Thread-134] WARN  hdfs.MiniDFSCluster (MiniDFSCluster.java:shouldWait(2694)) - BPOfferService in datanode DataNode{data=null, localName='127.0.0.1:33003', datanodeUuid='FixedDatanodeUuid', xmitsInProgress=0} failed to connect to namenode at localhost/127.0.0.1:36534
2020-04-02 05:08:08,326 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:08,326 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:08,326 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdownDataNode(2046)) - Shutting down DataNode 0
2020-04-02 05:08:08,326 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(1991)) - [msx-restart] DataNode 41902 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:08,326 [org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@7ba2df77] INFO  datanode.DataNode (DataXceiverServer.java:closeAllPeers(281)) - Closing all peers.
2020-04-02 05:08:08,349 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@72855905{/,null,UNAVAILABLE}{/datanode}
2020-04-02 05:08:08,350 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@72093c49{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:08,350 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@5e334c02{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:08,351 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@77a48038{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:08,351 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 41902
2020-04-02 05:08:08,353 [IPC Server listener on 41902] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 41902
2020-04-02 05:08:08,369 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:08,378 [Thread-134] INFO  datanode.DataNode (DataNode.java:shutdown(2159)) - Shutdown complete.
2020-04-02 05:08:08,378 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:08,379 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 36534 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:08,379 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:08,379 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:08,379 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@b0911dd] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:08,379 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@d2ca5ce] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:08,383 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 7 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 2 2 
2020-04-02 05:08:08,384 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:08,384 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:08,385 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:08,388 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 36534
2020-04-02 05:08:08,388 [IPC Server listener on 36534] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 36534
2020-04-02 05:08:08,392 [CacheReplicationMonitor(439173733)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:08,407 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:08,407 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:08,414 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:08,453 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:08,463 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:08,465 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@7f18d28f{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:08,472 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@172500ee{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:08,473 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2fd869ed{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:08,473 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@30c5f853{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:08,478 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping DataNode metrics system...
2020-04-02 05:08:08,478 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - DataNode metrics system stopped.
2020-04-02 05:08:08,480 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - DataNode metrics system shutdown complete.
2020-04-02 05:08:08,501 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:08,501 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 17*** NameNode upgrade with no edits file: numDirs=2
2020-04-02 05:08:08,550 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:08,550 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,551 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,551 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:08,554 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:08,558 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:08,558 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:08,558 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:08,559 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:08,563 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3fce4a50] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:08,563 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:08,564 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,565 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:08,565 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:08,566 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,566 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:08,567 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:08,567 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:08,567 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:08,568 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:08,568 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:08,569 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36040
2020-04-02 05:08:08,569 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:08,575 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@6984743e{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:08,575 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@2e358098{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:08,580 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@687d8e51{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:08,581 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@d75faaa{HTTP/1.1,[http/1.1]}{localhost:36040}
2020-04-02 05:08:08,581 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @19491ms
2020-04-02 05:08:08,581 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,582 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,582 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,582 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,582 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,583 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,583 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,583 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,584 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:08,585 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:08,585 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:08,585 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:08,585 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:08,586 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:08,586 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:08,586 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:08,586 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:08,587 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:08,587 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:08,587 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:08,588 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:08
2020-04-02 05:08:08,588 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:08,588 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,588 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:08,588 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:08,600 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:08,601 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:08,601 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:08,601 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:08,601 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:08,601 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:08,602 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:08,606 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:08,606 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,606 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:08,606 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:08,611 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:08,611 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:08,611 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:08,611 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:08,612 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:08,612 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:08,612 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:08,612 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,612 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:08,612 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:08,613 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:08,613 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:08,614 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:08,614 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:08,614 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:08,614 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:08,614 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,614 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:08,615 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:08,616 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,617 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,619 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:08,620 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:08,620 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:08,620 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 32 but unable to find any edit logs containing txid 32
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1709)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1684)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:701)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:323)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,621 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:08,622 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@687d8e51{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:08,640 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@d75faaa{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:08,640 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@2e358098{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:08,641 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@6984743e{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:08,646 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:08,647 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:08,647 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:08,647 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.IOException: Gap in transactions. Expected to be able to read up until at least txid 32 but unable to find any edit logs containing txid 32
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.checkForGaps(FSEditLog.java:1751)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1709)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLog.selectInputStreams(FSEditLog.java:1684)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:701)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:323)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,649 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:08,651 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:08,652 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:08,652 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 18*** NameNode upgrade with no image file: numDirs=2
2020-04-02 05:08:08,702 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:08,703 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,703 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,703 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:08,706 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:08,709 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:08,710 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:08,710 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:08,710 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:08,715 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@321de5fc] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:08,715 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:08,728 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,730 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:08,731 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:08,731 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,732 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:08,733 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:08,733 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:08,733 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:08,734 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:08,734 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:08,735 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 38752
2020-04-02 05:08:08,735 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:08,736 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@51d9e420{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:08,737 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@75bebbee{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:08,743 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@15e68582{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:08,746 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@1f72e0be{HTTP/1.1,[http/1.1]}{localhost:38752}
2020-04-02 05:08:08,747 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @19657ms
2020-04-02 05:08:08,749 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,750 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,752 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,753 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,753 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,754 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,754 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,754 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,755 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:08,756 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:08,765 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:08,765 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:08,765 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:08,766 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:08,766 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:08,766 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:08,766 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:08,766 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:08,766 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:08,767 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:08,767 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:08
2020-04-02 05:08:08,767 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:08,767 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,767 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:08,767 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:08,780 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:08,781 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:08,781 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:08,782 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:08,782 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:08,782 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:08,782 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:08,782 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:08,782 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,782 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:08,782 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:08,788 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:08,788 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:08,788 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:08,788 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:08,788 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:08,788 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:08,788 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:08,788 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,789 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:08,789 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:08,790 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:08,790 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:08,791 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:08,791 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:08,791 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:08,791 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:08,791 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,792 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:08,792 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:08,795 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,799 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,800 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:08,801 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.FileNotFoundException: No valid image files found
	at org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector.getLatestImages(FSImageTransactionalStorageInspector.java:158)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:672)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,802 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:08,806 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@15e68582{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:08,819 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@1f72e0be{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:08,819 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@75bebbee{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:08,820 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@51d9e420{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:08,823 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:08,823 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:08,823 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:08,824 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.FileNotFoundException: No valid image files found
	at org.apache.hadoop.hdfs.server.namenode.FSImageTransactionalStorageInspector.getLatestImages(FSImageTransactionalStorageInspector.java:158)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:672)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.doUpgrade(FSImage.java:443)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:310)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:329)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,824 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:08,834 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:08,835 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:08,835 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 19*** NameNode upgrade with corrupt version file: numDirs=2
2020-04-02 05:08:08,868 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:08,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,868 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,869 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:08,871 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:08,879 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:08,880 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:08,880 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:08,880 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:08,885 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@5834ab10] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:08,891 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:08,892 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,894 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:08,894 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:08,895 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:08,896 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:08,896 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:08,896 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:08,896 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:08,899 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:08,899 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:08,900 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 36708
2020-04-02 05:08:08,900 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:08,924 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@e10c07f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:08,924 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@799ac581{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:08,928 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6611faf6{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:08,928 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@14afdc66{HTTP/1.1,[http/1.1]}{localhost:36708}
2020-04-02 05:08:08,928 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @19838ms
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,929 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,930 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:08,930 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:08,938 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:08,939 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:08,945 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:08,946 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:08,946 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:08,946 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:08,946 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:08,946 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:08,947 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:08,947 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:08,947 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:08,947 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:08,948 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:08
2020-04-02 05:08:08,948 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:08,948 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,948 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:08,948 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:08,959 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:08,960 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:08,962 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:08,963 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:08,963 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:08,963 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,963 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:08,963 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:08,971 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:08,971 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:08,971 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:08,971 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:08,971 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:08,971 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:08,971 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:08,972 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,972 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:08,972 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:08,973 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:08,974 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:08,974 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:08,974 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:08,974 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:08,974 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:08,974 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:08,974 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:08,975 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:08,977 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:08,977 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: file VERSION has layoutVersion missing.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.getProperty(StorageInfo.java:240)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:203)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:340)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,978 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:08,980 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6611faf6{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:08,986 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@14afdc66{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:08,986 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@799ac581{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:08,987 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@e10c07f{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:08,987 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:08,988 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:08,988 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:08,988 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: file VERSION has layoutVersion missing.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.getProperty(StorageInfo.java:240)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:203)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:340)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:08,989 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:08,992 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:08,993 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:08,994 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 20*** NameNode upgrade with old layout version in current: numDirs=2
2020-04-02 05:08:09,028 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:09,029 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,029 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,029 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:09,036 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:09,045 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:09,046 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:09,046 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:09,046 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:09,052 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@3a9d3895] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:09,053 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:09,053 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,055 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:09,055 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:09,055 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,056 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:09,057 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:09,057 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:09,057 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:09,058 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:09,058 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:09,058 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 37110
2020-04-02 05:08:09,058 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:09,060 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1bb19fbe{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:09,060 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@1e016b04{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:09,063 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@369dd27f{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:09,064 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@5b20bf{HTTP/1.1,[http/1.1]}{localhost:37110}
2020-04-02 05:08:09,065 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @19974ms
2020-04-02 05:08:09,065 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,065 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,065 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,065 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,066 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,066 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,066 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,066 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:09,067 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:09,068 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:09,068 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:09,068 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:09,068 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:09,068 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:09,069 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:09
2020-04-02 05:08:09,069 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:09,069 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,069 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:09,069 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:09,093 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:09,093 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:09,093 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:09,094 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:09,095 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:09,095 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:09,095 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:09,095 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,095 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:09,095 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:09,112 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:09,112 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:09,112 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:09,112 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:09,112 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:09,113 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:09,113 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:09,113 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,113 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:09,113 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:09,117 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:09,117 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:09,117 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:09,118 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:09,118 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:09,118 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:09,118 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,118 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:09,118 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:09,123 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,125 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,125 [Thread-134] ERROR common.Storage (Storage.java:checkVersionUpgradable(1119)) - *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
2020-04-02 05:08:09,126 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
java.io.IOException: *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
	at org.apache.hadoop.hdfs.server.common.Storage.checkVersionUpgradable(Storage.java:1120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:353)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:09,126 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:09,127 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@369dd27f{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:09,128 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@5b20bf{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:09,128 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1e016b04{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:09,128 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@1bb19fbe{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:09,129 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:09,133 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:09,133 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:09,134 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

java.io.IOException: *********** Upgrade is not supported from this  older version -15 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -15 and latest layout version this software version can upgrade from is -16. ************
	at org.apache.hadoop.hdfs.server.common.Storage.checkVersionUpgradable(Storage.java:1120)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:250)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:353)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:09,134 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:09,136 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:09,137 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:09,137 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 21*** NameNode upgrade with future layout version in current: numDirs=2
2020-04-02 05:08:09,180 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:09,180 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,180 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,180 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:09,183 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:09,185 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:09,185 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:09,185 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:09,185 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:09,189 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@68fef68c] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:09,189 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:09,191 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,193 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:09,194 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:09,194 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,195 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:09,195 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:09,196 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:09,196 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:09,197 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:09,197 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:09,197 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 37608
2020-04-02 05:08:09,198 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:09,199 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@4ed3f5a5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:09,200 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@7ffd6144{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:09,213 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@6e68fc40{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:09,214 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@35d7f056{HTTP/1.1,[http/1.1]}{localhost:37608}
2020-04-02 05:08:09,215 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @20125ms
2020-04-02 05:08:09,215 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,215 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,215 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,216 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,216 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,216 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,216 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,216 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,217 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:09,218 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:09,219 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:09,219 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:09,219 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:09,219 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:09,220 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:09
2020-04-02 05:08:09,220 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:09,220 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,220 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:09,220 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:09,233 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:09,233 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:09,233 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:09,233 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:09,233 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:09,234 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:09,234 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:09,235 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,235 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:09,235 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:09,239 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:09,240 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:09,240 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:09,240 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:09,240 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:09,240 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:09,240 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:09,240 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,240 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:09,240 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:09,242 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:09,242 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:09,242 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:09,243 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:09,243 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:09,243 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:09,243 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,244 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:09,244 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:09,248 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,249 [Thread-134] WARN  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(716)) - Encountered exception loading fsimage
org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. Reported: -2147483648. Expecting = -64.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:09,250 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 0 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:09,252 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@6e68fc40{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:09,253 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@35d7f056{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:09,254 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@7ffd6144{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:09,254 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@4ed3f5a5{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:09,255 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:09,270 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:09,270 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
2020-04-02 05:08:09,270 [Thread-134] ERROR hdfs.MiniDFSCluster (MiniDFSCluster.java:initMiniDFSCluster(894)) - IOE creating namenodes. Permissions dump:
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data
	permissions: ----
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs
	permissions: drwx
path '/root/hadoop-3.1.2-src/hadoop-hdfs-project': 
	absolute:/root/hadoop-3.1.2-src/hadoop-hdfs-project
	permissions: drwx
path '/root/hadoop-3.1.2-src': 
	absolute:/root/hadoop-3.1.2-src
	permissions: drwx
path '/root': 
	absolute:/root
	permissions: drwx
path '/': 
	absolute:/
	permissions: drwx

org.apache.hadoop.hdfs.server.common.IncorrectVersionException: Unexpected version of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1. Reported: -2147483648. Expecting = -64.
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setLayoutVersion(StorageInfo.java:206)
	at org.apache.hadoop.hdfs.server.common.StorageInfo.setFieldsFromProperties(StorageInfo.java:158)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.setFieldsFromProperties(NNStorage.java:643)
	at org.apache.hadoop.hdfs.server.namenode.NNStorage.readProperties(NNStorage.java:676)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverStorageDirs(FSImage.java:388)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:227)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFSImage(FSNamesystem.java:1097)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.loadFromDisk(FSNamesystem.java:714)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:636)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:698)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:945)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:914)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1673)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:1313)
	at org.apache.hadoop.hdfs.MiniDFSCluster.configureNameService(MiniDFSCluster.java:1082)
	at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNodesAndSetConf(MiniDFSCluster.java:957)
	at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:889)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:517)
	at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:476)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:167)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.startNameNodeShouldFail(TestDFSUpgrade.java:146)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:366)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:09,271 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:09,272 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:startNameNodeShouldFail(184)) - Successfully detected expected NameNode startup failure.
2020-04-02 05:08:09,280 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(74)) - ============================================================
2020-04-02 05:08:09,281 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:log(75)) - ***TEST 22*** Normal NameNode upgrade: numDirs=4
2020-04-02 05:08:09,342 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:<init>(492)) - starting cluster: numNameNodes=1, numDataNodes=0
2020-04-02 05:08:09,342 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,342 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,343 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,343 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,343 [Thread-134] INFO  namenode.NameNode (NameNode.java:createNameNode(1613)) - createNameNode [-upgrade]
2020-04-02 05:08:09,345 [Thread-134] INFO  impl.MetricsConfig (MetricsConfig.java:loadFirst(121)) - loaded properties from hadoop-metrics2.properties
2020-04-02 05:08:09,346 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:startTimer(374)) - Scheduled Metric snapshot period at 0 second(s).
2020-04-02 05:08:09,347 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:start(191)) - NameNode metrics system started
2020-04-02 05:08:09,347 [Thread-134] INFO  conf.Configured (ReconfAgent.java:performReconf(74)) - [msx-restart] NameNode init, vvmode is none, do nothing
2020-04-02 05:08:09,347 [Thread-134] INFO  namenode.NameNodeUtils (NameNodeUtils.java:getClientNamenodeAddress(79)) - fs.defaultFS is hdfs://127.0.0.1:0
2020-04-02 05:08:09,352 [org.apache.hadoop.util.JvmPauseMonitor$Monitor@6cfbe721] INFO  util.JvmPauseMonitor (JvmPauseMonitor.java:run(188)) - Starting JVM pause monitor
2020-04-02 05:08:09,353 [Thread-134] INFO  hdfs.DFSUtil (DFSUtil.java:httpServerTemplateForNNAndJN(1605)) - Starting Web-server for hdfs at: http://localhost:0
2020-04-02 05:08:09,353 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,355 [Thread-134] INFO  server.AuthenticationFilter (AuthenticationFilter.java:constructSecretProvider(240)) - Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-04-02 05:08:09,355 [Thread-134] INFO  http.HttpRequestLog (HttpRequestLog.java:getRequestLog(81)) - Http request log for http.requests.namenode is not defined
2020-04-02 05:08:09,355 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:getWebAppsPath(1052)) - Web server is in development mode. Resources will be read from the source tree.
2020-04-02 05:08:09,356 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addGlobalFilter(970)) - Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-04-02 05:08:09,356 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(943)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-04-02 05:08:09,356 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-04-02 05:08:09,356 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addFilter(953)) - Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-04-02 05:08:09,358 [Thread-134] INFO  http.HttpServer2 (NameNodeHttpServer.java:initWebHdfs(100)) - Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-04-02 05:08:09,358 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:addJerseyResourcePackage(789)) - addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-04-02 05:08:09,358 [Thread-134] INFO  http.HttpServer2 (HttpServer2.java:bindListener(1186)) - Jetty bound to port 41451
2020-04-02 05:08:09,359 [Thread-134] INFO  server.Server (Server.java:doStart(351)) - jetty-9.3.24.v20180605, build timestamp: 2018-06-05T17:11:56Z, git hash: 84205aa28f11a4f31f2a3b86d1bba2cc8ab69827
2020-04-02 05:08:09,360 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@e8227c{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,AVAILABLE}
2020-04-02 05:08:09,361 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.s.ServletContextHandler@3a9df139{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,AVAILABLE}
2020-04-02 05:08:09,365 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStart(781)) - Started o.e.j.w.WebAppContext@76770e82{/,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/hdfs/,AVAILABLE}{/hdfs}
2020-04-02 05:08:09,366 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStart(278)) - Started ServerConnector@570615ed{HTTP/1.1,[http/1.1]}{localhost:41451}
2020-04-02 05:08:09,366 [Thread-134] INFO  server.Server (Server.java:doStart(419)) - Started @20276ms
2020-04-02 05:08:09,366 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,367 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,368 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,369 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,369 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,370 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:newInstance(227)) - Edit logging is async:true
2020-04-02 05:08:09,370 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(749)) - KeyProvider: null
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(121)) - fsLock is fair: true
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystemLock.java:<init>(139)) - Detailed lock hold time metrics enabled: false
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(774)) - fsOwner             = root (auth:SIMPLE)
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(775)) - supergroup          = supergroup
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(776)) - isPermissionEnabled = true
2020-04-02 05:08:09,371 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:<init>(787)) - HA Enabled: false
2020-04-02 05:08:09,372 [Thread-134] INFO  common.Util (Util.java:isDiskStatsEnabled(394)) - dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling
2020-04-02 05:08:09,372 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(301)) - dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000
2020-04-02 05:08:09,372 [Thread-134] INFO  blockmanagement.DatanodeManager (DatanodeManager.java:<init>(309)) - dfs.namenode.datanode.registration.ip-hostname-check=true
2020-04-02 05:08:09,372 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(79)) - dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-04-02 05:08:09,372 [Thread-134] INFO  blockmanagement.BlockManager (InvalidateBlocks.java:printBlockDeletionTime(85)) - The block deletion will start around 2020 Apr 02 05:08:09
2020-04-02 05:08:09,373 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map BlocksMap
2020-04-02 05:08:09,373 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,373 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 2.0% max memory 1.9 GB = 39.3 MB
2020-04-02 05:08:09,373 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^22 = 4194304 entries
2020-04-02 05:08:09,386 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:createBlockTokenSecretManager(579)) - dfs.block.access.token.enable = false
2020-04-02 05:08:09,386 [Thread-134] INFO  Configuration.deprecation (Configuration.java:logDeprecation(1394)) - No unit for dfs.namenode.safemode.extension(0) assuming MILLISECONDS
2020-04-02 05:08:09,386 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(161)) - dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-04-02 05:08:09,387 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(162)) - dfs.namenode.safemode.min.datanodes = 0
2020-04-02 05:08:09,387 [Thread-134] INFO  blockmanagement.BlockManagerSafeMode (BlockManagerSafeMode.java:<init>(164)) - dfs.namenode.safemode.extension = 0
2020-04-02 05:08:09,387 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(565)) - defaultReplication         = 0
2020-04-02 05:08:09,387 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(566)) - maxReplication             = 512
2020-04-02 05:08:09,387 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(567)) - minReplication             = 1
2020-04-02 05:08:09,388 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(568)) - maxReplicationStreams      = 2
2020-04-02 05:08:09,388 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(569)) - redundancyRecheckInterval  = 3000ms
2020-04-02 05:08:09,388 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(570)) - encryptDataTransfer        = false
2020-04-02 05:08:09,388 [Thread-134] INFO  blockmanagement.BlockManager (BlockManager.java:<init>(571)) - maxNumBlocksToLog          = 1000
2020-04-02 05:08:09,388 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map INodeMap
2020-04-02 05:08:09,389 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,389 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 1.0% max memory 1.9 GB = 19.6 MB
2020-04-02 05:08:09,389 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^21 = 2097152 entries
2020-04-02 05:08:09,403 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(285)) - ACLs enabled? false
2020-04-02 05:08:09,403 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(289)) - POSIX ACL inheritance enabled? true
2020-04-02 05:08:09,403 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:<init>(293)) - XAttrs enabled? true
2020-04-02 05:08:09,403 [Thread-134] INFO  namenode.NameNode (FSDirectory.java:<init>(357)) - Caching file names occurring more than 10 times
2020-04-02 05:08:09,403 [Thread-134] INFO  snapshot.SnapshotManager (SnapshotManager.java:<init>(124)) - Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true, maxSnapshotLimit: 65536
2020-04-02 05:08:09,403 [Thread-134] INFO  snapshot.SnapshotManager (DirectoryDiffListFactory.java:init(43)) - SkipList is disabled
2020-04-02 05:08:09,403 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map cachedBlocks
2020-04-02 05:08:09,403 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,403 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.25% max memory 1.9 GB = 4.9 MB
2020-04-02 05:08:09,403 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^19 = 524288 entries
2020-04-02 05:08:09,405 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(76)) - NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-04-02 05:08:09,405 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(78)) - NNTop conf: dfs.namenode.top.num.users = 10
2020-04-02 05:08:09,405 [Thread-134] INFO  metrics.TopMetrics (TopMetrics.java:logConf(80)) - NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-04-02 05:08:09,406 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(990)) - Retry cache on namenode is enabled
2020-04-02 05:08:09,406 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:initRetryCache(998)) - Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-04-02 05:08:09,406 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(395)) - Computing capacity for map NameNodeRetryCache
2020-04-02 05:08:09,406 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(396)) - VM type       = 64-bit
2020-04-02 05:08:09,406 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(397)) - 0.029999999329447746% max memory 1.9 GB = 603.0 KB
2020-04-02 05:08:09,406 [Thread-134] INFO  util.GSet (LightWeightGSet.java:computeCapacity(402)) - capacity      = 2^16 = 65536 entries
2020-04-02 05:08:09,412 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,413 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,415 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,416 [Thread-134] INFO  common.Storage (Storage.java:tryLock(905)) - Lock on /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/in_use.lock acquired by nodename 14279@8480ff0b2e43
2020-04-02 05:08:09,416 [Thread-134] INFO  common.Storage (NNStorage.java:processStartupOptionsForUpgrade(923)) - Using clusterid: testClusterID
2020-04-02 05:08:09,418 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current
2020-04-02 05:08:09,418 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current
2020-04-02 05:08:09,419 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current
2020-04-02 05:08:09,419 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:recoverUnfinalizedSegments(396)) - Recovering unfinalized segments in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current
2020-04-02 05:08:09,420 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImageFile(782)) - Planning to load image: FSImageFile(file=/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031, cpktTxId=0000000000000000031)
2020-04-02 05:08:09,421 [Thread-134] INFO  namenode.FSImageFormatPBINode (FSImageFormatPBINode.java:loadINodeSection(233)) - Loading 4 INodes.
2020-04-02 05:08:09,422 [Thread-134] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:load(183)) - Loaded FSImage in 0 seconds.
2020-04-02 05:08:09,422 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadFSImage(951)) - Loaded image for txid 31 from /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000031
2020-04-02 05:08:09,422 [Thread-134] INFO  namenode.FSImage (FSImage.java:loadEdits(887)) - Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@1cc7423c expecting start txid #32
2020-04-02 05:08:09,422 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(158)) - Start loading edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 maxTxnsToRead = 9223372036854775807
2020-04-02 05:08:09,423 [Thread-134] INFO  namenode.RedundantEditLogInputStream (RedundantEditLogInputStream.java:nextOp(177)) - Fast-forwarding stream '/root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061' to transaction ID 32
2020-04-02 05:08:09,446 [Thread-134] INFO  namenode.FSImage (FSEditLogLoader.java:loadFSEdits(162)) - Edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/edits_0000000000000000032-0000000000000000061, /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000032-0000000000000000061 of size 1694 edits # 30 loaded in 0 seconds
2020-04-02 05:08:09,446 [Thread-134] INFO  namenode.FSImage (FSImage.java:doUpgrade(454)) - Starting upgrade of local storage directories.
   old LV = -64; old CTime = 1585804070968.
   new LV = -64; new CTime = 1585804089446
2020-04-02 05:08:09,446 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:09,448 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:09,448 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3
2020-04-02 05:08:09,455 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doPreUpgrade(117)) - Starting upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4
2020-04-02 05:08:09,458 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:09,458 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:09,458 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:09,458 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(449)) - Saving image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 using no compression
2020-04-02 05:08:09,475 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:09,479 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:09,486 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:09,494 [FSImageSaver for /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 of type IMAGE_AND_EDITS] INFO  namenode.FSImageFormatProtobuf (FSImageFormatProtobuf.java:save(453)) - Image file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/fsimage.ckpt_0000000000000000061 of size 910 bytes saved in 0 seconds .
2020-04-02 05:08:09,514 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:09,514 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:09,514 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3
2020-04-02 05:08:09,514 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4
2020-04-02 05:08:09,515 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:09,515 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:09,515 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3
2020-04-02 05:08:09,516 [Thread-134] INFO  namenode.FSImageTransactionalStorageInspector (FSImageTransactionalStorageInspector.java:inspectDirectory(78)) - No version file in /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4
2020-04-02 05:08:09,516 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:09,520 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:09,522 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3
2020-04-02 05:08:09,526 [Thread-134] INFO  namenode.NNUpgradeUtil (NNUpgradeUtil.java:doUpgrade(184)) - Performing upgrade of storage directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4
2020-04-02 05:08:09,528 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFSImage(1102)) - Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-04-02 05:08:09,531 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:startLogSegment(1361)) - Starting log segment at 62
2020-04-02 05:08:09,587 [Thread-134] INFO  namenode.NameCache (NameCache.java:initialized(143)) - initialized with 0 entries 0 lookups
2020-04-02 05:08:09,588 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:loadFromDisk(721)) - Finished loading FSImage in 181 msecs
2020-04-02 05:08:09,588 [Thread-134] INFO  namenode.NameNode (NameNodeRpcServer.java:<init>(446)) - RPC server is binding to localhost:0
2020-04-02 05:08:09,588 [Thread-134] INFO  ipc.CallQueueManager (CallQueueManager.java:<init>(84)) - Using callQueue: class java.util.concurrent.LinkedBlockingQueue queueCapacity: 1000 scheduler: class org.apache.hadoop.ipc.DefaultRpcScheduler
2020-04-02 05:08:09,589 [Socket Reader #1 for port 33402] INFO  ipc.Server (Server.java:run(1070)) - Starting Socket Reader #1 for port 33402
2020-04-02 05:08:09,599 [Thread-134] INFO  namenode.NameNode (NameNode.java:initialize(710)) - Clients are to use localhost:33402 to access this namenode/service.
2020-04-02 05:08:09,599 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:registerMBean(5000)) - Registered FSNamesystemState, ReplicatedBlocksState and ECBlockGroupsState MBeans.
2020-04-02 05:08:09,600 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 in configuration.
2020-04-02 05:08:09,600 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2 in configuration.
2020-04-02 05:08:09,600 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3 in configuration.
2020-04-02 05:08:09,600 [Thread-134] INFO  common.Util (Util.java:stringAsURI(99)) - Assuming 'file' scheme for path /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4 in configuration.
2020-04-02 05:08:09,655 [Thread-134] INFO  namenode.LeaseManager (LeaseManager.java:getNumUnderConstructionBlocks(171)) - Number of blocks under construction: 0
2020-04-02 05:08:09,658 [Thread-134] INFO  hdfs.StateChange (BlockManagerSafeMode.java:reportStatus(607)) - STATE* Safe mode ON. 
The reported blocks 0 needs additional 15 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-04-02 05:08:09,662 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1310)) - IPC Server Responder: starting
2020-04-02 05:08:09,677 [IPC Server listener on 33402] INFO  ipc.Server (Server.java:run(1149)) - IPC Server listener on 33402: starting
2020-04-02 05:08:09,696 [Thread-134] INFO  namenode.NameNode (NameNode.java:startCommonServices(816)) - NameNode RPC up at: localhost/127.0.0.1:33402
2020-04-02 05:08:09,696 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:startActiveServices(1214)) - Starting services required for active state
2020-04-02 05:08:09,696 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(769)) - Initializing quota with 4 thread(s)
2020-04-02 05:08:09,697 [Thread-134] INFO  namenode.FSDirectory (FSDirectory.java:updateCountForQuota(778)) - Quota initialization completed in 1 milliseconds
name space=6
storage space=16384
storage types=RAM_DISK=0, SSD=0, DISK=0, ARCHIVE=0, PROVIDED=0
2020-04-02 05:08:09,698 [Thread-134] INFO  namenode.NameNode (NameNode.java:<init>(963)) - [msx-restart] NameNode 33402 start, check after start, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:09,705 [CacheReplicationMonitor(374551531)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(160)) - Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-04-02 05:08:09,755 [IPC Server handler 0 on 33402] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=datanodeReport	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:09,756 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:waitActive(2678)) - Cluster is active
2020-04-02 05:08:09,761 [IPC Server handler 2 on 33402] INFO  hdfs.StateChange (FSNamesystem.java:enterSafeMode(4593)) - STATE* Safe mode is ON.
It was turned on manually. Use "hdfs dfsadmin -safemode leave" to turn safe mode off.
2020-04-02 05:08:09,762 [IPC Server handler 2 on 33402] INFO  FSNamesystem.audit (FSNamesystem.java:logAuditMessage(7947)) - allowed=true	ugi=root (auth:SIMPLE)	ip=/127.0.0.1	cmd=safemode_enter	src=null	dst=null	perm=null	proto=rpc
2020-04-02 05:08:09,779 [IPC Server handler 3 on 33402] INFO  namenode.NameNode (NameNodeRpcServer.java:rollingUpgrade(1333)) - rollingUpgrade PREPARE
2020-04-02 05:08:09,779 [IPC Server handler 3 on 33402] INFO  ipc.Server (Server.java:logException(2722)) - IPC Server handler 3 on 33402, call Call#108 Retry#0 org.apache.hadoop.hdfs.protocol.ClientProtocol.rollingUpgrade from 127.0.0.1:40174
org.apache.hadoop.hdfs.server.common.InconsistentFSStateException: Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)
2020-04-02 05:08:09,781 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:testUpgrade(392)) - The exception is expected.
org.apache.hadoop.ipc.RemoteException(org.apache.hadoop.hdfs.server.common.InconsistentFSStateException): Directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1 is in an inconsistent state: previous fs state should not exist during upgrade. Finalize or rollback first.
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:411)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.checkUpgrade(FSImage.java:418)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgradeInternalForNonHA(FSNamesystem.java:6750)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startRollingUpgrade(FSNamesystem.java:6706)
	at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.rollingUpgrade(NameNodeRpcServer.java:1338)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.rollingUpgrade(ClientNamenodeProtocolServerSideTranslatorPB.java:923)
	at org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:523)
	at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:991)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:872)
	at org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:818)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:422)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1729)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:2678)

	at org.apache.hadoop.ipc.Client.getRpcResponse(Client.java:1511)
	at org.apache.hadoop.ipc.Client.call(Client.java:1457)
	at org.apache.hadoop.ipc.Client.call(Client.java:1367)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:228)
	at org.apache.hadoop.ipc.ProtobufRpcEngine$Invoker.invoke(ProtobufRpcEngine.java:116)
	at com.sun.proxy.$Proxy24.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolTranslatorPB.rollingUpgrade(ClientNamenodeProtocolTranslatorPB.java:857)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:422)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeMethod(RetryInvocationHandler.java:165)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invoke(RetryInvocationHandler.java:157)
	at org.apache.hadoop.io.retry.RetryInvocationHandler$Call.invokeOnce(RetryInvocationHandler.java:95)
	at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:359)
	at com.sun.proxy.$Proxy25.rollingUpgrade(Unknown Source)
	at org.apache.hadoop.hdfs.DFSClient.rollingUpgrade(DFSClient.java:2368)
	at org.apache.hadoop.hdfs.DistributedFileSystem.rollingUpgrade(DistributedFileSystem.java:1555)
	at org.apache.hadoop.hdfs.TestDFSUpgrade.testUpgrade(TestDFSUpgrade.java:387)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:47)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:12)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:44)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:17)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:74)
2020-04-02 05:08:09,782 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1
2020-04-02 05:08:09,782 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:09,783 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
2020-04-02 05:08:09,785 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2
2020-04-02 05:08:09,786 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:09,786 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
2020-04-02 05:08:09,787 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3
2020-04-02 05:08:09,787 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:09,787 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
2020-04-02 05:08:09,787 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(88)) - Checking namenode directory /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4
2020-04-02 05:08:09,788 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(89)) - ==== Contents ====:
  edits_0000000000000000032-0000000000000000061  
VERSION  
edits_0000000000000000001-0000000000000000031  
edits_inprogress_0000000000000000062  
fsimage_0000000000000000061  
seen_txid  
fsimage_0000000000000000061.md5
2020-04-02 05:08:09,788 [Thread-134] INFO  hdfs.TestDFSUpgrade (TestDFSUpgrade.java:checkNameNode(91)) - ==================
md5 of /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/fsimage_0000000000000000061: 8401ab878af417382fdf96914d80291c
2020-04-02 05:08:09,829 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:shutdown(1998)) - Shutting down the Mini HDFS Cluster
2020-04-02 05:08:09,829 [Thread-134] INFO  hdfs.MiniDFSCluster (MiniDFSCluster.java:stopAndJoinNameNode(2079)) - Shutting down the namenode
2020-04-02 05:08:09,829 [Thread-134] INFO  namenode.NameNode (NameNode.java:stop(1011)) - [msx-restart] NameNode 33402 stop, double check before stop, dfs.namenode.fs-limits.max-directory-items = 1048576
2020-04-02 05:08:09,829 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:09,835 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:endCurrentLogSegment(1406)) - Ending log segment 62, 62
2020-04-02 05:08:09,835 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$NameNodeEditLogRoller@75dfdd2e] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4018)) - NameNodeEditLogRoller was interrupted, exiting
2020-04-02 05:08:09,835 [org.apache.hadoop.hdfs.server.namenode.FSNamesystem$LazyPersistFileScrubber@2ab1ffb] INFO  namenode.FSNamesystem (FSNamesystem.java:run(4109)) - LazyPersistFileScrubber was interrupted, exiting
2020-04-02 05:08:09,841 [Thread-134] INFO  namenode.FSEditLog (FSEditLog.java:printStatistics(774)) - Number of transactions: 2 Total time for transactions(ms): 15 Number of transactions batched in Syncs: 61 Number of syncs: 3 SyncTimes(ms): 4 5 16 3 
2020-04-02 05:08:09,841 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name1/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:09,842 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name2/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:09,843 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name3/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:09,843 [Thread-134] INFO  namenode.FileJournalManager (FileJournalManager.java:finalizeLogSegment(143)) - Finalizing edits file /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/edits_inprogress_0000000000000000062 -> /root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/name4/current/edits_0000000000000000062-0000000000000000063
2020-04-02 05:08:09,843 [FSEditLogAsync] INFO  namenode.FSEditLog (FSEditLogAsync.java:run(253)) - FSEditLogAsync was interrupted, exiting
2020-04-02 05:08:09,843 [CacheReplicationMonitor(374551531)] INFO  blockmanagement.CacheReplicationMonitor (CacheReplicationMonitor.java:run(169)) - Shutting down CacheReplicationMonitor
2020-04-02 05:08:09,844 [Thread-134] INFO  ipc.Server (Server.java:stop(3077)) - Stopping server on 33402
2020-04-02 05:08:09,845 [IPC Server listener on 33402] INFO  ipc.Server (Server.java:run(1181)) - Stopping IPC Server listener on 33402
2020-04-02 05:08:09,855 [IPC Server Responder] INFO  ipc.Server (Server.java:run(1315)) - Stopping IPC Server Responder
2020-04-02 05:08:09,855 [RedundancyMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4557)) - Stopping RedundancyMonitor.
2020-04-02 05:08:09,855 [StorageInfoMonitor] INFO  blockmanagement.BlockManager (BlockManager.java:run(4592)) - Stopping thread.
2020-04-02 05:08:09,865 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopActiveServices(1323)) - Stopping services started for active state
2020-04-02 05:08:09,865 [Thread-134] INFO  namenode.FSNamesystem (FSNamesystem.java:stopStandbyServices(1419)) - Stopping services started for standby state
2020-04-02 05:08:09,866 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.w.WebAppContext@76770e82{/,null,UNAVAILABLE}{/hdfs}
2020-04-02 05:08:09,877 [Thread-134] INFO  server.AbstractConnector (AbstractConnector.java:doStop(318)) - Stopped ServerConnector@570615ed{HTTP/1.1,[http/1.1]}{localhost:0}
2020-04-02 05:08:09,877 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@3a9df139{/static,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/src/main/webapps/static/,UNAVAILABLE}
2020-04-02 05:08:09,878 [Thread-134] INFO  handler.ContextHandler (ContextHandler.java:doStop(910)) - Stopped o.e.j.s.ServletContextHandler@e8227c{/logs,file:///root/hadoop-3.1.2-src/hadoop-hdfs-project/hadoop-hdfs/target/log/,UNAVAILABLE}
2020-04-02 05:08:09,879 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(210)) - Stopping NameNode metrics system...
2020-04-02 05:08:09,884 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:stop(216)) - NameNode metrics system stopped.
2020-04-02 05:08:09,884 [Thread-134] INFO  impl.MetricsSystemImpl (MetricsSystemImpl.java:shutdown(607)) - NameNode metrics system shutdown complete.
[msx] test Finished org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgrade
[msx] writeFile testName = org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgrade
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] test Started org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgradeFromPreUpgradeLVFails
[msx] perform reset as unitTestCounterInClass 1 is larger than zero
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
2020-04-02 05:08:09,899 [main] ERROR common.Storage (Storage.java:checkVersionUpgradable(1119)) - *********** Upgrade is not supported from this  older version -2 of storage to the current version. Please upgrade to Hadoop-0.18 or a later version and then upgrade to current version. Old layout version is -2 and latest layout version this software version can upgrade from is -16. ************
[msx] test Finished org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgradeFromPreUpgradeLVFails
[msx] writeFile testName = org.apache.hadoop.hdfs.TestDFSUpgrade#testUpgradeFromPreUpgradeLVFails
[msx] succeed
[msx] reset reconf_instanceWithV2Alive to false
[msx] reset reconf_instanceWithV2HC to -1
[msx] reset reconf_init_point_index to 0
[msx] all testRunFinished
